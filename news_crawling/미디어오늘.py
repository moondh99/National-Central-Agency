import requests
import xml.etree.ElementTree as ET
import csv
import re
from datetime import datetime
from bs4 import BeautifulSoup
import time

def extract_mediatoday_article_content(url):
    """ë¯¸ë””ì–´ì˜¤ëŠ˜ ê¸°ì‚¬ URLì—ì„œ ì „ì²´ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë¯¸ë””ì–´ì˜¤ëŠ˜ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„)
        content_selectors = [
            'div.article_content',       # ê¸°ì‚¬ ì»¨í…ì¸ 
            'div.news-content',          # ë‰´ìŠ¤ ì»¨í…ì¸ 
            'div.content',               # ì»¨í…ì¸ 
            'div.article-body',          # ê¸°ì‚¬ ë³¸ë¬¸
            'div.news-text',             # ë‰´ìŠ¤ í…ìŠ¤íŠ¸
            '.news_txt',                 # ë‰´ìŠ¤ í…ìŠ¤íŠ¸ í´ë˜ìŠ¤
            'div.view-content',          # ë·° ì»¨í…ì¸ 
            '#content',                  # ID ê¸°ë°˜ ì»¨í…ì¸ 
            'div.text_area',             # í…ìŠ¤íŠ¸ ì˜ì—­
            'section.article-content',   # ì„¹ì…˜ ê¸°ì‚¬ ì»¨í…ì¸ 
            'div.detail-content'         # ìƒì„¸ ì»¨í…ì¸ 
        ]
        
        full_content = ""
        
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                for element in elements:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ ì œê±°
                    for unwanted in element.find_all([
                        'script', 'style', 'iframe', 'ins', 
                        'div.ad', '.advertisement', '.related-articles',
                        '.tags', '.share', '.comment', '.footer',
                        'div.reporter', '.reporter_info', '.social',
                        '.video', '.photo', '.image', '.btn',
                        'div.related'
                    ]):
                        unwanted.decompose()
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text = element.get_text(separator='\n', strip=True)
                    if text and len(text) > len(full_content):
                        full_content = text
                        break
                
                if full_content:
                    break
        
        # ë³¸ë¬¸ì´ ì—¬ì „íˆ ì§§ë‹¤ë©´ ì „ì²´ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if len(full_content) < 100:
            # ë¯¸ë””ì–´ì˜¤ëŠ˜ì˜ ê²½ìš° ê°„ë‹¨í•œ HTML êµ¬ì¡°
            page_text = soup.get_text(separator='\n', strip=True)
            lines = page_text.split('\n')
            
            content_lines = []
            start_found = False
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # ë³¸ë¬¸ ì‹œì‘ ì¡°ê±´ (ì œëª© ì´í›„)
                if not start_found:
                    # ì´ë¯¸ì§€ ìº¡ì…˜ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°ê°€ ë§ìŒ
                    if (line.startswith('â–²') or
                        (len(line) > 30 and 
                         not line.startswith('[') and 
                         not line.startswith('Copyright') and
                         'ë¯¸ë””ì–´ì˜¤ëŠ˜' not in line and
                         'ê¸°ì' not in line[-20:])):
                        start_found = True
                        content_lines.append(line)
                    continue
                
                # ë³¸ë¬¸ ë ì¡°ê±´
                if (line.startswith('Copyright') or
                    line.startswith('â–¶') or
                    line.startswith('â–·') or
                    'ì €ì‘ê¶Œì' in line or
                    'ë¬´ë‹¨ì „ì¬' in line or
                    'ì¬ë°°í¬ ê¸ˆì§€' in line or
                    'ê´€ë ¨ê¸°ì‚¬' in line):
                    break
                
                # ìœ íš¨í•œ ë³¸ë¬¸ ë¼ì¸ ì¶”ê°€
                if len(line) > 5:
                    content_lines.append(line)
            
            if content_lines:
                full_content = '\n'.join(content_lines)
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        if full_content:
            # ì €ì‘ê¶Œ ê´€ë ¨ ì •ë³´ ì œê±°
            full_content = re.sub(r'Copyright.*?ë¯¸ë””ì–´ì˜¤ëŠ˜.*?ê¸ˆì§€', '', full_content, flags=re.DOTALL)
            full_content = re.sub(r'â–¶.*?ë¯¸ë””ì–´ì˜¤ëŠ˜.*?$', '', full_content, flags=re.MULTILINE)
            full_content = re.sub(r'ê´€ë ¨ê¸°ì‚¬.*?$', '', full_content, flags=re.DOTALL)
            # ê¸°ìëª… ë¼ì¸ ì œê±° (ë§ˆì§€ë§‰ì— ìˆëŠ” ê²½ìš°)
            full_content = re.sub(r'\n[ê°€-í£]{2,4}\s*(ê¸°ì|íŠ¹íŒŒì›).*?$', '', full_content, flags=re.MULTILINE)
            # ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
            full_content = re.sub(r'\n+', '\n', full_content)
            full_content = re.sub(r'\s+', ' ', full_content)
            full_content = full_content.strip()
        
        return full_content
        
    except Exception as e:
        print(f"ë³¸ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""

def extract_mediatoday_reporter_name(soup, article_text):
    """ë¯¸ë””ì–´ì˜¤ëŠ˜ ê¸°ìëª…ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        # ë¯¸ë””ì–´ì˜¤ëŠ˜ì˜ ê¸°ìëª… ì¶”ì¶œ íŒ¨í„´
        reporter_patterns = [
            # HTMLì—ì„œ ê¸°ìëª… ì¶”ì¶œ
            r'<span[^>]*class[^>]*reporter[^>]*>([^<]+)</span>',
            r'<div[^>]*class[^>]*reporter[^>]*>([^<]+)</div>',
            
            # í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìëª… ì¶”ì¶œ (ë¯¸ë””ì–´ì˜¤ëŠ˜ íŠ¹ì„±ì— ë§ê²Œ)
            r'([ê°€-í£]{2,4})\s*(ê¸°ì|íŠ¹íŒŒì›)(?:\s*=|\s*âˆ™|\s*Â·|\s*ì…ë ¥|\s*ìˆ˜ì •|\s*ì‘ì„±)',
            r'ê¸°ì\s*([ê°€-í£]{2,4})(?:\s*=|\s*âˆ™|\s*Â·)',
            r'([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›',
            r'([ê°€-í£]{2,4})\s*í¸ì§‘ìœ„ì›',
            r'([ê°€-í£]{2,4})\s*ë…¼ì„¤ìœ„ì›',
            r'/\s*([ê°€-í£]{2,4})\s*ê¸°ì',
            r'=\s*([ê°€-í£]{2,4})\s*ê¸°ì',
            r'âˆ™\s*([ê°€-í£]{2,4})\s*ê¸°ì',
            r'Â·\s*([ê°€-í£]{2,4})\s*ê¸°ì',
            r'ê¸°ì\s*:\s*([ê°€-í£]{2,4})',
            r'\[([ê°€-í£]{2,4})\s*ê¸°ì\]',
            r'ì·¨ì¬\s*:\s*([ê°€-í£]{2,4})',  # ì·¨ì¬: ê¸°ìëª…
            r'ê¸€\s*:\s*([ê°€-í£]{2,4})',   # ê¸€: ê¸°ìëª…
            r'ì •ë¦¬\s*:\s*([ê°€-í£]{2,4})'  # ì •ë¦¬: ê¸°ìëª…
        ]
        
        # BeautifulSoup ê°ì²´ì—ì„œ ê¸°ìëª… ì°¾ê¸°
        if soup:
            # ê¸°ìëª…ì´ í¬í•¨ë  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ìš”ì†Œë“¤ ì°¾ê¸°
            reporter_elements = soup.find_all(['span', 'div', 'p'], string=re.compile(r'ê¸°ì|íŠ¹íŒŒì›|ì·¨ì¬|ì •ë¦¬'))
            for element in reporter_elements:
                text = element.get_text(strip=True)
                if ('ê¸°ì' in text or 'íŠ¹íŒŒì›' in text):
                    match = re.search(r'([ê°€-í£]{2,4})', text)
                    if match:
                        name = match.group(1)
                        if 'ê¸°ì' in text:
                            return name + ' ê¸°ì'
                        elif 'íŠ¹íŒŒì›' in text:
                            return name + ' íŠ¹íŒŒì›'
        
        # ê¸°ì‚¬ í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìëª… ì°¾ê¸°
        full_text = str(soup) + '\n' + article_text if soup else article_text
        
        for pattern in reporter_patterns:
            matches = re.findall(pattern, full_text)
            if matches:
                if isinstance(matches[0], tuple):
                    reporter = matches[0][0].strip()
                    role = matches[0][1] if len(matches[0]) > 1 else 'ê¸°ì'
                else:
                    reporter = matches[0].strip()
                    role = 'ê¸°ì'
                
                if reporter and len(reporter) >= 2:
                    return reporter + f' {role}' if role not in reporter else reporter
        
        return "ê¸°ìëª… ì—†ìŒ"
        
    except Exception as e:
        print(f"ê¸°ìëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return "ê¸°ìëª… ì—†ìŒ"

def parse_mediatoday_rss_full_content(category='all'):
    """ë¯¸ë””ì–´ì˜¤ëŠ˜ RSSë¥¼ íŒŒì‹±í•˜ì—¬ ì „ì²´ ë³¸ë¬¸ê³¼ í•¨ê»˜ CSVë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    
    # ë¯¸ë””ì–´ì˜¤ëŠ˜ RSS URL ëª©ë¡
    category_urls = {
        'all': 'https://www.mediatoday.co.kr/rss/allArticle.xml',     # ì „ì²´ ê¸°ì‚¬
        'clicktop': 'https://www.mediatoday.co.kr/rss/clickTop.xml', # ì¸ê¸° ê¸°ì‚¬
        'politics': 'https://www.mediatoday.co.kr/rss/S1N1.xml',     # ì •ì¹˜
        'economy': 'https://www.mediatoday.co.kr/rss/S1N3.xml',      # ê²½ì œ
        'society': 'https://www.mediatoday.co.kr/rss/S1N4.xml',      # ì‚¬íšŒ
        'culture': 'https://www.mediatoday.co.kr/rss/S1N5.xml',      # ë¬¸í™”
        'international': 'https://www.mediatoday.co.kr/rss/S1N6.xml', # ì„¸ê³„
        'opinion': 'https://www.mediatoday.co.kr/rss/S1N7.xml'       # ì˜¤í”¼ë‹ˆì–¸
    }
    
    if category not in category_urls:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤.")
        print(f"âœ… ì§€ì› ì¹´í…Œê³ ë¦¬: {', '.join(category_urls.keys())}")
        return None
    
    rss_url = category_urls[category]
    
    try:
        print(f"ğŸ“¡ ë¯¸ë””ì–´ì˜¤ëŠ˜ {category} RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        response = requests.get(rss_url, headers=headers, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        # XML íŒŒì‹±
        root = ET.fromstring(response.content)
        items = root.findall('.//item')
        news_data = []
        
        print(f"ì´ {len(items)}ê°œì˜ ë‰´ìŠ¤ í•­ëª©ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        print("ê° ê¸°ì‚¬ì˜ ì „ì²´ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” ì¤‘... (ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        
        for i, item in enumerate(items):
            try:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = item.find('title').text if item.find('title') is not None else "ì œëª© ì—†ìŒ"
                title = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', title)
                title = re.sub(r'<[^>]+>', '', title).strip()
                
                # ë§í¬ ì¶”ì¶œ
                link = item.find('link').text if item.find('link') is not None else ""
                
                # ë‚ ì§œ ì¶”ì¶œ
                pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ""
                formatted_date = ""
                if pub_date:
                    try:
                        # ë¯¸ë””ì–´ì˜¤ëŠ˜ RSS ë‚ ì§œ í˜•ì‹: 2025-06-28 15:22:32
                        date_obj = datetime.strptime(pub_date, '%Y-%m-%d %H:%M:%S')
                        formatted_date = date_obj.strftime('%Y-%m-%d %H:%M:%S')
                    except:
                        formatted_date = pub_date
                else:
                    formatted_date = datetime.now().strftime('%Y-%m-%d %H:%M')
                
                print(f"[{i+1}/{len(items)}] ì²˜ë¦¬ ì¤‘: {title[:60]}...")
                
                if link:
                    # ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ
                    try:
                        article_response = requests.get(link, headers=headers, timeout=20)
                        article_response.encoding = 'utf-8'
                        soup = BeautifulSoup(article_response.text, 'html.parser')
                        
                        # ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ
                        full_content = extract_mediatoday_article_content(link)
                        
                        # ê¸°ìëª… ì¶”ì¶œ
                        reporter_name = extract_mediatoday_reporter_name(soup, full_content)
                        
                        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš° RSS descriptionë„ í¬í•¨
                        if len(full_content) < 200:
                            rss_description = item.find('description').text if item.find('description') is not None else ""
                            rss_description = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', rss_description)
                            rss_description = re.sub(r'<[^>]+>', '', rss_description).strip()
                            
                            if rss_description:
                                full_content = rss_description + '\n\n' + full_content if full_content else rss_description
                        
                        # ë°ì´í„° ì €ì¥
                        if full_content.strip():  # ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš°ë§Œ ì €ì¥
                            news_data.append({
                                'ì œëª©': title,
                                'ë‚ ì§œ': formatted_date,
                                'ê¸°ìëª…': reporter_name,
                                'ë³¸ë¬¸': full_content
                            })
                        else:
                            print(f"  â¤ ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                        
                        # ì„œë²„ ë¶€í•˜ ë°©ì§€
                        time.sleep(1)
                        
                    except Exception as e:
                        print(f"  â¤ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ RSS ê¸°ë³¸ ì •ë³´ëŠ” ì €ì¥
                        description = item.find('description').text if item.find('description') is not None else ""
                        description = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', description)
                        description = re.sub(r'<[^>]+>', '', description).strip()
                        
                        news_data.append({
                            'ì œëª©': title,
                            'ë‚ ì§œ': formatted_date,
                            'ê¸°ìëª…': "ê¸°ìëª… ì—†ìŒ",
                            'ë³¸ë¬¸': description
                        })
                        continue
                
            except Exception as e:
                print(f"RSS í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # CSV íŒŒì¼ë¡œ ì €ì¥
        if news_data:
            filename = f"ë¯¸ë””ì–´ì˜¤ëŠ˜_{category}_ì „ì²´ë³¸ë¬¸_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            
            with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['ì œëª©', 'ë‚ ì§œ', 'ê¸°ìëª…', 'ë³¸ë¬¸']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                
                writer.writeheader()
                writer.writerows(news_data)
            
            print(f"\nâœ… ì„±ê³µì ìœ¼ë¡œ {len(news_data)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“ íŒŒì¼ëª…: {filename}")
            
            # í†µê³„ ì •ë³´ ì¶œë ¥
            total_chars = sum(len(item['ë³¸ë¬¸']) for item in news_data)
            avg_chars = total_chars // len(news_data) if news_data else 0
            print(f"ğŸ“Š í‰ê·  ë³¸ë¬¸ ê¸¸ì´: {avg_chars:,}ì")
            print(f"ğŸ“Š ì´ ë³¸ë¬¸ ê¸¸ì´: {total_chars:,}ì")
            
            return filename
        else:
            print("âŒ ì¶”ì¶œëœ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
    except Exception as e:
        print(f"âŒ RSS íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def scrape_mediatoday_multiple_categories(categories=['all'], max_articles_per_category=20):
    """ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì˜ ë¯¸ë””ì–´ì˜¤ëŠ˜ ë‰´ìŠ¤ë¥¼ ë™ì‹œì— ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜"""
    
    print("ğŸ—ï¸  ë¯¸ë””ì–´ì˜¤ëŠ˜ ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘")
    print("=" * 60)
    
    total_collected = 0
    
    for category in categories:
        print(f"\nğŸ“° {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¤‘...")
        
        try:
            result = parse_mediatoday_rss_full_content(category)
            
            if result:
                print(f"âœ… {category} ì¹´í…Œê³ ë¦¬ ì™„ë£Œ")
                total_collected += 1
            else:
                print(f"âŒ {category} ì¹´í…Œê³ ë¦¬ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"âŒ {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
    
    print(f"\nğŸ‰ ì´ {total_collected}ê°œ ì¹´í…Œê³ ë¦¬ì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
    return total_collected

if __name__ == "__main__":
    print("ğŸ—ï¸  ë¯¸ë””ì–´ì˜¤ëŠ˜ RSS ì „ì²´ ë³¸ë¬¸ í¬ë¡¤ë§")
    print("=" * 60)
    
    try:
        print("\nğŸ“‹ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ì „ì²´ ê¸°ì‚¬")
        print("2. íŠ¹ì • ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤")
        print("3. ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤")
        print("4. ì¸ê¸° ê¸°ì‚¬")
        
        choice = input("\nì„ íƒ (1-4): ").strip()
        
        if choice == "1":
            print("\nğŸš€ ì „ì²´ ê¸°ì‚¬ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            result = parse_mediatoday_rss_full_content('all')
            
        elif choice == "2":
            print("\nğŸ“‚ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
            categories = ['all', 'clicktop', 'politics', 'economy', 'society', 'culture', 'international', 'opinion']
            korean_names = {
                'all': 'ì „ì²´ ê¸°ì‚¬', 'clicktop': 'ì¸ê¸° ê¸°ì‚¬', 'politics': 'ì •ì¹˜', 
                'economy': 'ê²½ì œ', 'society': 'ì‚¬íšŒ', 'culture': 'ë¬¸í™”',
                'international': 'ì„¸ê³„', 'opinion': 'ì˜¤í”¼ë‹ˆì–¸'
            }
            for i, cat in enumerate(categories, 1):
                print(f"{i}. {cat} ({korean_names.get(cat, cat)})")
            
            cat_choice = input("\nì¹´í…Œê³ ë¦¬ ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„: ").strip()
            
            if cat_choice.isdigit() and 1 <= int(cat_choice) <= len(categories):
                selected_category = categories[int(cat_choice) - 1]
            elif cat_choice in categories:
                selected_category = cat_choice
            else:
                selected_category = 'all'
                print("âš ï¸  ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ì „ì²´ ê¸°ì‚¬ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            
            print(f"\nğŸš€ {selected_category} ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            result = parse_mediatoday_rss_full_content(selected_category)
            
        elif choice == "3":
            print("\nğŸ“‚ ìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬ë“¤ì„ ì„ íƒí•˜ì„¸ìš” (ì‰¼í‘œë¡œ êµ¬ë¶„):")
            print("ì˜ˆ: politics,economy,society")
            print("ì‚¬ìš© ê°€ëŠ¥: all, clicktop, politics, economy, society, culture, international, opinion")
            
            cats_input = input("\nì¹´í…Œê³ ë¦¬ë“¤: ").strip()
            if cats_input:
                selected_categories = [cat.strip() for cat in cats_input.split(',')]
            else:
                selected_categories = ['politics', 'economy', 'society']
                print("âš ï¸  ê¸°ë³¸ ì¹´í…Œê³ ë¦¬(ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ)ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            
            print(f"\nğŸš€ ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤: {', '.join(selected_categories)}")
            result = scrape_mediatoday_multiple_categories(selected_categories)
            
        elif choice == "4":
            print("\nğŸš€ ì¸ê¸° ê¸°ì‚¬ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            result = parse_mediatoday_rss_full_content('clicktop')
            
        else:
            print("âš ï¸  ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ì „ì²´ ê¸°ì‚¬ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            result = parse_mediatoday_rss_full_content('all')
        
        if result:
            if isinstance(result, str):
                print(f"\nğŸ‰ ì™„ë£Œ! íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {result}")
            else:
                print(f"\nğŸ‰ ì™„ë£Œ! {result}ê°œ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ì™„ë£Œ")
        else:
            print("\nâŒ ì‘ì—…ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
