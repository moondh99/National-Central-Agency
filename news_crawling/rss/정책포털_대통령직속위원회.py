#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì •ì±…ë¸Œë¦¬í•‘(korea.kr) ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒë³„ RSS í¬ë¡¤ë§ ì½”ë“œ
4ê°œ ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ RSS í”¼ë“œ ì „ìš© í¬ë¡¤ëŸ¬

ì‘ì„±ì¼: 2025-08-02
"""

import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import pandas as pd
import time
import random
import re
from datetime import datetime
import csv
from urllib.parse import urljoin, urlparse
import logging

class PresidentialCommitteeRSSCrawler:
    def __init__(self):
        """ì •ì±…ë¸Œë¦¬í•‘ ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒë³„ RSS í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.base_url = "https://www.korea.kr"
        
        # 4ê°œ ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ RSS í”¼ë“œ
        self.presidential_committee_feeds = {
            'êµ­ë¯¼í†µí•©ìœ„ì›íšŒ': 'https://www.korea.kr/rss/dept_k_cohesion.xml',
            'ì €ì¶œì‚°ê³ ë ¹ì‚¬íšŒìœ„ì›íšŒ': 'https://www.korea.kr/rss/dept_betterfuture.xml',
            'ê²½ì œì‚¬íšŒë…¸ë™ìœ„ì›íšŒ': 'https://www.korea.kr/rss/dept_esdc.xml',
            'íƒ„ì†Œì¤‘ë¦½ë…¹ìƒ‰ì„±ì¥ìœ„ì›íšŒ': 'https://www.korea.kr/rss/dept_cnc.xml'
        }
        
        # ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒë³„ ì£¼ìš” ì—…ë¬´ ë¶„ì•¼ (ë¶„ì„ìš©)
        self.committee_areas = {
            'êµ­ë¯¼í†µí•©ìœ„ì›íšŒ': 'ì‚¬íšŒí†µí•©, ê°ˆë“±í•´ê²°, êµ­ë¯¼í™”í•©, ì†Œí†µì •ì±…',
            'ì €ì¶œì‚°ê³ ë ¹ì‚¬íšŒìœ„ì›íšŒ': 'ì €ì¶œì‚°ì •ì±…, ê³ ë ¹ì‚¬íšŒëŒ€ì‘, ì¸êµ¬ì •ì±…, ê°€ì¡±ì •ì±…',
            'ê²½ì œì‚¬íšŒë…¸ë™ìœ„ì›íšŒ': 'ë…¸ì‚¬ê´€ê³„, ê²½ì œë¯¼ì£¼í™”, ì‚¬íšŒëŒ€í™”, ë…¸ë™ì •ì±…',
            'íƒ„ì†Œì¤‘ë¦½ë…¹ìƒ‰ì„±ì¥ìœ„ì›íšŒ': 'íƒ„ì†Œì¤‘ë¦½, ê·¸ë¦°ë‰´ë”œ, ê¸°í›„ë³€í™”ëŒ€ì‘, ì¹œí™˜ê²½ì •ì±…'
        }
        
        # ìœ„ì›íšŒ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        self.committee_categories = {
            'ì‚¬íšŒÂ·í†µí•©': ['êµ­ë¯¼í†µí•©ìœ„ì›íšŒ'],
            'ì¸êµ¬Â·ë³µì§€': ['ì €ì¶œì‚°ê³ ë ¹ì‚¬íšŒìœ„ì›íšŒ'],
            'ë…¸ë™Â·ê²½ì œ': ['ê²½ì œì‚¬íšŒë…¸ë™ìœ„ì›íšŒ'],
            'í™˜ê²½Â·ê¸°í›„': ['íƒ„ì†Œì¤‘ë¦½ë…¹ìƒ‰ì„±ì¥ìœ„ì›íšŒ']
        }
        
        # ìœ„ì›íšŒë³„ ì£¼ìš” í‚¤ì›Œë“œ
        self.committee_keywords = {
            'êµ­ë¯¼í†µí•©ìœ„ì›íšŒ': ['í†µí•©', 'í™”í•©', 'ê°ˆë“±', 'ì†Œí†µ', 'í˜‘ì¹˜', 'ìƒìƒ', 'ëŒ€í™”', 'í¬ìš©'],
            'ì €ì¶œì‚°ê³ ë ¹ì‚¬íšŒìœ„ì›íšŒ': ['ì €ì¶œì‚°', 'ê³ ë ¹í™”', 'ì¸êµ¬', 'ì¶œì‚°', 'ìœ¡ì•„', 'ë³´ìœ¡', 'ë…¸ì¸', 'ì‹¤ë²„'],
            'ê²½ì œì‚¬íšŒë…¸ë™ìœ„ì›íšŒ': ['ë…¸ì‚¬', 'ë…¸ë™', 'ì„ê¸ˆ', 'ì¼ìë¦¬', 'ê³ ìš©', 'ê·¼ë¡œ', 'ì‚¬íšŒëŒ€í™”', 'í˜‘ì•½'],
            'íƒ„ì†Œì¤‘ë¦½ë…¹ìƒ‰ì„±ì¥ìœ„ì›íšŒ': ['íƒ„ì†Œì¤‘ë¦½', 'ê·¸ë¦°ë‰´ë”œ', 'ì¹œí™˜ê²½', 'ì¬ìƒì—ë„ˆì§€', 'ê¸°í›„ë³€í™”', 'ì˜¨ì‹¤ê°€ìŠ¤', 'ë„·ì œë¡œ']
        }
        
        # ìœ„ì›íšŒë³„ ì£¼ìš” ì •ì±… ë°©í–¥
        self.policy_directions = {
            'êµ­ë¯¼í†µí•©ìœ„ì›íšŒ': 'ì‚¬íšŒ ê°ˆë“± í•´ì†Œ ë° êµ­ë¯¼ í™”í•© ë„ëª¨',
            'ì €ì¶œì‚°ê³ ë ¹ì‚¬íšŒìœ„ì›íšŒ': 'ì €ì¶œì‚°Â·ê³ ë ¹ì‚¬íšŒ ëŒ€ì‘ ì •ì±… ìˆ˜ë¦½',
            'ê²½ì œì‚¬íšŒë…¸ë™ìœ„ì›íšŒ': 'ë…¸ì‚¬ ê°„ ëŒ€í™”ì™€ í˜‘ë ¥ì„ í†µí•œ ìƒìƒë°œì „',
            'íƒ„ì†Œì¤‘ë¦½ë…¹ìƒ‰ì„±ì¥ìœ„ì›íšŒ': '2050 íƒ„ì†Œì¤‘ë¦½ ë‹¬ì„± ë° ë…¹ìƒ‰ì„±ì¥ ì¶”ì§„'
        }
        
        # User-Agent ë¦¬ìŠ¤íŠ¸
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15'
        ]
        
        self.articles = []
        self.session = requests.Session()
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)

    def get_random_headers(self):
        """ëœë¤ í—¤ë” ìƒì„±"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }

    def random_delay(self, min_delay=1, max_delay=3):
        """ëœë¤ ë”œë ˆì´"""
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)

    def fetch_rss_feed(self, rss_url, max_retries=3):
        """RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°"""
        for attempt in range(max_retries):
            try:
                headers = self.get_random_headers()
                response = self.session.get(rss_url, headers=headers, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except Exception as e:
                self.logger.warning(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    self.random_delay(2, 5)
                else:
                    self.logger.error(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ìµœì¢… ì‹¤íŒ¨: {rss_url}")
                    return None

    def parse_rss_feed(self, rss_content):
        """RSS í”¼ë“œ íŒŒì‹±"""
        try:
            root = ET.fromstring(rss_content)
            items = []
            
            for item in root.findall('.//item'):
                article_info = {}
                
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title_elem = item.find('title')
                article_info['title'] = title_elem.text.strip() if title_elem is not None else ''
                
                link_elem = item.find('link')
                article_info['link'] = link_elem.text.strip() if link_elem is not None else ''
                
                pubdate_elem = item.find('pubDate')
                article_info['pub_date'] = pubdate_elem.text.strip() if pubdate_elem is not None else ''
                
                guid_elem = item.find('guid')
                article_info['guid'] = guid_elem.text.strip() if guid_elem is not None else ''
                
                # dc:creator ì¶”ì¶œ (ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê³ ë ¤)
                creator_elem = item.find('.//{http://purl.org/dc/elements/1.1/}creator')
                article_info['creator'] = creator_elem.text.strip() if creator_elem is not None else ''
                
                # descriptionì—ì„œ ê°„ë‹¨í•œ ë‚´ìš© ì¶”ì¶œ
                desc_elem = item.find('description')
                if desc_elem is not None:
                    desc_text = desc_elem.text or ''
                    # CDATA ì²˜ë¦¬
                    if desc_text.startswith('<![CDATA[') and desc_text.endswith(']]>'):
                        desc_text = desc_text[9:-3]
                    
                    # HTML íƒœê·¸ ì œê±°í•˜ì—¬ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                    soup = BeautifulSoup(desc_text, 'html.parser')
                    article_info['description'] = soup.get_text().strip()[:400] + '...' if len(soup.get_text().strip()) > 400 else soup.get_text().strip()
                else:
                    article_info['description'] = ''
                
                items.append(article_info)
            
            return items
        except Exception as e:
            self.logger.error(f"RSS íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []

    def extract_article_content(self, article_url, max_retries=3):
        """ê°œë³„ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ - ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ í˜ì´ì§€ ìµœì í™”"""
        for attempt in range(max_retries):
            try:
                headers = self.get_random_headers()
                response = self.session.get(article_url, headers=headers, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒë³„ í˜ì´ì§€ êµ¬ì¡°ì— ìµœì í™”ëœ ë³¸ë¬¸ ì¶”ì¶œ ì…€ë ‰í„°
                content_selectors = [
                    '.presidential_cont',  # ëŒ€í†µë ¹ì§ì† ì½˜í…ì¸ 
                    '.committee_cont',     # ìœ„ì›íšŒ ì½˜í…ì¸ 
                    '.policy_cont',        # ì •ì±… ì½˜í…ì¸ 
                    '.press_cont',         # ë³´ë„ìë£Œ ì½˜í…ì¸ 
                    '.meeting_cont',       # íšŒì˜ ì½˜í…ì¸ 
                    '.article_body',       # ì¼ë°˜ ê¸°ì‚¬
                    '.rbody',             # ë¸Œë¦¬í•‘ í˜ì´ì§€
                    '.view_cont',         # ë·° í˜ì´ì§€
                    '.cont_body',         # ì½˜í…ì¸  ë³¸ë¬¸
                    '.policy_body',       # ì •ì±… ë³¸ë¬¸
                    '.briefing_cont',     # ë¸Œë¦¬í•‘ ë‚´ìš©
                    '.news_cont',         # ë‰´ìŠ¤ ë‚´ìš©
                    '.agenda_cont'        # ì˜ì œ ë‚´ìš©
                ]
                
                content = ""
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text().strip()
                        break
                
                # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„
                if not content:
                    # í—¤ë”, í‘¸í„°, ì‚¬ì´ë“œë°” ë“± ì œê±°
                    for elem in soup.find_all(['header', 'footer', 'nav', 'aside', 'script', 'style']):
                        elem.decompose()
                    
                    main_content = soup.find('main') or soup.find('div', class_='content') or soup.find('body')
                    if main_content:
                        content = main_content.get_text().strip()
                
                # ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒë³„ íŠ¹í™” ì •ë³´ ì¶”ì¶œ
                contact_info = self.extract_presidential_contact_info(content)
                policy_keywords = self.extract_policy_keywords(content)
                meeting_type = self.extract_meeting_type(content)
                stakeholders = self.extract_stakeholders(content)
                
                # í…ìŠ¤íŠ¸ ì •ë¦¬
                content = re.sub(r'\s+', ' ', content).strip()
                
                return {
                    'content': content[:4000] + '...' if len(content) > 4000 else content,
                    'contact_info': contact_info,
                    'policy_keywords': policy_keywords,
                    'meeting_type': meeting_type,
                    'stakeholders': stakeholders
                }
                
            except Exception as e:
                self.logger.warning(f"ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    self.random_delay(1, 3)
                else:
                    return {'content': 'ì¶”ì¶œ ì‹¤íŒ¨', 'contact_info': '', 'policy_keywords': '', 'meeting_type': '', 'stakeholders': ''}

    def extract_presidential_contact_info(self, content):
        """ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ ì—°ë½ì²˜/ë‹´ë‹¹ì ì •ë³´ ì¶”ì¶œ"""
        # ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒë³„ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ íŒ¨í„´
        patterns = [
            r'ë¬¸ì˜\s*:\s*([^(]+?)(?:\(([^)]+)\))?',
            r'ë‹´ë‹¹\s*:\s*([^(]+?)(?:\(([^)]+)\))?',
            r'ì—°ë½ì²˜\s*:\s*([^(]+?)(?:\(([^)]+)\))?',
            r'ì‚¬ë¬´ì²˜\s*:\s*([^(]+?)(?:\(([^)]+)\))?',
            r'ìœ„ì›íšŒ\s+ì‚¬ë¬´ì²˜\s*(?:\(([^)]+)\))?',
            r'ëŒ€í†µë ¹ì§ì†\s+([ê°€-í£]+ìœ„ì›íšŒ)\s+ì‚¬ë¬´ì²˜',
            r'í™ˆí˜ì´ì§€\s*:\s*(https?://[^\s]+)'
        ]
        
        contact_info = {}
        
        for pattern in patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if isinstance(match, tuple):
                    if len(match) >= 2:
                        dept = match[0].strip() if match[0] else ''
                        contact = match[1].strip() if match[1] else ''
                        
                        if dept and len(dept) > 1 and len(dept) < 100:
                            contact_info['department'] = dept
                        if contact:
                            if 'http' in contact:
                                contact_info['website'] = contact
                            elif any(prefix in contact for prefix in ['02-', '044-', '070-']):
                                contact_info['phone'] = contact
                else:
                    if match and len(match) > 1 and len(match) < 100:
                        contact_info['department'] = match.strip()
        
        return '; '.join([f"{k}: {v}" for k, v in contact_info.items()])

    def extract_policy_keywords(self, content):
        """ì •ì±… í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒë³„ ì •ì±… í‚¤ì›Œë“œ íŒ¨í„´
        policy_patterns = [
            r'(ì •ì±…|ì œë„|ë°©ì•ˆ|ê³„íš|ì „ëµ|ë¡œë“œë§µ|ê°€ì´ë“œë¼ì¸)',
            r'(íšŒì˜|ë…¼ì˜|ì‹¬ì˜|ì˜ê²°|ê²°ì •|í•©ì˜|í˜‘ì˜)',
            r'(ë°œí‘œ|ê³µí‘œ|ê³µê°œ|ê³µì§€|ì•ˆë‚´|í™ë³´)',
            r'(ê°œì„ |ê°•í™”|í™•ëŒ€|ë„ì…|ì‹œí–‰|ì¶”ì§„|ì‹¤ì‹œ)',
            r'(í˜‘ë ¥|ì—°ê³„|í˜‘ì—…|íŒŒíŠ¸ë„ˆì‹­|ê±°ë²„ë„ŒìŠ¤)',
            r'(ë¯¼ê´€|ì‚°í•™ì—°|ì‹œë¯¼ì‚¬íšŒ|ì´í•´ê´€ê³„ì)',
            r'(í˜ì‹ |ê°œí˜|ì „í™˜|ë³€í™”|ë°œì „|ì„±ì¥)'
        ]
        
        keywords = set()
        for pattern in policy_patterns:
            matches = re.findall(pattern, content)
            keywords.update(matches)
        
        return ', '.join(list(keywords)[:15])  # ìµœëŒ€ 15ê°œ í‚¤ì›Œë“œ

    def extract_meeting_type(self, content):
        """íšŒì˜/í–‰ì‚¬ ìœ í˜• ì¶”ì¶œ"""
        meeting_patterns = [
            r'(ì „ì²´íšŒì˜|ë³¸íšŒì˜|ì‹¤ë¬´íšŒì˜)',
            r'(í¬ëŸ¼|ì„¸ë¯¸ë‚˜|ì‹¬í¬ì§€ì—„|ì›Œí¬ìˆ)',
            r'(í† ë¡ íšŒ|ê°„ë‹´íšŒ|ê³µì²­íšŒ|ì„¤ëª…íšŒ)',
            r'(ì»¨í¼ëŸ°ìŠ¤|ì´íšŒ|ì •ê¸°íšŒì˜|ì„ì‹œíšŒì˜)',
            r'(ë°œí‘œíšŒ|ë³´ê³ íšŒ|í‰ê°€íšŒ|ì ê²€íšŒì˜)',
            r'(í˜‘ì•½ì‹|ì„œëª…ì‹|ì¶œë²”ì‹|ê°œìµœì‹)'
        ]
        
        for pattern in meeting_patterns:
            matches = re.findall(pattern, content)
            if matches:
                return matches[0]
        
        return ''

    def extract_stakeholders(self, content):
        """ì´í•´ê´€ê³„ì ì¶”ì¶œ"""
        stakeholder_patterns = [
            r'(ì •ë¶€|ë¶€ì²˜|ê¸°ê´€|ì²­)',
            r'(ê¸°ì—…|ì—…ê³„|ì‚°ì—…ê³„|ê²½ì œê³„)',
            r'(ë…¸ë™ê³„|ë…¸ì¡°|ê·¼ë¡œì)',
            r'(ì‹œë¯¼ì‚¬íšŒ|NGO|NPO)',
            r'(í•™ê³„|ì—°êµ¬ê¸°ê´€|ì „ë¬¸ê°€)',
            r'(ì§€ë°©ìì¹˜ë‹¨ì²´|ì§€ìì²´|ì‹œë„)',
            r'(êµ­ì œê¸°êµ¬|í•´ì™¸ê¸°ê´€)'
        ]
        
        stakeholders = set()
        for pattern in stakeholder_patterns:
            matches = re.findall(pattern, content)
            stakeholders.update(matches)
        
        return ', '.join(list(stakeholders)[:8])  # ìµœëŒ€ 8ê°œ

    def get_committee_category(self, committee_name):
        """ìœ„ì›íšŒì˜ ì¹´í…Œê³ ë¦¬ ë°˜í™˜"""
        for category, committees in self.committee_categories.items():
            if committee_name in committees:
                return category
        return 'ê¸°íƒ€'

    def get_relevant_keywords(self, committee_name, content):
        """ìœ„ì›íšŒë³„ ê´€ë ¨ í‚¤ì›Œë“œ ë§¤ì¹­"""
        if committee_name in self.committee_keywords:
            keywords = self.committee_keywords[committee_name]
            found_keywords = []
            for keyword in keywords:
                if keyword in content:
                    found_keywords.append(keyword)
            return ', '.join(found_keywords[:10])  # ìµœëŒ€ 10ê°œ
        return ''

    def crawl_presidential_committee_feed(self, committee, rss_url, max_items=35):
        """ê°œë³„ ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ RSS í”¼ë“œ í¬ë¡¤ë§"""
        self.logger.info(f"ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ í¬ë¡¤ë§ ì‹œì‘: {committee}")
        
        # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
        rss_content = self.fetch_rss_feed(rss_url)
        if not rss_content:
            return
        
        # RSS íŒŒì‹±
        rss_items = self.parse_rss_feed(rss_content)
        if not rss_items:
            self.logger.warning(f"RSS ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤: {committee}")
            return
        
        # ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ë§Œ ì²˜ë¦¬
        items_to_process = rss_items[:max_items]
        
        for i, item in enumerate(items_to_process, 1):
            try:
                self.logger.info(f"{committee} ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘: {i}/{len(items_to_process)} - {item['title'][:50]}...")
                
                # ê¸°ì‚¬ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
                if item['link']:
                    article_detail = self.extract_article_content(item['link'])
                    
                    article_data = {
                        'presidential_committee': committee,
                        'committee_category': self.get_committee_category(committee),
                        'business_area': self.committee_areas.get(committee, ''),
                        'policy_direction': self.policy_directions.get(committee, ''),
                        'title': item['title'],
                        'link': item['link'],
                        'pub_date': item['pub_date'],
                        'creator': item['creator'],
                        'description': item['description'],
                        'content': article_detail['content'],
                        'contact_info': article_detail['contact_info'],
                        'policy_keywords': article_detail['policy_keywords'],
                        'meeting_type': article_detail['meeting_type'],
                        'stakeholders': article_detail['stakeholders'],
                        'relevant_keywords': self.get_relevant_keywords(committee, article_detail['content']),
                        'collected_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    
                    self.articles.append(article_data)
                
                # ë”œë ˆì´
                self.random_delay(1, 3)
                
            except Exception as e:
                self.logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        self.logger.info(f"{committee} í¬ë¡¤ë§ ì™„ë£Œ: {len(items_to_process)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬")

    def crawl_all_presidential_committees(self, max_items_per_committee=35):
        """ëª¨ë“  ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ RSS í”¼ë“œ í¬ë¡¤ë§"""
        total_committees = len(self.presidential_committee_feeds)
        self.logger.info(f"ì „ì²´ {total_committees}ê°œ ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ RSS í”¼ë“œ í¬ë¡¤ë§ ì‹œì‘")
        
        for i, (committee, rss_url) in enumerate(self.presidential_committee_feeds.items(), 1):
            try:
                self.logger.info(f"[{i}/{total_committees}] {committee} í”¼ë“œ í¬ë¡¤ë§ ì¤‘...")
                self.crawl_presidential_committee_feed(committee, rss_url, max_items_per_committee)
                
                # ìœ„ì›íšŒ ê°„ ë”œë ˆì´
                if i < total_committees:
                    self.random_delay(4, 7)
                    
            except Exception as e:
                self.logger.error(f"{committee} ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                continue
        
        self.logger.info(f"ì „ì²´ ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ í¬ë¡¤ë§ ì™„ë£Œ: {len(self.articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        self.print_statistics()

    def crawl_specific_committees(self, committee_names, max_items_per_committee=35):
        """íŠ¹ì • ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒë“¤ë§Œ í¬ë¡¤ë§"""
        for committee_name in committee_names:
            if committee_name in self.presidential_committee_feeds:
                self.crawl_presidential_committee_feed(committee_name, self.presidential_committee_feeds[committee_name], max_items_per_committee)
            else:
                self.logger.warning(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ: {committee_name}")
                available_committees = list(self.presidential_committee_feeds.keys())
                self.logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ: {available_committees}")

    def crawl_by_category(self, categories, max_items_per_committee=30):
        """ì¹´í…Œê³ ë¦¬ë³„ ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ í¬ë¡¤ë§"""
        target_committees = []
        
        for category in categories:
            if category in self.committee_categories:
                target_committees.extend(self.committee_categories[category])
            else:
                self.logger.warning(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬: {category}")
        
        if target_committees:
            self.logger.info(f"ì¹´í…Œê³ ë¦¬ '{', '.join(categories)}'ì— í•´ë‹¹í•˜ëŠ” ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ: {target_committees}")
            self.crawl_specific_committees(target_committees, max_items_per_committee)
        else:
            self.logger.warning(f"í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ë§ëŠ” ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {categories}")

    def save_to_csv(self, filename=None):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'results/ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ_RSS_{timestamp}.csv'
        
        try:
            df = pd.DataFrame(self.articles)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            self.logger.info(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            self.logger.info(f"ì´ {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥")
        except Exception as e:
            self.logger.error(f"CSV ì €ì¥ ì˜¤ë¥˜: {e}")

    def save_by_committee(self):
        """ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒë³„ë¡œ ê°œë³„ CSV íŒŒì¼ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        df = pd.DataFrame(self.articles)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        for committee in df['presidential_committee'].unique():
            committee_df = df[df['presidential_committee'] == committee]
            filename = f'results/ëŒ€í†µë ¹ì§ì†_{committee}_{timestamp}.csv'
            committee_df.to_csv(filename, index=False, encoding='utf-8-sig')
            self.logger.info(f"{committee} ì €ì¥ ì™„ë£Œ: {filename} ({len(committee_df)}ê°œ ê¸°ì‚¬)")

    def save_by_category(self):
        """ì¹´í…Œê³ ë¦¬ë³„ë¡œ CSV íŒŒì¼ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        df = pd.DataFrame(self.articles)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        for category in df['committee_category'].unique():
            category_df = df[df['committee_category'] == category]
            filename = f'results/ëŒ€í†µë ¹ì§ì†ì¹´í…Œê³ ë¦¬_{category}_{timestamp}.csv'
            category_df.to_csv(filename, index=False, encoding='utf-8-sig')
            self.logger.info(f"{category} ì¹´í…Œê³ ë¦¬ ì €ì¥ ì™„ë£Œ: {filename} ({len(category_df)}ê°œ ê¸°ì‚¬)")

    def print_statistics(self):
        """í¬ë¡¤ë§ í†µê³„ ì¶œë ¥"""
        if not self.articles:
            return
        
        df = pd.DataFrame(self.articles)
        
        print("\n" + "="*70)
        print("ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒë³„ RSS í¬ë¡¤ë§ í†µê³„")
        print("="*70)
        
        # ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒë³„ í†µê³„
        committee_stats = df['presidential_committee'].value_counts()
        print(f"\nğŸ›ï¸ ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒë³„ ê¸°ì‚¬ ìˆ˜:")
        for committee, count in committee_stats.items():
            business_area = self.committee_areas.get(committee, '')
            policy_direction = self.policy_directions.get(committee, '')
            print(f"  â€¢ {committee}")
            print(f"    - ì—…ë¬´ë¶„ì•¼: {business_area}")
            print(f"    - ì •ì±…ë°©í–¥: {policy_direction}")
            print(f"    - ê¸°ì‚¬ ìˆ˜: {count}ê°œ\n")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = df['committee_category'].value_counts()
        print(f"ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜:")
        for category, count in category_stats.items():
            print(f"  â€¢ {category}: {count}ê°œ")
        
        # íšŒì˜/í–‰ì‚¬ ìœ í˜•ë³„ í†µê³„
        meeting_stats = df[df['meeting_type'] != '']['meeting_type'].value_counts().head(10)
        if not meeting_stats.empty:
            print(f"\nğŸ¤ ì£¼ìš” íšŒì˜/í–‰ì‚¬ ìœ í˜•ë³„ ê¸°ì‚¬ ìˆ˜:")
            for meeting_type, count in meeting_stats.items():
                print(f"  â€¢ {meeting_type}: {count}ê°œ")
        
        # ì´í•´ê´€ê³„ì í†µê³„
        stakeholder_stats = df[df['stakeholders'] != '']['stakeholders'].str.split(', ').explode().value_counts().head(8)
        if not stakeholder_stats.empty:
            print(f"\nğŸ‘¥ ì£¼ìš” ì´í•´ê´€ê³„ìë³„ ì–¸ê¸‰ ìˆ˜:")
            for stakeholder, count in stakeholder_stats.items():
                print(f"  â€¢ {stakeholder}: {count}íšŒ")
        
        # ì •ì±… í‚¤ì›Œë“œ í†µê³„
        policy_available = len(df[df['policy_keywords'] != ''])
        print(f"\nğŸ“‹ ì •ì±… í‚¤ì›Œë“œ:")
        print(f"  â€¢ í‚¤ì›Œë“œ ì¶”ì¶œ ì„±ê³µ: {policy_available}ê°œ")
        print(f"  â€¢ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {len(df) - policy_available}ê°œ")
        
        # ì—°ë½ì²˜ ì •ë³´ í†µê³„
        contact_available = len(df[df['contact_info'] != ''])
        print(f"\nğŸ“ ì—°ë½ì²˜ ì •ë³´:")
        print(f"  â€¢ ì—°ë½ì²˜ ì¶”ì¶œ ì„±ê³µ: {contact_available}ê°œ")
        print(f"  â€¢ ì—°ë½ì²˜ ì¶”ì¶œ ì‹¤íŒ¨: {len(df) - contact_available}ê°œ")
        
        print(f"\nğŸ“ˆ ì „ì²´ ìš”ì•½:")
        print(f"  â€¢ ì´ ê¸°ì‚¬ ìˆ˜: {len(self.articles)}ê°œ")
        print(f"  â€¢ í¬ë¡¤ë§ ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ ìˆ˜: {len(committee_stats)}ê°œ")
        print(f"  â€¢ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {len(df[df['content'] != 'ì¶”ì¶œ ì‹¤íŒ¨'])}ê°œ")
        print(f"  â€¢ ê´€ë ¨ í‚¤ì›Œë“œ ë§¤ì¹­: {len(df[df['relevant_keywords'] != ''])}ê°œ")
        print("="*70)

    def get_available_committees(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ ëª©ë¡ ë°˜í™˜"""
        return list(self.presidential_committee_feeds.keys())

    def get_categories(self):
        """ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë°˜í™˜"""
        return list(self.committee_categories.keys())


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ì •ì±…ë¸Œë¦¬í•‘ ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒë³„ RSS í¬ë¡¤ëŸ¬")
    print("="*50)
    
    crawler = PresidentialCommitteeRSSCrawler()
    
    # ì‚¬ìš© ì˜ˆì‹œ 1: ì „ì²´ ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ í¬ë¡¤ë§ (ê° ìœ„ì›íšŒë‹¹ 25ê°œì”©)
    print("ì „ì²´ ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒ RSS í”¼ë“œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    crawler.crawl_all_presidential_committees(max_items_per_committee=10)
    
    # CSV ì €ì¥
    crawler.save_to_csv()
    
    # ìœ„ì›íšŒë³„ ê°œë³„ íŒŒì¼ ì €ì¥
    # crawler.save_by_committee()
    
    # ì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼ ì €ì¥
    # crawler.save_by_category()
    
    # ì‚¬ìš© ì˜ˆì‹œ 2: íŠ¹ì • ëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒë§Œ í¬ë¡¤ë§
    # crawler.crawl_specific_committees(['íƒ„ì†Œì¤‘ë¦½ë…¹ìƒ‰ì„±ì¥ìœ„ì›íšŒ', 'ê²½ì œì‚¬íšŒë…¸ë™ìœ„ì›íšŒ'])
    
    # ì‚¬ìš© ì˜ˆì‹œ 3: ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§
    # crawler.crawl_by_category(['í™˜ê²½Â·ê¸°í›„', 'ë…¸ë™Â·ê²½ì œ'], max_items_per_committee=30)
    
    print("\nëŒ€í†µë ¹ì§ì†ìœ„ì›íšŒë³„ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
