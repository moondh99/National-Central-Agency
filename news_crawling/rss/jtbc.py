import feedparser
import csv
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import time
import random
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager


def get_random_user_agent():
    """ëœë¤ User-Agent ë°˜í™˜"""
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    ]
    return random.choice(user_agents)


def extract_jtbc_article_content(url, rss_summary=""):
    """JTBC ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ê³¼ ê¸°ìëª…ì„ ì¶”ì¶œ"""
    try:
        session = requests.Session()

        headers = {
            "User-Agent": get_random_user_agent(),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Referer": "https://news-ex.jtbc.co.kr/",
            "Cache-Control": "no-cache",
        }

        print(f"    ì ‘ì† ì‹œë„: {url[:80]}...")

        # ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ í›„ ì‹¤ì œ ê¸°ì‚¬ ì ‘ê·¼
        try:
            session.get("https://news-ex.jtbc.co.kr/", headers=headers, timeout=5)
            time.sleep(0.5)

            response = session.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            if len(response.content) < 3000:  # 3KB ë¯¸ë§Œì´ë©´ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ
                print(f"    âš  ì‘ë‹µ í¬ê¸°ê°€ ì‘ìŒ (í¬ê¸°: {len(response.content)} bytes)")

        except Exception as e:
            print(f"    âš  ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return "", rss_summary if rss_summary else "ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨"

        soup = BeautifulSoup(response.content, "html.parser")
        full_text = soup.get_text()

        # ê¸°ìëª… ì¶”ì¶œ - JTBC íŒ¨í„´ì— ë§ê²Œ ìˆ˜ì •
        reporter = ""
        reporter_patterns = [
            r"([ê°€-í£]{2,4})\s*ê¸°ì\s*([a-zA-Z0-9_.+-]+@jtbc\.co\.kr)",  # ê¸°ìëª… ê¸°ì ì´ë©”ì¼@jtbc.co.kr
            r"([ê°€-í£]{2,4})\s*ê¸°ì\s*[a-zA-Z0-9_.+-]+@jtbc\.co\.kr",  # ê¸°ìëª… ê¸°ì ì´ë©”ì¼
            r"([ê°€-í£]{2,4})\s*ê¸°ì",  # ê¸°ìëª… ê¸°ì
            r"ê¸°ì\s*([ê°€-í£]{2,4})",  # ê¸°ì ê¸°ìëª…
            r"([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›",  # ê¸°ìëª… íŠ¹íŒŒì›
            r"([ê°€-í£]{2,4})\s*ì•µì»¤",  # ê¸°ìëª… ì•µì»¤
            r"([ê°€-í£]{2,4})\s*ì•„ë‚˜ìš´ì„œ",  # ê¸°ìëª… ì•„ë‚˜ìš´ì„œ
            r"([ê°€-í£]{2,4})\s*ê¸°ì\s*=",  # ê¸°ìëª… ê¸°ì =
            r"ì·¨ì¬\s*([ê°€-í£]{2,4})",  # ì·¨ì¬ ê¸°ìëª…
            r"ê¸€\s*([ê°€-í£]{2,4})",  # ê¸€ ê¸°ìëª…
            r"([ê°€-í£]{2,4})\s*ì„ ì„ê¸°ì",  # ê¸°ìëª… ì„ ì„ê¸°ì
            r"([ê°€-í£]{2,4})\s*ìˆ˜ì„ê¸°ì",  # ê¸°ìëª… ìˆ˜ì„ê¸°ì
            r"([ê°€-í£]{2,4})\s*ë…¼ì„¤ìœ„ì›",  # ê¸°ìëª… ë…¼ì„¤ìœ„ì›
            r"JTBC\s*([ê°€-í£]{2,4})",  # JTBC ê¸°ìëª…
        ]

        # ê¸°ì‚¬ ë³¸ë¬¸ ë ë¶€ë¶„ì—ì„œ ê¸°ìëª…ì„ ì°¾ëŠ” ê²ƒì´ ë” ì •í™•
        article_end = full_text[-1500:]  # ë§ˆì§€ë§‰ 1500ìì—ì„œ ì°¾ê¸°

        for pattern in reporter_patterns:
            match = re.search(pattern, article_end)
            if match:
                reporter = match.group(1)
                reporter = re.sub(
                    r"ê¸°ì|íŠ¹íŒŒì›|ì•µì»¤|ì•„ë‚˜ìš´ì„œ|ì·¨ì¬|ê¸€|ì„ ì„ê¸°ì|ìˆ˜ì„ê¸°ì|ë…¼ì„¤ìœ„ì›|JTBC", "", reporter
                ).strip()
                if len(reporter) >= 2 and len(reporter) <= 4:
                    break

        # ë³¸ë¬¸ ì¶”ì¶œ - JTBC HTML êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
        content = ""

        # ë°©ë²• 1: JTBC ê¸°ì‚¬ ë³¸ë¬¸ êµ¬ì¡° ì°¾ê¸°
        content_selectors = [
            "div.article_content",  # JTBC ì£¼ìš” ê¸°ì‚¬ ë³¸ë¬¸ í´ë˜ìŠ¤
            'div[class*="article"]',  # article ê´€ë ¨ í´ë˜ìŠ¤
            'div[class*="content"]',  # content ê´€ë ¨ í´ë˜ìŠ¤
            'div[class*="news"]',  # news ê´€ë ¨ í´ë˜ìŠ¤
            'div[class*="text"]',  # text ê´€ë ¨ í´ë˜ìŠ¤
            "div.news_content",  # ë‰´ìŠ¤ ì»¨í…ì¸ 
            "div.view_content",  # ë·° ì»¨í…ì¸ 
            "article",  # article íƒœê·¸
            "main",  # main íƒœê·¸
            'div[id*="article"]',  # article ID ê´€ë ¨
            "div.bodycontent",  # ë°”ë”” ì»¨í…ì¸ 
            "div.story",  # ìŠ¤í† ë¦¬ ì»¨í…ì¸ 
            "div.article_body",  # ê¸°ì‚¬ ë³¸ë¬¸
            "div.articleView",  # ê¸°ì‚¬ ë·°
            "div.news_body",  # ë‰´ìŠ¤ ë°”ë””
        ]

        for selector in content_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text().strip()
                if len(text) > len(content):
                    content = text

        # ë°©ë²• 2: P íƒœê·¸ ê¸°ë°˜ ì¶”ì¶œ (JTBC íŠ¹ì„±ì— ë§ê²Œ ìˆ˜ì •)
        if len(content) < 200:
            paragraphs = soup.find_all("p")
            content_parts = []

            for p in paragraphs:
                text = p.get_text().strip()
                if (
                    len(text) > 20
                    and not re.search(r"ì…ë ¥\s*\d{4}|ìˆ˜ì •\s*\d{4}|Copyright|ì €ì‘ê¶Œ|JTBC|jtbc", text)
                    and not text.startswith(("â–¶", "â˜", "â€»", "â– ", "â–²", "[", "â€»", "â—†", "â—‹", "â–³"))
                    and "@jtbc.co.kr" not in text
                    and "ë¬´ë‹¨ ì „ì¬" not in text
                    and "ì¬ë°°í¬ ê¸ˆì§€" not in text
                    and "ê¸°ì‚¬ì œë³´" not in text
                    and "ë‰´ìŠ¤ë£¸" not in text
                ):
                    content_parts.append(text)

            if content_parts:
                content = " ".join(content_parts)

        # ë°©ë²• 3: id="ijam_content" ê¸°ë°˜ ì¶”ì¶œ (ìƒˆë¡œìš´ JTBC êµ¬ì¡°)
        if len(content) < 200:
            container = soup.find("div", id="ijam_content")
            if container:
                parts = []
                for tag in container.find_all(["p", "span"]):
                    text = tag.get_text().strip()
                    if text and not re.search(r"ADVERTISEMENT|AD|iframe|script", text):
                        parts.append(text)
                if parts:
                    content = " ".join(parts)
        # ë³¸ë¬¸ ì •ì œ
        content = clean_jtbc_content(content)

        # ì›ë¬¸ ìš°ì„ : ë³¸ë¬¸ì´ ë¹„ì–´ìˆê±°ë‚˜ ë§¤ìš° ì§§ì„ ë•Œë§Œ RSS ìš”ì•½ ì‚¬ìš©
        if rss_summary and len(content.strip()) < 20:
            content = rss_summary
            print(f"    RSS ìš”ì•½ ì±„íƒ (ë³¸ë¬¸ ë¶€ì¡±ìœ¼ë¡œ ëŒ€ì²´, ê¸¸ì´: {len(rss_summary)})")

        print(f"    ìµœì¢… ë³¸ë¬¸ ê¸¸ì´: {len(content)}")
        return reporter, content

    except Exception as e:
        print(f"    âŒ ì—ëŸ¬: {e}")
        return "", rss_summary if rss_summary else f"ì˜¤ë¥˜: {str(e)}"


def build_selenium_driver():
    """Headless Chrome ë“œë¼ì´ë²„ ìƒì„±"""
    ua = get_random_user_agent()
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1366,768")
    chrome_options.add_argument(f"--user-agent={ua}")
    chrome_prefs = {
        "profile.default_content_setting_values.notifications": 2,
        "profile.managed_default_content_settings.images": 2,  # ì´ë¯¸ì§€ ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
    }
    chrome_options.add_experimental_option("prefs", chrome_prefs)
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver


def extract_jtbc_article_content_selenium(driver, url, rss_summary=""):
    """Seleniumìœ¼ë¡œ JTBC ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ"""
    try:
        print(f"    [SELENIUM] ì ‘ì† ì‹œë„: {url[:80]}...")
        # JTBC ë©”ì¸ ë¨¼ì € ë°©ë¬¸ (ê°„ë‹¨í•œ ìš°íšŒ)
        try:
            driver.get("https://news-ex.jtbc.co.kr/")
            time.sleep(0.5)
        except Exception:
            pass

        driver.get(url)

        # ì£¼ìš” ì»¨í…Œì´ë„ˆ ëŒ€ê¸°
        wait = WebDriverWait(driver, 10)
        selectors = [
            (By.CSS_SELECTOR, "div#ijam_content"),
            (By.CSS_SELECTOR, "div.article_content"),
            (By.CSS_SELECTOR, 'div[class*="article"]'),
            (By.CSS_SELECTOR, "article"),
            (By.CSS_SELECTOR, 'div[class*="content"]'),
        ]

        found = False
        for by, sel in selectors:
            try:
                wait.until(EC.presence_of_element_located((by, sel)))
                found = True
                break
            except Exception:
                continue

        # ìŠ¤í¬ë¡¤ë¡œ ì§€ì—° ë¡œë”© ìš”ì†Œ ë¡œë“œ
        try:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
            time.sleep(0.3)
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(0.3)
        except Exception:
            pass

        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")

        # ê¸°ìëª…ì€ í†µì¼í•´ì„œ ì‚¬ìš©í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë³¸ë¬¸ë§Œ ì¶”ì¶œ
        content = ""

        # ìš°ì„ : ijam_content
        container = soup.select_one("div#ijam_content")
        if container:
            parts = []
            for tag in container.find_all(["p", "span"]):
                text = tag.get_text().strip()
                if text and not re.search(r"ADVERTISEMENT|AD|iframe|script", text, flags=re.I):
                    parts.append(text)
            if parts:
                content = " ".join(parts)

        # ë³´ì¡°: ê¸°íƒ€ ê¸°ì‚¬ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆë“¤
        if len(content) < 200:
            content_selectors = [
                "div.article_content",
                'div[class*="article"]',
                'div[class*="content"]',
                'div[class*="news"]',
                'div[class*="text"]',
                "div.news_content",
                "div.view_content",
                "article",
                "main",
                'div[id*="article"]',
                "div.bodycontent",
                "div.story",
                "div.article_body",
                "div.articleView",
                "div.news_body",
            ]
            best = ""
            for sel in content_selectors:
                for el in soup.select(sel):
                    txt = el.get_text().strip()
                    if len(txt) > len(best):
                        best = txt
            if best:
                content = best

        # P íƒœê·¸ ë°±ì—… ìˆ˜ì§‘
        if len(content) < 200:
            parts = []
            for p in soup.find_all("p"):
                text = p.get_text().strip()
                if (
                    len(text) > 20
                    and not re.search(r"ì…ë ¥\s*\d{4}|ìˆ˜ì •\s*\d{4}|Copyright|ì €ì‘ê¶Œ|JTBC|jtbc", text)
                    and not text.startswith(("â–¶", "â˜", "â€»", "â– ", "â–²", "[", "â€»", "â—†", "â—‹", "â–³"))
                    and "@jtbc.co.kr" not in text
                    and "ë¬´ë‹¨ ì „ì¬" not in text
                    and "ì¬ë°°í¬ ê¸ˆì§€" not in text
                    and "ê¸°ì‚¬ì œë³´" not in text
                    and "ë‰´ìŠ¤ë£¸" not in text
                ):
                    parts.append(text)
            if parts:
                content = " ".join(parts)

        content = clean_jtbc_content(content)

        if rss_summary and len(content.strip()) < 20:
            content = rss_summary
            print(f"    RSS ìš”ì•½ ì±„íƒ (ë³¸ë¬¸ ë¶€ì¡±ìœ¼ë¡œ ëŒ€ì²´, ê¸¸ì´: {len(rss_summary)})")

        print(f"    [SELENIUM] ìµœì¢… ë³¸ë¬¸ ê¸¸ì´: {len(content)}")
        return "jtbc", content
    except Exception as e:
        print(f"    âŒ [SELENIUM] ì—ëŸ¬: {e}")
        return "jtbc", rss_summary if rss_summary else f"ì˜¤ë¥˜: {str(e)}"


def clean_jtbc_content(content):
    """JTBC ê¸°ì‚¬ ë³¸ë¬¸ ì •ì œ"""
    if not content:
        return ""

    # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ë“¤ ì œê±° - JTBC íŠ¹ì„±ì— ë§ê²Œ ìˆ˜ì •
    remove_patterns = [
        r"ì…ë ¥\s*\d{4}[-./]\d{2}[-./]\d{2}.*?\d{2}:\d{2}",
        r"ìˆ˜ì •\s*\d{4}[-./]\d{2}[-./]\d{2}.*?\d{2}:\d{2}",
        r"ì—…ë°ì´íŠ¸\s*\d{4}[-./]\d{2}[-./]\d{2}.*?\d{2}:\d{2}",
        r"JTBC.*ë¬´ë‹¨.*ì „ì¬.*ê¸ˆì§€",
        r"ë¬´ë‹¨.*ì „ì¬.*ì¬ë°°í¬.*ê¸ˆì§€",
        r"ì €ì‘ê¶Œ.*JTBC",
        r"ê´€ë ¨ê¸°ì‚¬.*ë”ë³´ê¸°",
        r"í˜ì´ìŠ¤ë¶.*íŠ¸ìœ„í„°.*ì¹´ì¹´ì˜¤",
        r"êµ¬ë….*ì‹ ì²­",
        r"ê´‘ê³ ",
        r"[ê°€-í£]{2,4}\s*ê¸°ì\s*[a-zA-Z0-9_.+-]+@jtbc\.co\.kr",  # ê¸°ì ì´ë©”ì¼ ì œê±°
        r"ì—°í•©ë‰´ìŠ¤.*ì œê³µ",  # ë‰´ìŠ¤ ì¶œì²˜ ì œê±°
        r"ë‰´ì‹œìŠ¤.*ì œê³µ",  # ë‰´ìŠ¤ ì¶œì²˜ ì œê±°
        r"JTBC.*ì œê³µ",  # ì‚¬ì§„ ì¶œì²˜ ì œê±°
        r"â“’.*JTBC",
        r"jtbc\.co\.kr",
        r"ê¸°ì‚¬ì œë³´.*ë¬¸ì˜",
        r"ë…ìíˆ¬ê³ .*ë¬¸ì˜",
        r"ì²­ì†Œë…„.*ë³´í˜¸.*ì±…ì„ì",
        r"ê°œì¸ì •ë³´.*ì²˜ë¦¬.*ë°©ì¹¨",
        r"ì´ë©”ì¼.*ë¬´ë‹¨.*ìˆ˜ì§‘.*ê±°ë¶€",
        r"Copyright.*\d{4}.*JTBC",
        r"ë‰´ìŠ¤ë£¸.*ì•µì»¤",
        r"ì´ ì‹œê°.*ë‰´ìŠ¤",
        r"JTBC.*ë‰´ìŠ¤",
    ]

    for pattern in remove_patterns:
        content = re.sub(pattern, "", content, flags=re.IGNORECASE)

    # ê³µë°± ì •ë¦¬
    content = re.sub(r"\s+", " ", content).strip()

    # ê¸¸ì´ ì œí•œ
    if len(content) > 1800:
        content = content[:1800] + "..."

    return content


def fetch_jtbc_rss_to_csv(rss_url, output_file, max_articles=30):
    """JTBC RSSë¥¼ íŒŒì‹±í•˜ì—¬ CSVë¡œ ì €ì¥"""

    print(f"JTBC RSS í”¼ë“œ íŒŒì‹± ì¤‘: {rss_url}")

    # RSS íŒŒì‹±
    try:
        headers = {"User-Agent": get_random_user_agent()}
        response = requests.get(rss_url, headers=headers, timeout=10)
        # JTBC RSSëŠ” UTF-8 ì¸ì½”ë”© ì‚¬ìš©
        response.encoding = "utf-8"
        feed = feedparser.parse(response.content)
    except:
        feed = feedparser.parse(rss_url)

    if not feed.entries:
        print("âŒ RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"âœ… RSSì—ì„œ {len(feed.entries)}ê°œ ê¸°ì‚¬ ë°œê²¬")

    success_count = 0
    total_count = min(len(feed.entries), max_articles)

    # CSV íŒŒì¼ ìƒì„±
    with open(output_file, "w", newline="", encoding="utf-8-sig") as csvfile:
        fieldnames = ["ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        print(f"ì´ {total_count}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘...\n")

        for i, entry in enumerate(feed.entries[:max_articles]):
            try:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = entry.title.strip()
                title = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", title)

                link = entry.link

                # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (JTBC RSS êµ¬ì¡°ì— ë§ê²Œ)
                category = ""
                if hasattr(entry, "category"):
                    category = entry.category.strip()
                elif hasattr(entry, "tags") and entry.tags:
                    category = entry.tags[0].term if entry.tags else ""

                # URLì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì‹œë„ (JTBC URL êµ¬ì¡° ê¸°ë°˜)
                if not category:
                    url_category_map = {
                        # ì£¼ìš”ë‰´ìŠ¤
                        "newsflash": "ì†ë³´",
                        "issue": "ì´ìŠˆ Top10",
                        # ë¶„ì•¼ë³„ ë‰´ìŠ¤
                        "politics": "ì •ì¹˜",
                        "economy": "ê²½ì œ",
                        "society": "ì‚¬íšŒ",
                        "international": "êµ­ì œ",
                        "culture": "ë¬¸í™”",
                        "sports": "ìŠ¤í¬ì¸ ",
                        "entertainment": "ì—°ì˜ˆ",
                        "weather": "ë‚ ì”¨",
                    }

                    for url_part, cat_name in url_category_map.items():
                        if url_part in rss_url:
                            category = cat_name
                            break

                # RSS ìš”ì•½ ì •ë³´ ì¶”ì¶œ
                summary = ""
                if hasattr(entry, "description"):
                    summary = entry.description.strip()
                    # HTML íƒœê·¸ì™€ CDATA ì œê±°
                    summary = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", summary, flags=re.DOTALL)
                    summary = re.sub(r"<[^>]+>", "", summary)  # HTML íƒœê·¸ ì œê±°
                    summary = clean_jtbc_content(summary)

                # ë‚ ì§œ í˜•ì‹ ë³€í™˜
                if hasattr(entry, "published_parsed") and entry.published_parsed:
                    date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S")
                else:
                    date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                print(f"[{i+1}/{total_count}] {title[:50]}...")

                # ê¸°ì‚¬ ë³¸ë¬¸ ë° ê¸°ìëª… ì¶”ì¶œ
                reporter, content = extract_jtbc_article_content(link, summary)

                # ìµœì†Œ ì¡°ê±´ í™•ì¸
                if len(content.strip()) < 20:
                    print(f"    âš  ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ ê±´ë„ˆëœ€ (ê¸¸ì´: {len(content)})\n")
                    continue

                # CSVì— ì“°ê¸°
                writer.writerow(
                    {
                        "ì œëª©": title,
                        "ë‚ ì§œ": date,
                        "ì¹´í…Œê³ ë¦¬": category if category else "ë¯¸ë¶„ë¥˜",
                        "ê¸°ìëª…": reporter if reporter else "ë¯¸ìƒ",
                        "ë³¸ë¬¸": content,
                    }
                )

                success_count += 1
                print(
                    f"    âœ… ì„±ê³µ! (ì¹´í…Œê³ ë¦¬: {category}, ê¸°ì: {reporter if reporter else 'ë¯¸ìƒ'}, ë³¸ë¬¸: {len(content)}ì)"
                )

                # ì§„í–‰ë¥  í‘œì‹œ
                if (i + 1) % 5 == 0:
                    print(f"\nğŸ“Š ì§„í–‰ë¥ : {i+1}/{total_count} ({(i+1)/total_count*100:.1f}%)")
                    print(f"ğŸ“ˆ ì„±ê³µë¥ : {success_count}/{i+1} ({success_count/(i+1)*100:.1f}%)\n")

                # ëœë¤ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                delay = random.uniform(2.0, 4.0)  # JTBCëŠ” ì¡°ê¸ˆ ë” ê¸´ ë”œë ˆì´
                time.sleep(delay)

            except KeyboardInterrupt:
                print("\nâš  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"    âŒ ì˜¤ë¥˜: {e}")
                continue

        print(f"\n{'='*70}")
        print(f"ğŸ‰ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥: {output_file}")
        print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {success_count}/{total_count}ê°œ ì„±ê³µ ({success_count/total_count*100:.1f}%)")
        print(f"{'='*70}")


# ë‹¨ì¼ CSV writerì— ëˆ„ì  ì €ì¥í•˜ëŠ” ë²„ì „ (ì–¸ë¡ ì‚¬/ê¸°ìëª… í†µì¼)
def fetch_jtbc_rss_to_writer(rss_url, writer, max_articles=30, media_name="jtbc", category_label=None):
    """JTBC RSSë¥¼ íŒŒì‹±í•˜ì—¬ ì „ë‹¬ëœ writerì— ëˆ„ì  ì €ì¥ (ì–¸ë¡ ì‚¬, ì œëª©, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, ê¸°ìëª…, ë³¸ë¬¸)"""

    print(f"JTBC RSS í”¼ë“œ íŒŒì‹± ì¤‘: {rss_url}")

    # RSS íŒŒì‹±
    try:
        headers = {"User-Agent": get_random_user_agent()}
        response = requests.get(rss_url, headers=headers, timeout=10)
        response.encoding = "utf-8"
        feed = feedparser.parse(response.content)
    except Exception:
        feed = feedparser.parse(rss_url)

    if not feed.entries:
        print("âŒ RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    print(f"âœ… RSSì—ì„œ {len(feed.entries)}ê°œ ê¸°ì‚¬ ë°œê²¬")

    success_count = 0
    total_count = min(len(feed.entries), max_articles)
    print(f"ì´ {total_count}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘...\n")

    # ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ ë™ì•ˆ í•˜ë‚˜ì˜ ë“œë¼ì´ë²„ë¥¼ ì¬ì‚¬ìš©
    driver = build_selenium_driver()
    try:
        for i, entry in enumerate(feed.entries[:max_articles]):
            try:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = entry.title.strip()
                title = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", title)

                link = entry.link

                # ì¹´í…Œê³ ë¦¬ ê²°ì •
                category = category_label or ""
                if not category:
                    if hasattr(entry, "category"):
                        category = entry.category.strip()
                    elif hasattr(entry, "tags") and entry.tags:
                        category = entry.tags[0].term if entry.tags else ""
                if not category:
                    url_category_map = {
                        "politics": "ì •ì¹˜",
                        "economy": "ê²½ì œ",
                        "society": "ì‚¬íšŒ",
                        "international": "êµ­ì œ",
                        "culture": "ë¬¸í™”",
                    }
                    for url_part, cat_name in url_category_map.items():
                        if url_part in rss_url:
                            category = cat_name
                            break
                if not category:
                    category = "ë¯¸ë¶„ë¥˜"

                # RSS ìš”ì•½ ì •ë³´ ì¶”ì¶œ
                summary = ""
                if hasattr(entry, "description"):
                    summary = entry.description.strip()
                    summary = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", summary, flags=re.DOTALL)
                    summary = re.sub(r"<[^>]+>", "", summary)
                    summary = clean_jtbc_content(summary)

                # ë‚ ì§œ í˜•ì‹ ë³€í™˜
                if hasattr(entry, "published_parsed") and entry.published_parsed:
                    date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S")
                else:
                    date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                print(f"[{i+1}/{total_count}] {title[:50]}...")

                # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (Selenium)
                _reporter, content = extract_jtbc_article_content_selenium(driver, link, summary)

                if len(content.strip()) < 20:
                    print(f"    âš  ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ ê±´ë„ˆëœ€ (ê¸¸ì´: {len(content)})\n")
                    continue

                writer.writerow(
                    {
                        "ì–¸ë¡ ì‚¬": media_name,
                        "ì œëª©": title,
                        "ë‚ ì§œ": date,
                        "ì¹´í…Œê³ ë¦¬": category,
                        "ê¸°ìëª…": "jtbc",
                        "ë³¸ë¬¸": content,
                    }
                )

                success_count += 1
                print(f"    âœ… ì„±ê³µ! (ì¹´í…Œê³ ë¦¬: {category}, ê¸°ì: jtbc, ë³¸ë¬¸: {len(content)}ì)")

                if (i + 1) % 5 == 0:
                    print(f"\nğŸ“Š ì§„í–‰ë¥ : {i+1}/{total_count} ({(i+1)/total_count*100:.1f}%)")
                    print(f"ğŸ“ˆ ì„±ê³µë¥ : {success_count}/{i+1} ({success_count/(i+1)*100:.1f}%)\n")

                time.sleep(random.uniform(1.0, 2.0))

            except KeyboardInterrupt:
                print("\nâš  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"    âŒ ì˜¤ë¥˜: {e}")
                continue
    finally:
        try:
            driver.quit()
        except Exception:
            pass

    print(f"\n{'='*70}")
    print(f"ğŸ‰ ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì™„ë£Œ (ì´ ì„±ê³µ: {success_count}/{total_count})")
    print(f"{'='*70}")
    return success_count


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ì™€ RSS URL
    jtbc_rss_options = {
        "ì •ì¹˜": "https://news-ex.jtbc.co.kr/v1/get/rss/section/politics",
        "ê²½ì œ": "https://news-ex.jtbc.co.kr/v1/get/rss/section/economy",
        "ì‚¬íšŒ": "https://news-ex.jtbc.co.kr/v1/get/rss/section/society",
        "êµ­ì œ": "https://news-ex.jtbc.co.kr/v1/get/rss/section/international",
        "ë¬¸í™”": "https://news-ex.jtbc.co.kr/v1/get/rss/section/culture",
    }
    categories = ["ì •ì¹˜", "ê²½ì œ", "ì‚¬íšŒ", "êµ­ì œ", "ë¬¸í™”"]
    max_articles = 20
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"results/jtbc_ì „ì²´_{timestamp}.csv"

    print("\n" + "=" * 50)
    print(f"ğŸš€ 5ê°œ ì¹´í…Œê³ ë¦¬ì—ì„œ ê° {max_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘!")
    print(f"ğŸ“ ì €ì¥ íŒŒì¼: {output_file}\n")

    total_success = 0
    with open(output_file, "w", newline="", encoding="utf-8-sig") as csvfile:
        fieldnames = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        for category in categories:
            rss_url = jtbc_rss_options[category]
            print("\n" + "-" * 40)
            print(f"â–¶ {category} ì²˜ë¦¬ ì‹œì‘")
            total_success += fetch_jtbc_rss_to_writer(
                rss_url, writer, max_articles=max_articles, media_name="jtbc", category_label=category
            )

    print("\nâœ… ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ")
    print(f"ğŸ“ˆ ì´ ìˆ˜ì§‘ ì„±ê³µ ê¸°ì‚¬ ìˆ˜: {total_success}")
