import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import csv
import time
import random
import logging
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse


class BusanFinancialNewsCrawler:
    def __init__(self):
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler("busan_financial_news_rss_crawler.log", encoding="utf-8"),
                logging.StreamHandler(),
            ],
        )
        self.logger = logging.getLogger(__name__)

        # ë¶€ì‚°íŒŒì´ë‚¸ì…œë‰´ìŠ¤ RSS í”¼ë“œ ëª©ë¡
        self.rss_feeds = {
            "ì „ì²´ê¸°ì‚¬": "http://www.fnnews.com/rss/r20/fn_realnews_all.xml",
            "ì •ì¹˜": "http://www.fnnews.com/rss/r20/fn_realnews_politics.xml",
            "êµ­ì œ": "http://www.fnnews.com/rss/r20/fn_realnews_international.xml",
            "ì‚¬íšŒ": "http://www.fnnews.com/rss/r20/fn_realnews_society.xml",
            "ê²½ì œ": "http://www.fnnews.com/rss/r20/fn_realnews_economy.xml",
            "ì¦ê¶Œ": "http://www.fnnews.com/rss/r20/fn_realnews_stock.xml",
            "ê¸ˆìœµ": "http://www.fnnews.com/rss/r20/fn_realnews_finance.xml",
            "ë¶€ë™ì‚°": "http://www.fnnews.com/rss/r20/fn_realnews_realestate.xml",
            "ì‚°ì—…": "http://www.fnnews.com/rss/r20/fn_realnews_industry.xml",
            "IT": "http://www.fnnews.com/rss/r20/fn_realnews_it.xml",
            "ì‚¬ì„¤ì¹¼ëŸ¼": "http://www.fnnews.com/rss/r20/fn_realnews_column.xml",
        }

        # User-Agent ëª©ë¡
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.59",
        ]

        self.articles = []

    def get_random_headers(self):
        """ëœë¤ User-Agent í—¤ë” ë°˜í™˜"""
        return {
            "User-Agent": random.choice(self.user_agents),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }

    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜"""
        if not text:
            return ""

        # CDATA íƒœê·¸ ì œê±°
        text = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", text, flags=re.DOTALL)

        # HTML íƒœê·¸ ì œê±°
        soup = BeautifulSoup(text, "html.parser")
        text = soup.get_text()

        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€í™˜
        text = re.sub(r"\s+", " ", text)

        return text.strip()

    def extract_reporter_name(self, content):
        """ê¸°ì‚¬ ë‚´ìš©ì—ì„œ ê¸°ìëª… ì¶”ì¶œ"""
        if not content:
            return "ê¸°ìëª… ì—†ìŒ"

        # ë¶€ì‚°íŒŒì´ë‚¸ì…œë‰´ìŠ¤ ê¸°ìëª… íŒ¨í„´ë“¤
        patterns = [
            r"([ê°€-í£]{2,4})\s*ê¸°ì",
            r"ê¸°ì\s*([ê°€-í£]{2,4})",
            r"([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›",
            r"([ê°€-í£]{2,4})\s*ë…¼ì„¤ìœ„ì›",
            r"([ê°€-í£]{2,4})\s*í¸ì§‘ìœ„ì›",
            r"íŒŒì´ë‚¸ì…œë‰´ìŠ¤\s*([ê°€-í£]{2,4})",
            r"FN\s*([ê°€-í£]{2,4})",
            r"ë¦¬í¬í„°\s*([ê°€-í£]{2,4})",
            r"([ê°€-í£]{2,4})\s*ë¦¬í¬í„°",
        ]

        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                return match.group(1) + " ê¸°ì"

        return "ê¸°ìëª… ì—†ìŒ"

    def format_date(self, date_str):
        """ë‚ ì§œ í˜•ì‹ ë³€í™˜"""
        try:
            # RFC 2822 í˜•ì‹ íŒŒì‹± ì‹œë„
            dt = datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S %z")
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        except:
            try:
                # GMT ì—†ëŠ” í˜•ì‹ ì‹œë„
                dt = datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S GMT")
                return dt.strftime("%Y-%m-%d %H:%M:%S")
            except:
                try:
                    # KST í˜•ì‹ ì‹œë„
                    dt = datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S KST")
                    return dt.strftime("%Y-%m-%d %H:%M:%S")
                except:
                    try:
                        # ë‹¤ë¥¸ í˜•ì‹ ì‹œë„
                        dt = datetime.strptime(date_str[:19], "%Y-%m-%d %H:%M:%S")
                        return dt.strftime("%Y-%m-%d %H:%M:%S")
                    except:
                        return date_str

    def get_article_content(self, url):
        """ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            headers = self.get_random_headers()
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            # ë¶€ì‚°íŒŒì´ë‚¸ì…œë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ì…€ë ‰í„°ë“¤
            content_selectors = [
                "div.cont_view#article_content[itemprop='articleBody']",
                "div.article_content",
                "div.news_view_detail",
                "div.article_txt",
                "div.view_txt",
                ".news_cnt_detail_wrap",
                "div.news_text",
                "div.article-body",
                ".article_wrap .content",
                "div.story",
                ".detail_story",
                "div.article_view",
                ".view_cont",
            ]

            content = ""
            for selector in content_selectors:
                content_element = soup.select_one(selector)
                if content_element:
                    content = content_element.get_text()
                    break

            if not content:
                # ê¸°ë³¸ì ì¸ p íƒœê·¸ì—ì„œ ë‚´ìš© ì¶”ì¶œ
                paragraphs = soup.find_all("p")
                content = " ".join([p.get_text() for p in paragraphs])

            # ë‚´ìš© ì •ì œ
            content = self.clean_text(content)

            return content

        except Exception as e:
            self.logger.warning(f"ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
            return "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"

    def parse_rss_feed(self, category, url):
        """RSS í”¼ë“œ íŒŒì‹±"""
        try:
            self.logger.info(f"{category} RSS í”¼ë“œ í¬ë¡¤ë§ ì‹œì‘: {url}")

            headers = self.get_random_headers()
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()

            # ì‘ë‹µì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if not response.content.strip():
                self.logger.error(f"{category} RSS í”¼ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {url}")
                return []

            # XML íŒŒì‹±
            try:
                root = ET.fromstring(response.content)
            except ET.ParseError as e:
                self.logger.error(f"{category} RSS XML íŒŒì‹± ì˜¤ë¥˜: {e}")
                return []

            # RSS ì•„ì´í…œ ì¶”ì¶œ
            items = root.findall(".//item")

            if not items:
                self.logger.warning(f"{category} RSS í”¼ë“œì—ì„œ ì•„ì´í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []

            articles = []

            # ìµœëŒ€ 20ê°œ ê¸°ì‚¬ë§Œ ì²˜ë¦¬
            for i, item in enumerate(items[:1]):
                try:
                    title_elem = item.find("title")
                    link_elem = item.find("link")
                    pubdate_elem = item.find("pubDate")
                    description_elem = item.find("description")

                    if title_elem is None or link_elem is None:
                        continue

                    title = self.clean_text(title_elem.text)
                    link = link_elem.text.strip()
                    pub_date = self.format_date(pubdate_elem.text) if pubdate_elem is not None else ""
                    description = self.clean_text(description_elem.text) if description_elem is not None else ""

                    # ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
                    self.logger.info(f"ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì¤‘: {title[:30]}...")
                    content = self.get_article_content(link)

                    # ê¸°ìëª… ì¶”ì¶œ: RSS author ìš°ì„  ì‚¬ìš©
                    author_elem = item.find("author")
                    if author_elem is not None and author_elem.text:
                        author_text = self.clean_text(author_elem.text)
                        match = re.search(r"([ê°€-í£]{2,4})", author_text)
                        if match:
                            reporter = match.group(1) + " ê¸°ì"
                        else:
                            reporter = author_text
                    else:
                        reporter = self.extract_reporter_name(content)

                    article = {
                        "source": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
                        "title": title,
                        "date": pub_date,
                        "category": category,
                        "reporter": reporter,
                        "content": content,
                    }

                    articles.append(article)

                    # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
                    time.sleep(random.uniform(2, 5))

                except Exception as e:
                    self.logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue

            self.logger.info(f"{category} ì¹´í…Œê³ ë¦¬ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
            return articles

        except requests.RequestException as e:
            self.logger.error(f"{category} RSS í”¼ë“œ ìš”ì²­ ì˜¤ë¥˜: {e}")
            return []
        except Exception as e:
            self.logger.error(f"{category} RSS í”¼ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    def crawl_all_feeds(self):
        """ëª¨ë“  RSS í”¼ë“œ í¬ë¡¤ë§"""
        self.logger.info("ë¶€ì‚°íŒŒì´ë‚¸ì…œë‰´ìŠ¤ RSS í¬ë¡¤ë§ ì‹œì‘")

        # ì£¼ìš” ì¹´í…Œê³ ë¦¬ë§Œ ì„ ë³„í•˜ì—¬ í¬ë¡¤ë§ (ì „ì²´ê¸°ì‚¬ ì œì™¸)
        main_categories = [
            "ì •ì¹˜",
            "ê²½ì œ",
            "ì¦ê¶Œ",
            "ê¸ˆìœµ",
            "ë¶€ë™ì‚°",
            "ì‚°ì—…",
            "IT",
            "ì‚¬íšŒ",
            "êµ­ì œ",
            "ì—°ì˜ˆ",
            "ìŠ¤í¬ì¸ ",
            "ë¬¸í™”",
        ]

        for category in main_categories:
            if category in self.rss_feeds:
                url = self.rss_feeds[category]
                self.logger.info(f"\n=== {category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ===")

                articles = self.parse_rss_feed(category, url)
                self.articles.extend(articles)

                # ì¹´í…Œê³ ë¦¬ ê°„ ë”œë ˆì´
                time.sleep(random.uniform(3, 7))

        self.logger.info(f"\nì´ {len(self.articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")

    def save_to_csv(self, filename=None):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"results/íŒŒì´ë‚¸ì…œë‰´ìŠ¤_ì „ì²´_{timestamp}.csv"

        try:
            with open(filename, "w", newline="", encoding="utf-8-sig") as csvfile:
                fieldnames = ["source", "title", "date", "category", "reporter", "content"]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

                writer.writeheader()
                writer.writerows(self.articles)

            self.logger.info(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {filename}")
            print(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(self.articles)}ê°œ")

        except Exception as e:
            self.logger.error(f"CSV íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = BusanFinancialNewsCrawler()

    print("ğŸš€ ë¶€ì‚°íŒŒì´ë‚¸ì…œë‰´ìŠ¤ RSS í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ğŸ“° ìˆ˜ì§‘ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬: ì •ì¹˜, ê²½ì œ, ì¦ê¶Œ, ê¸ˆìœµ, ë¶€ë™ì‚°, ì‚°ì—…, IT, ì‚¬íšŒ, êµ­ì œ, ì—°ì˜ˆ, ìŠ¤í¬ì¸ , ë¬¸í™”")
    print("â³ ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•´ ë”œë ˆì´ê°€ ì ìš©ë©ë‹ˆë‹¤.\n")

    try:
        # RSS í”¼ë“œ í¬ë¡¤ë§
        crawler.crawl_all_feeds()

        # CSV íŒŒì¼ë¡œ ì €ì¥
        crawler.save_to_csv()

        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        if crawler.articles:
            print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ í˜„í™©:")
            category_count = {}
            for article in crawler.articles:
                category = article["category"]
                category_count[category] = category_count.get(category, 0) + 1

            for category, count in category_count.items():
                print(f"   â€¢ {category}: {count}ê°œ")

    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.error(f"ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    main()
