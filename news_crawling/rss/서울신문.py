import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
from datetime import datetime
import time
import os
from urllib.parse import urljoin, urlparse
import logging
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class SeoulShinmunCrawler:
    def __init__(self):
        self.base_url = "https://www.seoul.co.kr"
        self.sections = {
            "politics": {"url": "https://www.seoul.co.kr/newsList/politics/", "name": "ì •ì¹˜"},
            "society": {"url": "https://www.seoul.co.kr/newsList/society/", "name": "ì‚¬íšŒ"},
            "economy": {"url": "https://www.seoul.co.kr/newsList/economy/", "name": "ê²½ì œ"},
            "international": {"url": "https://www.seoul.co.kr/newsList/international/", "name": "êµ­ì œ"},
            "life": {"url": "https://www.seoul.co.kr/newsList/life/", "name": "ìƒí™œ"},
        }
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }

    def get_page_content(self, url, timeout=10):
        """ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
        # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ì‹œ ìµœëŒ€ 3íšŒ ì¬ì‹œë„
        for attempt in range(3):
            try:
                response = requests.get(url, headers=self.headers, timeout=timeout)
                response.raise_for_status()
                response.encoding = "utf-8"
                return response.text
            except requests.RequestException as e:
                logger.warning(f"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨({attempt+1}/3) ({url}): {e}")
                time.sleep(1)
        logger.error(f"í˜ì´ì§€ ìš”ì²­ ì¬ì‹œë„ ì‹¤íŒ¨ ({url})")
        return None

    def extract_article_urls_from_section(self, section_url, max_pages=10):
        """ì„¹ì…˜ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ URLë“¤ì„ ì¶”ì¶œ(ìµœëŒ€ max_pages)"""
        # ìµœëŒ€ max_pagesí˜ì´ì§€ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
        article_urls = []
        for page in range(1, max_pages + 1):
            if page == 1:
                url = section_url
            else:
                url = f"{section_url}?page={page}"
            page_content = self.get_page_content(url)
            if not page_content:
                break
            soup = BeautifulSoup(page_content, "html.parser")
            list_wrap = soup.find("ul", class_="sectionContentWrap")
            if not list_wrap:
                break
            items = list_wrap.find_all("li", class_="newsBox_row1")
            if not items:
                break
            for li in items:
                a_tag = li.find("a", href=True)
                if a_tag:
                    full_url = urljoin(self.base_url, a_tag["href"])
                    if full_url not in article_urls:
                        article_urls.append(full_url)
        logger.info(f"ì¶”ì¶œëœ ê¸°ì‚¬ URL ìˆ˜: {len(article_urls)}")
        return article_urls

    def extract_article_info(self, article_url, section_name):
        """ê°œë³„ ê¸°ì‚¬ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        content = self.get_page_content(article_url)
        if not content:
            return None

        try:
            soup = BeautifulSoup(content, "html.parser")

            # ì œëª© ì¶”ì¶œ
            title = ""
            title_selectors = ["h1.title", "h1", ".article-title", ".news-title", "h2.title", "h2"]

            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text().strip()
                    if title:
                        break

            # í…ìŠ¤íŠ¸ì—ì„œ ì œëª© ì¶”ì¶œ ì‹œë„ (ë©”íƒ€ íƒœê·¸)
            if not title:
                title_meta = soup.find("meta", {"property": "og:title"})
                if title_meta:
                    title = title_meta.get("content", "").strip()

            # ë‚ ì§œ ì¶”ì¶œ
            date = ""
            # URLì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
            date_match = re.search(r"id=(\d{8})", article_url)
            if date_match:
                date_str = date_match.group(1)
                try:
                    date_obj = datetime.strptime(date_str, "%Y%m%d")
                    date = date_obj.strftime("%Y-%m-%d")
                except:
                    pass

            # HTMLì—ì„œ ë‚ ì§œ ì°¾ê¸°
            if not date:
                date_patterns = [r"(\d{4}-\d{2}-\d{2})", r"(\d{4}\.\d{2}\.\d{2})", r"(\d{4}/\d{2}/\d{2})"]

                for pattern in date_patterns:
                    date_match = re.search(pattern, content)
                    if date_match:
                        date = date_match.group(1)
                        break

            # ê¸°ìëª… ì¶”ì¶œ
            author = ""
            author_patterns = [r"([ê°€-í£]{2,4})\s*ê¸°ì", r"ê¸°ì\s*([ê°€-í£]{2,4})", r"([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›"]

            for pattern in author_patterns:
                author_match = re.search(pattern, content)
                if author_match:
                    author = author_match.group(1)
                    break

            # ë³¸ë¬¸ ì¶”ì¶œ
            article_text = ""
            # ë³¸ë¬¸ ì¶”ì¶œ ìš°ì„ : viewContent body18 color700
            view_elem = soup.select_one("div.viewContent.body18.color700")
            if view_elem:
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                for tag in view_elem.find_all(["script", "style", "aside", "nav", "iframe", "figure"]):
                    tag.decompose()
                article_text = view_elem.get_text(separator=" ", strip=True)
            else:
                # ê¸°ì¡´ ë³¸ë¬¸ ì¶”ì¶œ ë¡œì§
                article_text = ""
                article_selectors = [
                    "div.viewContent.body18.color700",
                    ".article-content",
                    ".news-content",
                    ".content",
                    "#article_txt",
                    ".article_txt",
                ]

                for selector in article_selectors:
                    article_elem = soup.select_one(selector)
                    if article_elem:
                        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                        for tag in article_elem.find_all(["script", "style", "aside", "nav"]):
                            tag.decompose()

                        article_text = article_elem.get_text().strip()
                        if article_text:
                            break

            # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
            if not article_text:
                # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
                for tag in soup.find_all(["script", "style", "nav", "footer", "header"]):
                    tag.decompose()

                article_text = soup.get_text()
                # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
                article_text = re.sub(r"\s+", " ", article_text).strip()

                # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì ì ˆíˆ ìë¥´ê¸°
                if len(article_text) > 5000:
                    article_text = article_text[:5000] + "..."

            return {
                "ì–¸ë¡ ì‚¬ëª…": "ì„œìš¸ì‹ ë¬¸",
                "ì¹´í…Œê³ ë¦¬": section_name,
                "ì œëª©": title,
                "ë‚ ì§œ": date,
                "ê¸°ìëª…": author,
                "ë³¸ë¬¸": article_text,
                "URL": article_url,
            }

        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ ({article_url}): {e}")
            return None

    def crawl_section(self, section_key, max_pages=10):
        """íŠ¹ì • ì„¹ì…˜ í¬ë¡¤ë§"""
        if section_key not in self.sections:
            logger.error(f"ì•Œ ìˆ˜ ì—†ëŠ” ì„¹ì…˜: {section_key}")
            return []

        section = self.sections[section_key]
        section_name = section["name"]
        section_url = section["url"]

        logger.info(f"[{section_name}] ì„¹ì…˜ í¬ë¡¤ë§ ì‹œì‘")

        # ê¸°ì‚¬ URLë“¤ ì¶”ì¶œ
        article_urls = self.extract_article_urls_from_section(section_url, max_pages)

        if not article_urls:
            logger.warning(f"[{section_name}] ê¸°ì‚¬ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        articles = []

        for i, url in enumerate(article_urls, 1):
            logger.info(f"[{section_name}] ê¸°ì‚¬ {i}/{len(article_urls)} ì²˜ë¦¬ ì¤‘...")

            article_info = self.extract_article_info(url, section_name)

            if article_info and article_info["ì œëª©"]:
                articles.append(article_info)
                logger.info(f"ê¸°ì‚¬ ì¶”ì¶œ ì™„ë£Œ: {article_info['ì œëª©'][:50]}...")
            else:
                logger.warning(f"ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {url}")

            # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            time.sleep(0.5)

        logger.info(f"[{section_name}] ì„¹ì…˜ í¬ë¡¤ë§ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬")
        return articles

    def crawl_all_sections(self, sections_to_crawl=None, max_articles_per_section=20):
        """ëª¨ë“  ì„¹ì…˜ í¬ë¡¤ë§"""
        if sections_to_crawl is None:
            sections_to_crawl = list(self.sections.keys())

        logger.info(f"í¬ë¡¤ë§ ëŒ€ìƒ ì„¹ì…˜: {', '.join([self.sections[s]['name'] for s in sections_to_crawl])}")

        all_articles = []
        section_results = {}

        for section_key in sections_to_crawl:
            try:
                articles = self.crawl_section(section_key, max_articles_per_section)
                all_articles.extend(articles)
                section_results[self.sections[section_key]["name"]] = len(articles)

                # ì„¹ì…˜ ê°„ ê°„ê²©
                time.sleep(1)

            except Exception as e:
                logger.error(f"[{self.sections[section_key]['name']}] ì„¹ì…˜ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                section_results[self.sections[section_key]["name"]] = 0

        return all_articles, section_results

    def save_to_csv(self, articles, filename=None, split_by_section=False):
        """ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not articles:
            logger.warning("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        df = pd.DataFrame(articles)

        # ì—´ ìˆœì„œ ì •ë¦¬ (URL ì œì™¸)
        column_order = ["ì–¸ë¡ ì‚¬ëª…", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
        df = df.reindex(columns=column_order)

        # results ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("results", exist_ok=True)

        saved_files = []

        if split_by_section:
            # ì„¹ì…˜ë³„ë¡œ íŒŒì¼ ì €ì¥
            for category in df["ì¹´í…Œê³ ë¦¬"].unique():
                section_df = df[df["ì¹´í…Œê³ ë¦¬"] == category]
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                section_filename = f"results/ì„œìš¸ì‹ ë¬¸_{category}_{timestamp}.csv"

                section_df.to_csv(section_filename, index=False, encoding="utf-8-sig")

                logger.info(f"âœ“ [ì„œìš¸ì‹ ë¬¸ {category}] CSV íŒŒì¼ ì €ì¥: {section_filename}")
                logger.info(f"  - {len(section_df)}ê°œ ê¸°ì‚¬, {os.path.getsize(section_filename):,} bytes")
                saved_files.append(section_filename)
        else:
            # í†µí•© íŒŒì¼ë¡œ ì €ì¥
            if filename is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"results/ì„œìš¸ì‹ ë¬¸_ì „ì²´_{timestamp}.csv"

            df.to_csv(filename, index=False, encoding="utf-8-sig")

            logger.info(f"âœ“ ì„œìš¸ì‹ ë¬¸ í†µí•© CSV íŒŒì¼ ì €ì¥: {filename}")
            logger.info(f"  - ì´ {len(df)}ê°œ ê¸°ì‚¬, {os.path.getsize(filename):,} bytes")
            saved_files.append(filename)

        return saved_files

    def setup_chrome_driver(self, headless=True):
        options = Options()
        if headless:
            options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920,1080")
        service = Service(ChromeDriverManager().install())
        return webdriver.Chrome(service=service, options=options)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("ì„œìš¸ì‹ ë¬¸ í¬ë¡¤ë§ í”„ë¡œê·¸ë¨")
    print("=" * 60)

    crawler = SeoulShinmunCrawler()

    print("\nì‚¬ìš© ê°€ëŠ¥í•œ ì„¹ì…˜:")
    for key, info in crawler.sections.items():
        print(f"  - {key}: {info['name']}")

    # ì‚¬ìš©ì ì„¤ì •
    sections_to_crawl = None  # ëª¨ë“  ì„¹ì…˜ (íŠ¹ì • ì„¹ì…˜: ['politics', 'society'])
    max_articles_per_section = 20  # ê° ì„¹ì…˜ë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
    split_by_section = False  # True: ì„¹ì…˜ë³„ íŒŒì¼, False: í†µí•© íŒŒì¼

    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        articles, section_results = crawler.crawl_all_sections(sections_to_crawl, max_articles_per_section)

        if articles:
            # CSV íŒŒì¼ë¡œ ì €ì¥
            saved_files = crawler.save_to_csv(articles, split_by_section=split_by_section)

            # ê²°ê³¼ ìš”ì•½
            print("\n" + "=" * 60)
            print("ì„œìš¸ì‹ ë¬¸ í¬ë¡¤ë§ ì™„ë£Œ ê²°ê³¼")
            print("=" * 60)
            print(f"âœ“ ì´ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜: {len(articles):,}ê°œ")

            print(f"\nğŸ“Š ì„¹ì…˜ë³„ ìˆ˜ì§‘ ê²°ê³¼:")
            for section, count in section_results.items():
                print(f"  - {section}: {count:,}ê°œ")

            print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
            for file in saved_files:
                print(f"  - {file}")

            # ìƒ˜í”Œ ë°ì´í„° í‘œì‹œ
            df = pd.DataFrame(articles)
            print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
            category_counts = df["ì¹´í…Œê³ ë¦¬"].value_counts()
            for category, count in category_counts.items():
                print(f"  - {category}: {count}ê°œ")

            print(f"\nğŸ“° ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìƒ˜í”Œ (ìµœëŒ€ 3ê°œ):")
            for i in range(min(3, len(df))):
                row = df.iloc[i]
                print(f"{i+1}. [ì„œìš¸ì‹ ë¬¸ {row['ì¹´í…Œê³ ë¦¬']}] {row['ì œëª©']}")
                print(f"   ê¸°ì: {row['ê¸°ìëª…']}, ë‚ ì§œ: {row['ë‚ ì§œ']}")
                print()

            return saved_files
        else:
            print("âŒ í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

    except Exception as e:
        logger.error(f"âŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def crawl_specific_sections(section_list, max_articles=10):
    """íŠ¹ì • ì„¹ì…˜ë“¤ë§Œ í¬ë¡¤ë§í•˜ëŠ” í¸ì˜ í•¨ìˆ˜"""
    crawler = SeoulShinmunCrawler()
    articles, section_results = crawler.crawl_all_sections(section_list, max_articles)

    if articles:
        saved_files = crawler.save_to_csv(articles, split_by_section=True)
        print(f"\nâœ… ì„œìš¸ì‹ ë¬¸ í¬ë¡¤ë§ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        return saved_files
    else:
        print("âŒ ì„œìš¸ì‹ ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨")
        return None


if __name__ == "__main__":
    # ë©”ì¸ í”„ë¡œê·¸ë¨ ì‹¤í–‰
    result_files = main()
