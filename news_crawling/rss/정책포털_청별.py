#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì •ì±…ë¸Œë¦¬í•‘(korea.kr) ì •ë¶€ ì‚°í•˜ê¸°ê´€ë³„ RSS í¬ë¡¤ë§ ì½”ë“œ
18ê°œ ì •ë¶€ ì‚°í•˜ê¸°ê´€ RSS í”¼ë“œ ì „ìš© í¬ë¡¤ëŸ¬

ì‘ì„±ì¼: 2025-08-02
"""

import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import pandas as pd
import time
import random
import re
from datetime import datetime
import csv
from urllib.parse import urljoin, urlparse
import logging

class KoreaGovernmentAgencyRSSCrawler:
    def __init__(self):
        """ì •ì±…ë¸Œë¦¬í•‘ ì •ë¶€ ì‚°í•˜ê¸°ê´€ë³„ RSS í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.base_url = "https://www.korea.kr"
        
        # 18ê°œ ì •ë¶€ ì‚°í•˜ê¸°ê´€ RSS í”¼ë“œ
        self.agency_feeds = {
            'êµ­ì„¸ì²­': 'https://www.korea.kr/rss/dept_nts.xml',
            'ê´€ì„¸ì²­': 'https://www.korea.kr/rss/dept_customs.xml',
            'ì¡°ë‹¬ì²­': 'https://www.korea.kr/rss/dept_pps.xml',
            'í†µê³„ì²­': 'https://www.korea.kr/rss/dept_kostat.xml',
            'ìš°ì£¼í•­ê³µì²­': 'https://www.korea.kr/rss/dept_kasa.xml',
            'ì¬ì™¸ë™í¬ì²­': 'https://www.korea.kr/rss/dept_oka.xml',
            'ê²€ì°°ì²­': 'https://www.korea.kr/rss/dept_spo.xml',
            'ë³‘ë¬´ì²­': 'https://www.korea.kr/rss/dept_mma.xml',
            'ë°©ìœ„ì‚¬ì—…ì²­': 'https://www.korea.kr/rss/dept_dapa.xml',
            'ê²½ì°°ì²­': 'https://www.korea.kr/rss/dept_npa.xml',
            'ì†Œë°©ì²­': 'https://www.korea.kr/rss/dept_nfa.xml',
            'êµ­ê°€ìœ ì‚°ì²­': 'https://www.korea.kr/rss/dept_khs.xml',
            'ë†ì´Œì§„í¥ì²­': 'https://www.korea.kr/rss/dept_rda.xml',
            'ì‚°ë¦¼ì²­': 'https://www.korea.kr/rss/dept_forest.xml',
            'íŠ¹í—ˆì²­': 'https://www.korea.kr/rss/dept_kipo.xml',
            'ì§ˆë³‘ê´€ë¦¬ì²­': 'https://www.korea.kr/rss/dept_kdca.xml',
            'ê¸°ìƒì²­': 'https://www.korea.kr/rss/dept_kma.xml',
            'í–‰ì •ì¤‘ì‹¬ë³µí•©ë„ì‹œê±´ì„¤ì²­': 'https://www.korea.kr/rss/dept_macc.xml',
            'ìƒˆë§Œê¸ˆê°œë°œì²­': 'https://www.korea.kr/rss/dept_sda.xml',
            'í•´ì–‘ê²½ì°°ì²­': 'https://www.korea.kr/rss/dept_kcg.xml'
        }
        
        # ì‚°í•˜ê¸°ê´€ë³„ ì£¼ìš” ì—…ë¬´ ë¶„ì•¼ (ë¶„ì„ìš©)
        self.agency_areas = {
            'êµ­ì„¸ì²­': 'ì„¸ë¬´í–‰ì •, êµ­ì„¸ì§•ìˆ˜, ì„¸ë¬´ì¡°ì‚¬',
            'ê´€ì„¸ì²­': 'ê´€ì„¸í–‰ì •, ìˆ˜ì¶œì…í†µê´€, ë°€ìˆ˜ë‹¨ì†',
            'ì¡°ë‹¬ì²­': 'ê³µê³µì¡°ë‹¬, ì •ë¶€êµ¬ë§¤, ë¬¼í’ˆê´€ë¦¬',
            'í†µê³„ì²­': 'êµ­ê°€í†µê³„, ì¸êµ¬ì¡°ì‚¬, ê²½ì œí†µê³„',
            'ìš°ì£¼í•­ê³µì²­': 'ìš°ì£¼ê°œë°œ, í•­ê³µìš°ì£¼ê¸°ìˆ , ìœ„ì„±',
            'ì¬ì™¸ë™í¬ì²­': 'ì¬ì™¸ë™í¬ì§€ì›, í•´ì™¸í•œì¸ì‚¬íšŒ',
            'ê²€ì°°ì²­': 'ê²€ì°°ì—…ë¬´, í˜•ì‚¬ì‚¬ë²•, ìˆ˜ì‚¬',
            'ë³‘ë¬´ì²­': 'ë³‘ì—­í–‰ì •, ì§•ë³‘ê²€ì‚¬, êµ°ë³µë¬´',
            'ë°©ìœ„ì‚¬ì—…ì²­': 'ë°©ì‚°ì—…ë¬´, ë¬´ê¸°ì²´ê³„, êµ­ë°©íšë“',
            'ê²½ì°°ì²­': 'ì¹˜ì•ˆí–‰ì •, ë²”ì£„ìˆ˜ì‚¬, êµí†µì•ˆì „',
            'ì†Œë°©ì²­': 'í™”ì¬ì˜ˆë°©, êµ¬ê¸‰êµ¬ì¡°, ì¬ë‚œëŒ€ì‘',
            'êµ­ê°€ìœ ì‚°ì²­': 'ë¬¸í™”ì¬ë³´í˜¸, ì—­ì‚¬ìœ ì , ì „í†µë¬¸í™”',
            'ë†ì´Œì§„í¥ì²­': 'ë†ì—…ê¸°ìˆ , ë†ì´Œê°œë°œ, ë†ì—…ì—°êµ¬',
            'ì‚°ë¦¼ì²­': 'ì‚°ë¦¼ë³´í˜¸, ì„ì—…ì •ì±…, ì‚°ë¶ˆë°©ì§€',
            'íŠ¹í—ˆì²­': 'íŠ¹í—ˆí–‰ì •, ì§€ì‹ì¬ì‚°ê¶Œ, ìƒí‘œë“±ë¡',
            'ì§ˆë³‘ê´€ë¦¬ì²­': 'ì§ˆë³‘ì˜ˆë°©, ê°ì—¼ë³‘ê´€ë¦¬, ë°©ì—­',
            'ê¸°ìƒì²­': 'ê¸°ìƒì˜ˆë³´, ë‚ ì”¨ì •ë³´, ê¸°í›„ë³€í™”',
            'í–‰ì •ì¤‘ì‹¬ë³µí•©ë„ì‹œê±´ì„¤ì²­': 'ì„¸ì¢…ì‹œê±´ì„¤, ë„ì‹œê°œë°œ',
            'ìƒˆë§Œê¸ˆê°œë°œì²­': 'ìƒˆë§Œê¸ˆê°œë°œ, ê°„ì²™ì‚¬ì—…',
            'í•´ì–‘ê²½ì°°ì²­': 'í•´ìƒì¹˜ì•ˆ, í•´ì–‘ì•ˆì „, í•´ìƒêµ¬ì¡°'
        }
        
        # ì‚°í•˜ê¸°ê´€ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        self.agency_categories = {
            'ì„¸ë¬´Â·ì¬ì •': ['êµ­ì„¸ì²­', 'ê´€ì„¸ì²­', 'ì¡°ë‹¬ì²­'],
            'í†µê³„Â·ì •ë³´': ['í†µê³„ì²­'],
            'ê³¼í•™Â·ìš°ì£¼': ['ìš°ì£¼í•­ê³µì²­'],
            'ì™¸êµÂ·ë™í¬': ['ì¬ì™¸ë™í¬ì²­'],
            'ì‚¬ë²•Â·ì¹˜ì•ˆ': ['ê²€ì°°ì²­', 'ê²½ì°°ì²­', 'í•´ì–‘ê²½ì°°ì²­'],
            'êµ­ë°©Â·ë³‘ë¬´': ['ë³‘ë¬´ì²­', 'ë°©ìœ„ì‚¬ì—…ì²­'],
            'ì•ˆì „Â·ë°©ì¬': ['ì†Œë°©ì²­', 'ì§ˆë³‘ê´€ë¦¬ì²­'],
            'ë¬¸í™”Â·ìœ ì‚°': ['êµ­ê°€ìœ ì‚°ì²­'],
            'ë†ë¦¼Â·ìˆ˜ì‚°': ['ë†ì´Œì§„í¥ì²­', 'ì‚°ë¦¼ì²­'],
            'ì‚°ì—…Â·íŠ¹í—ˆ': ['íŠ¹í—ˆì²­'],
            'ê¸°ìƒÂ·í™˜ê²½': ['ê¸°ìƒì²­'],
            'ë„ì‹œÂ·ê°œë°œ': ['í–‰ì •ì¤‘ì‹¬ë³µí•©ë„ì‹œê±´ì„¤ì²­', 'ìƒˆë§Œê¸ˆê°œë°œì²­']
        }
        
        # User-Agent ë¦¬ìŠ¤íŠ¸
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15'
        ]
        
        self.articles = []
        self.session = requests.Session()
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)

    def get_random_headers(self):
        """ëœë¤ í—¤ë” ìƒì„±"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }

    def random_delay(self, min_delay=1, max_delay=3):
        """ëœë¤ ë”œë ˆì´"""
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)

    def fetch_rss_feed(self, rss_url, max_retries=3):
        """RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°"""
        for attempt in range(max_retries):
            try:
                headers = self.get_random_headers()
                response = self.session.get(rss_url, headers=headers, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except Exception as e:
                self.logger.warning(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    self.random_delay(2, 5)
                else:
                    self.logger.error(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ìµœì¢… ì‹¤íŒ¨: {rss_url}")
                    return None

    def parse_rss_feed(self, rss_content):
        """RSS í”¼ë“œ íŒŒì‹±"""
        try:
            root = ET.fromstring(rss_content)
            items = []
            
            for item in root.findall('.//item'):
                article_info = {}
                
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title_elem = item.find('title')
                article_info['title'] = title_elem.text.strip() if title_elem is not None else ''
                
                link_elem = item.find('link')
                article_info['link'] = link_elem.text.strip() if link_elem is not None else ''
                
                pubdate_elem = item.find('pubDate')
                article_info['pub_date'] = pubdate_elem.text.strip() if pubdate_elem is not None else ''
                
                guid_elem = item.find('guid')
                article_info['guid'] = guid_elem.text.strip() if guid_elem is not None else ''
                
                # dc:creator ì¶”ì¶œ (ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê³ ë ¤)
                creator_elem = item.find('.//{http://purl.org/dc/elements/1.1/}creator')
                article_info['creator'] = creator_elem.text.strip() if creator_elem is not None else ''
                
                # descriptionì—ì„œ ê°„ë‹¨í•œ ë‚´ìš© ì¶”ì¶œ
                desc_elem = item.find('description')
                if desc_elem is not None:
                    desc_text = desc_elem.text or ''
                    # CDATA ì²˜ë¦¬
                    if desc_text.startswith('<![CDATA[') and desc_text.endswith(']]>'):
                        desc_text = desc_text[9:-3]
                    
                    # HTML íƒœê·¸ ì œê±°í•˜ì—¬ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                    soup = BeautifulSoup(desc_text, 'html.parser')
                    article_info['description'] = soup.get_text().strip()[:300] + '...' if len(soup.get_text().strip()) > 300 else soup.get_text().strip()
                else:
                    article_info['description'] = ''
                
                items.append(article_info)
            
            return items
        except Exception as e:
            self.logger.error(f"RSS íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []

    def extract_article_content(self, article_url, max_retries=3):
        """ê°œë³„ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ - ì‚°í•˜ê¸°ê´€ í˜ì´ì§€ ìµœì í™”"""
        for attempt in range(max_retries):
            try:
                headers = self.get_random_headers()
                response = self.session.get(article_url, headers=headers, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ì‚°í•˜ê¸°ê´€ë³„ í˜ì´ì§€ êµ¬ì¡°ì— ìµœì í™”ëœ ë³¸ë¬¸ ì¶”ì¶œ ì…€ë ‰í„°
                content_selectors = [
                    '.agency_cont',     # ì‚°í•˜ê¸°ê´€ ì½˜í…ì¸ 
                    '.press_cont',      # ë³´ë„ìë£Œ ì½˜í…ì¸ 
                    '.article_body',    # ì¼ë°˜ ê¸°ì‚¬
                    '.rbody',          # ë¸Œë¦¬í•‘ í˜ì´ì§€
                    '.view_cont',      # ë·° í˜ì´ì§€
                    '.cont_body',      # ì½˜í…ì¸  ë³¸ë¬¸
                    '.policy_body',    # ì •ì±… ë³¸ë¬¸
                    '.briefing_cont',  # ë¸Œë¦¬í•‘ ë‚´ìš©
                    '.news_cont',      # ë‰´ìŠ¤ ë‚´ìš©
                    '.notice_cont',    # ê³µì§€ì‚¬í•­ ë‚´ìš©
                    '.info_cont'       # ì •ë³´ ë‚´ìš©
                ]
                
                content = ""
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text().strip()
                        break
                
                # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„
                if not content:
                    # í—¤ë”, í‘¸í„°, ì‚¬ì´ë“œë°” ë“± ì œê±°
                    for elem in soup.find_all(['header', 'footer', 'nav', 'aside', 'script', 'style']):
                        elem.decompose()
                    
                    main_content = soup.find('main') or soup.find('div', class_='content') or soup.find('body')
                    if main_content:
                        content = main_content.get_text().strip()
                
                # ê¸°ê´€ë³„ íŠ¹í™” ì •ë³´ ì¶”ì¶œ
                contact_info = self.extract_agency_contact_info(content)
                service_keywords = self.extract_service_keywords(content)
                
                # í…ìŠ¤íŠ¸ ì •ë¦¬
                content = re.sub(r'\s+', ' ', content).strip()
                
                return {
                    'content': content[:3000] + '...' if len(content) > 3000 else content,
                    'contact_info': contact_info,
                    'service_keywords': service_keywords
                }
                
            except Exception as e:
                self.logger.warning(f"ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    self.random_delay(1, 3)
                else:
                    return {'content': 'ì¶”ì¶œ ì‹¤íŒ¨', 'contact_info': '', 'service_keywords': ''}

    def extract_agency_contact_info(self, content):
        """ì‚°í•˜ê¸°ê´€ ì—°ë½ì²˜/ë‹´ë‹¹ì ì •ë³´ ì¶”ì¶œ"""
        # ì‚°í•˜ê¸°ê´€ë³„ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ íŒ¨í„´
        patterns = [
            r'ë¬¸ì˜\s*:\s*([^(]+?)(?:\(([^)]+)\))?',
            r'ë‹´ë‹¹\s*:\s*([^(]+?)(?:\(([^)]+)\))?',
            r'ì—°ë½ì²˜\s*:\s*([^(]+?)(?:\(([^)]+)\))?',
            r'ë¬¸ì˜ì²˜\s*:\s*([^(]+?)(?:\(([^)]+)\))?',
            r'ë‹´ë‹¹ë¶€ì„œ\s*:\s*([^(]+?)(?:\(([^)]+)\))?',
            r'ë‹´ë‹¹ì\s*:\s*([^(]+?)(?:\(([^)]+)\))?',
            r'í™ˆí˜ì´ì§€\s*:\s*(https?://[^\s]+)',
            r'([ê°€-í£]+ì²­|[ê°€-í£]+ì²­|[ê°€-í£]+ì›|[ê°€-í£]+ì†Œ)\s+([ê°€-í£]+ê³¼|[ê°€-í£]+íŒ€|[ê°€-í£]+êµ­)\s*(?:\(([^)]+)\))?'
        ]
        
        contact_info = {}
        
        for pattern in patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if isinstance(match, tuple):
                    if len(match) >= 2:
                        dept = match[0].strip() if match[0] else ''
                        contact = match[1].strip() if match[1] else ''
                        
                        if dept and len(dept) > 1 and len(dept) < 100:
                            contact_info['department'] = dept
                        if contact and ('02-' in contact or '044-' in contact or '070-' in contact or 'http' in contact):
                            if 'http' in contact:
                                contact_info['website'] = contact
                            else:
                                contact_info['phone'] = contact
                else:
                    if match and len(match) > 1 and len(match) < 100:
                        contact_info['department'] = match.strip()
        
        return '; '.join([f"{k}: {v}" for k, v in contact_info.items()])

    def extract_service_keywords(self, content):
        """ì‚°í•˜ê¸°ê´€ ì„œë¹„ìŠ¤/ì—…ë¬´ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ì‚°í•˜ê¸°ê´€ë³„ ì£¼ìš” ì„œë¹„ìŠ¤ í‚¤ì›Œë“œ íŒ¨í„´
        service_patterns = [
            r'(ì‹ ì²­|ì ‘ìˆ˜|ë°œê¸‰|ë“±ë¡|ìŠ¹ì¸|í—ˆê°€|ì¸ì¦|ê²€ì‚¬|ê²€ì¦)',
            r'(ì„œë¹„ìŠ¤|ì§€ì›|ìƒë‹´|ì•ˆë‚´|ì •ë³´ì œê³µ|êµìœ¡|í›ˆë ¨)',
            r'(ì˜¨ë¼ì¸|ì „ì|ë””ì§€í„¸|ëª¨ë°”ì¼|ì•±|ì‹œìŠ¤í…œ)',
            r'(ìˆ˜ìˆ˜ë£Œ|ìš”ê¸ˆ|ë¹„ìš©|ê¸°ì¤€|ì ˆì°¨|ë°©ë²•)',
            r'(ì•ˆì „|ë³´ì•ˆ|ì˜ˆë°©|ì ê²€|ê´€ë¦¬|ê°ì‹œ|ë‹¨ì†)',
            r'(ê°œì„ |ê°œë°œ|ì—°êµ¬|ì¡°ì‚¬|ë¶„ì„|í‰ê°€)',
            r'(êµ­ë¯¼|ì‹œë¯¼|ì—…ì²´|ê¸°ì—…|ì‚¬ì—…ì|ê°œì¸)'
        ]
        
        keywords = set()
        for pattern in service_patterns:
            matches = re.findall(pattern, content)
            keywords.update(matches)
        
        return ', '.join(list(keywords)[:10])  # ìµœëŒ€ 10ê°œ í‚¤ì›Œë“œ

    def get_agency_category(self, agency_name):
        """ì‚°í•˜ê¸°ê´€ì˜ ì¹´í…Œê³ ë¦¬ ë°˜í™˜"""
        for category, agencies in self.agency_categories.items():
            if agency_name in agencies:
                return category
        return 'ê¸°íƒ€'

    def crawl_agency_feed(self, agency, rss_url, max_items=25):
        """ê°œë³„ ì‚°í•˜ê¸°ê´€ RSS í”¼ë“œ í¬ë¡¤ë§"""
        self.logger.info(f"ì‚°í•˜ê¸°ê´€ í¬ë¡¤ë§ ì‹œì‘: {agency}")
        
        # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
        rss_content = self.fetch_rss_feed(rss_url)
        if not rss_content:
            return
        
        # RSS íŒŒì‹±
        rss_items = self.parse_rss_feed(rss_content)
        if not rss_items:
            self.logger.warning(f"RSS ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤: {agency}")
            return
        
        # ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ë§Œ ì²˜ë¦¬
        items_to_process = rss_items[:max_items]
        
        for i, item in enumerate(items_to_process, 1):
            try:
                self.logger.info(f"{agency} ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘: {i}/{len(items_to_process)} - {item['title'][:50]}...")
                
                # ê¸°ì‚¬ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
                if item['link']:
                    article_detail = self.extract_article_content(item['link'])
                    
                    article_data = {
                        'agency': agency,
                        'agency_category': self.get_agency_category(agency),
                        'business_area': self.agency_areas.get(agency, ''),
                        'title': item['title'],
                        'link': item['link'],
                        'pub_date': item['pub_date'],
                        'creator': item['creator'],
                        'description': item['description'],
                        'content': article_detail['content'],
                        'contact_info': article_detail['contact_info'],
                        'service_keywords': article_detail['service_keywords'],
                        'collected_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    
                    self.articles.append(article_data)
                
                # ë”œë ˆì´
                self.random_delay(1, 3)
                
            except Exception as e:
                self.logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        self.logger.info(f"{agency} í¬ë¡¤ë§ ì™„ë£Œ: {len(items_to_process)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬")

    def crawl_all_agencies(self, max_items_per_agency=25):
        """ëª¨ë“  ì‚°í•˜ê¸°ê´€ RSS í”¼ë“œ í¬ë¡¤ë§"""
        total_agencies = len(self.agency_feeds)
        self.logger.info(f"ì „ì²´ {total_agencies}ê°œ ì‚°í•˜ê¸°ê´€ RSS í”¼ë“œ í¬ë¡¤ë§ ì‹œì‘")
        
        for i, (agency, rss_url) in enumerate(self.agency_feeds.items(), 1):
            try:
                self.logger.info(f"[{i}/{total_agencies}] {agency} í”¼ë“œ í¬ë¡¤ë§ ì¤‘...")
                self.crawl_agency_feed(agency, rss_url, max_items_per_agency)
                
                # ê¸°ê´€ ê°„ ë”œë ˆì´
                if i < total_agencies:
                    self.random_delay(3, 6)
                    
            except Exception as e:
                self.logger.error(f"{agency} ì‚°í•˜ê¸°ê´€ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                continue
        
        self.logger.info(f"ì „ì²´ ì‚°í•˜ê¸°ê´€ í¬ë¡¤ë§ ì™„ë£Œ: {len(self.articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        self.print_statistics()

    def crawl_specific_agencies(self, agency_names, max_items_per_agency=25):
        """íŠ¹ì • ì‚°í•˜ê¸°ê´€ë“¤ë§Œ í¬ë¡¤ë§"""
        for agency_name in agency_names:
            if agency_name in self.agency_feeds:
                self.crawl_agency_feed(agency_name, self.agency_feeds[agency_name], max_items_per_agency)
            else:
                self.logger.warning(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì‚°í•˜ê¸°ê´€: {agency_name}")
                available_agencies = list(self.agency_feeds.keys())
                self.logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì‚°í•˜ê¸°ê´€: {available_agencies}")

    def crawl_by_category(self, categories, max_items_per_agency=20):
        """ì¹´í…Œê³ ë¦¬ë³„ ì‚°í•˜ê¸°ê´€ í¬ë¡¤ë§"""
        target_agencies = []
        
        for category in categories:
            if category in self.agency_categories:
                target_agencies.extend(self.agency_categories[category])
            else:
                self.logger.warning(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬: {category}")
        
        if target_agencies:
            self.logger.info(f"ì¹´í…Œê³ ë¦¬ '{', '.join(categories)}'ì— í•´ë‹¹í•˜ëŠ” ì‚°í•˜ê¸°ê´€: {target_agencies}")
            self.crawl_specific_agencies(target_agencies, max_items_per_agency)
        else:
            self.logger.warning(f"í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ë§ëŠ” ì‚°í•˜ê¸°ê´€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {categories}")

    def save_to_csv(self, filename=None):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'results/ì‚°í•˜ê¸°ê´€ë³„_RSS_{timestamp}.csv'
        
        try:
            df = pd.DataFrame(self.articles)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            self.logger.info(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            self.logger.info(f"ì´ {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥")
        except Exception as e:
            self.logger.error(f"CSV ì €ì¥ ì˜¤ë¥˜: {e}")

    def save_by_agency(self):
        """ì‚°í•˜ê¸°ê´€ë³„ë¡œ ê°œë³„ CSV íŒŒì¼ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        df = pd.DataFrame(self.articles)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        for agency in df['agency'].unique():
            agency_df = df[df['agency'] == agency]
            filename = f'results/ì‚°í•˜ê¸°ê´€_{agency}_{timestamp}.csv'
            agency_df.to_csv(filename, index=False, encoding='utf-8-sig')
            self.logger.info(f"{agency} ì €ì¥ ì™„ë£Œ: {filename} ({len(agency_df)}ê°œ ê¸°ì‚¬)")

    def save_by_category(self):
        """ì¹´í…Œê³ ë¦¬ë³„ë¡œ CSV íŒŒì¼ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        df = pd.DataFrame(self.articles)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        for category in df['agency_category'].unique():
            category_df = df[df['agency_category'] == category]
            filename = f'results/ì¹´í…Œê³ ë¦¬_{category}_{timestamp}.csv'
            category_df.to_csv(filename, index=False, encoding='utf-8-sig')
            self.logger.info(f"{category} ì¹´í…Œê³ ë¦¬ ì €ì¥ ì™„ë£Œ: {filename} ({len(category_df)}ê°œ ê¸°ì‚¬)")

    def print_statistics(self):
        """í¬ë¡¤ë§ í†µê³„ ì¶œë ¥"""
        if not self.articles:
            return
        
        df = pd.DataFrame(self.articles)
        
        print("\n" + "="*60)
        print("ì •ë¶€ ì‚°í•˜ê¸°ê´€ë³„ RSS í¬ë¡¤ë§ í†µê³„")
        print("="*60)
        
        # ì‚°í•˜ê¸°ê´€ë³„ í†µê³„
        agency_stats = df['agency'].value_counts()
        print(f"\nğŸ¢ ì‚°í•˜ê¸°ê´€ë³„ ê¸°ì‚¬ ìˆ˜:")
        for agency, count in agency_stats.items():
            business_area = self.agency_areas.get(agency, '')
            print(f"  â€¢ {agency} ({business_area}): {count}ê°œ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = df['agency_category'].value_counts()
        print(f"\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜:")
        for category, count in category_stats.items():
            print(f"  â€¢ {category}: {count}ê°œ")
        
        # ì—…ë¬´ ë¶„ì•¼ë³„ í†µê³„
        business_area_stats = df['business_area'].value_counts().head(10)
        if not business_area_stats.empty:
            print(f"\nğŸ’¼ ì£¼ìš” ì—…ë¬´ ë¶„ì•¼ë³„ ê¸°ì‚¬ ìˆ˜:")
            for area, count in business_area_stats.items():
                print(f"  â€¢ {area}: {count}ê°œ")
        
        # ì—°ë½ì²˜ ì •ë³´ í†µê³„
        contact_available = len(df[df['contact_info'] != ''])
        print(f"\nğŸ“ ì—°ë½ì²˜ ì •ë³´:")
        print(f"  â€¢ ì—°ë½ì²˜ ì¶”ì¶œ ì„±ê³µ: {contact_available}ê°œ")
        print(f"  â€¢ ì—°ë½ì²˜ ì¶”ì¶œ ì‹¤íŒ¨: {len(df) - contact_available}ê°œ")
        
        print(f"\nğŸ“ˆ ì „ì²´ ìš”ì•½:")
        print(f"  â€¢ ì´ ê¸°ì‚¬ ìˆ˜: {len(self.articles)}ê°œ")
        print(f"  â€¢ í¬ë¡¤ë§ ì‚°í•˜ê¸°ê´€ ìˆ˜: {len(agency_stats)}ê°œ")
        print(f"  â€¢ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {len(df[df['content'] != 'ì¶”ì¶œ ì‹¤íŒ¨'])}ê°œ")
        print(f"  â€¢ ì„œë¹„ìŠ¤ í‚¤ì›Œë“œ ì¶”ì¶œ: {len(df[df['service_keywords'] != ''])}ê°œ")
        print("="*60)

    def get_available_agencies(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì‚°í•˜ê¸°ê´€ ëª©ë¡ ë°˜í™˜"""
        return list(self.agency_feeds.keys())

    def get_categories(self):
        """ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë°˜í™˜"""
        return list(self.agency_categories.keys())


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ì •ì±…ë¸Œë¦¬í•‘ ì •ë¶€ ì‚°í•˜ê¸°ê´€ë³„ RSS í¬ë¡¤ëŸ¬")
    print("="*50)
    
    crawler = KoreaGovernmentAgencyRSSCrawler()
    
    # ì‚¬ìš© ì˜ˆì‹œ 1: ì „ì²´ ì‚°í•˜ê¸°ê´€ í¬ë¡¤ë§ (ê° ê¸°ê´€ë‹¹ 15ê°œì”©)
    print("ì „ì²´ ì‚°í•˜ê¸°ê´€ RSS í”¼ë“œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    crawler.crawl_all_agencies(max_items_per_agency=10)
    
    # CSV ì €ì¥
    crawler.save_to_csv()
    
    # ì‚°í•˜ê¸°ê´€ë³„ ê°œë³„ íŒŒì¼ ì €ì¥
    # crawler.save_by_agency()
    
    # ì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼ ì €ì¥
    # crawler.save_by_category()
    
    # ì‚¬ìš© ì˜ˆì‹œ 2: íŠ¹ì • ì‚°í•˜ê¸°ê´€ë§Œ í¬ë¡¤ë§
    # crawler.crawl_specific_agencies(['êµ­ì„¸ì²­', 'ê´€ì„¸ì²­', 'í†µê³„ì²­'])
    
    # ì‚¬ìš© ì˜ˆì‹œ 3: ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§
    # crawler.crawl_by_category(['ì„¸ë¬´Â·ì¬ì •', 'ì‚¬ë²•Â·ì¹˜ì•ˆ'], max_items_per_agency=20)
    
    print("\nì‚°í•˜ê¸°ê´€ë³„ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
