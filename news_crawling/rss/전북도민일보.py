import feedparser
import requests
from bs4 import BeautifulSoup
import csv
import time
import random
from datetime import datetime
import re
import urllib.parse


class DominRSSCollector:
    def __init__(self):
        self.base_url = "http://www.domin.co.kr"
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
        ]

        # ì „ë¶ë„ë¯¼ì¼ë³´ RSS í”¼ë“œ ì¹´í…Œê³ ë¦¬ (ì´ë¯¸ì§€ì—ì„œ í™•ì¸í•œ ì •í™•í•œ êµ¬ì¡°)
        self.rss_categories = {
            "ì „ì²´ê¸°ì‚¬": "allArticle.xml",
            "í—¤ë“œë¼ì¸ê¸°ì‚¬": "headArticle.xml",
            "ì£¼ìš”ê¸°ì‚¬": "clickTop.xml",
            "ì „ì£¼": "S2N24.xml",
            "êµ°ì‚°": "S2N25.xml",
            "ìµì‚°": "S2N26.xml",
            "ì •ì": "S2N27.xml",
            "ë‚¨ì›": "S2N28.xml",
            "ê¹€ì œ": "S2N29.xml",
            "ì™„ì£¼": "S2N30.xml",
            "ì§„ì•ˆ": "S2N31.xml",
            "ë¬´ì£¼": "S2N32.xml",
            "ì¥ìˆ˜": "S2N33.xml",
            "ì„ì‹¤": "S2N34.xml",
            "ìˆœì°½": "S2N35.xml",
            "ê³ ì°½": "S2N36.xml",
            "ë¶€ì•ˆ": "S2N37.xml",
        }

        self.session = requests.Session()

    def get_random_user_agent(self):
        """ëœë¤ User-Agent ë°˜í™˜"""
        return random.choice(self.user_agents)

    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text:
            return ""

        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r"<[^>]+>", "", text)
        # íŠ¹ìˆ˜ë¬¸ì ì •ì œ
        text = re.sub(r"[\r\n\t]+", " ", text)
        text = re.sub(r"\s+", " ", text)
        # ë”°ì˜´í‘œ ì²˜ë¦¬
        text = text.replace('"', '""')

        return text.strip()

    def extract_reporter_name(self, article_url):
        """ê¸°ì‚¬ URLì—ì„œ ê¸°ìëª… ì¶”ì¶œ"""
        try:
            headers = {"User-Agent": self.get_random_user_agent()}
            response = self.session.get(article_url, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            # ì „ë¶ë„ë¯¼ì¼ë³´ ê¸°ìëª… íŒ¨í„´ ì°¾ê¸°
            reporter_patterns = [
                # ê¸°ë³¸ ê¸°ìëª… íŒ¨í„´: "ê¹€í•™ìˆ˜ ê¸°ì"
                r"([ê°€-í£]{2,4})\s*ê¸°ì",
                # ì´ë©”ì¼ê³¼ í•¨ê»˜: "ê¹€í•™ìˆ˜ê¸°ì kimhs@domin.co.kr"
                r"([ê°€-í£]{2,4})ê¸°ì\s+[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
                # íƒœê·¸ ë‚´ ê¸°ìëª…
                r'<[^>]*class="reporter"[^>]*>([ê°€-í£]{2,4})',
                r'<[^>]*class="writer"[^>]*>([ê°€-í£]{2,4})',
                # ê¸°ì‚¬ ì •ë³´ ì˜ì—­
                r"ê¸°ì\s*[:ï¼š]\s*([ê°€-í£]{2,4})",
                r"ê¸€\s*[:ï¼š]\s*([ê°€-í£]{2,4})",
                r"ì·¨ì¬\s*[:ï¼š]\s*([ê°€-í£]{2,4})",
                # ì§€ì—­=ê¸°ìëª… íŒ¨í„´ (ì „ë¶ë„ë¯¼ì¼ë³´ íŠ¹ì„±)
                r"([ê°€-í£]{2,8})=([ê°€-í£]{2,4})\s*ê¸°ì",
                # ê¸°ì‚¬ í•˜ë‹¨ ì„œëª…
                r"([ê°€-í£]{2,4})\s*<[^>]*>",
                r"ì €ì‘ê¶Œì.*ì „ë¶ë„ë¯¼ì¼ë³´.*ë¬´ë‹¨ì „ì¬",  # ì €ì‘ê¶Œ ë¬¸êµ¬ ì „ ê¸°ìëª… í™•ì¸
            ]

            article_text = soup.get_text()

            for pattern in reporter_patterns:
                if "=" in pattern:  # ì§€ì—­=ê¸°ìëª… íŒ¨í„´
                    matches = re.findall(r"([ê°€-í£]{2,8})=([ê°€-í£]{2,4})\s*ê¸°ì", article_text)
                    if matches:
                        # ì§€ì—­ëª…=ê¸°ìëª… ì—ì„œ ê¸°ìëª…ë§Œ ì¶”ì¶œ
                        return matches[-1][1].strip()
                else:
                    matches = re.findall(pattern, article_text, re.MULTILINE)
                    if matches:
                        reporter_name = matches[-1].strip()
                        if len(reporter_name) >= 2 and not re.search(r"[0-9]", reporter_name):
                            # ì „ë¶ë„ë¯¼ì¼ë³´ íŠ¹ì„±: íŠ¹ì • ë‹¨ì–´ ì œì™¸
                            if reporter_name not in ["ì „ë¶ë„ë¯¼ì¼ë³´", "ì €ì‘ê¶Œì", "ë¬´ë‹¨ì „ì¬", "ì¬ë°°í¬"]:
                                return reporter_name

        except Exception as e:
            print(f"ê¸°ìëª… ì¶”ì¶œ ì˜¤ë¥˜ ({article_url}): {e}")

        return "ì •ë³´ì—†ìŒ"

    def extract_article_content(self, article_url):
        """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            headers = {"User-Agent": self.get_random_user_agent()}
            response = self.session.get(article_url, headers=headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")
            content_div = soup.find("div", id="article-view-content-div")
            if content_div:
                return self.clean_text(content_div.get_text(separator="\n"))
        except Exception as e:
            print(f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜ ({article_url}): {e}")
        return ""

    def collect_rss_feed(self, category_name, rss_file):
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ RSS í”¼ë“œì—ì„œ 20ê°œ ê¸°ì‚¬ ìë™ ìˆ˜ì§‘"""
        rss_url = f"{self.base_url}/rss/{rss_file}"
        print(f"{category_name} ìë™ ìˆ˜ì§‘ ì¤‘: {rss_url}")
        headers = {"User-Agent": self.get_random_user_agent()}
        response = self.session.get(rss_url, headers=headers, timeout=15)
        response.raise_for_status()
        feed = feedparser.parse(response.content)
        entries = feed.entries[:20]
        articles = []
        for entry in entries:
            title = self.clean_text(entry.title)
            link = entry.link
            pub_date = getattr(entry, "published", getattr(entry, "updated", ""))
            reporter = self.clean_text(entry.author) if hasattr(entry, "author") and entry.author else "ì •ë³´ì—†ìŒ"
            # í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ, ì‹¤íŒ¨ ì‹œ RSS summary fallback
            content = self.extract_article_content(link)
            if not content:
                if hasattr(entry, "summary"):
                    content = self.clean_text(entry.summary)
                elif hasattr(entry, "description"):
                    content = self.clean_text(entry.description)
            articles.append(
                {
                    "ì–¸ë¡ ì‚¬": "ì „ë¶ë„ë¯¼ì¼ë³´",
                    "ì œëª©": title,
                    "ë‚ ì§œ": pub_date,
                    "ì¹´í…Œê³ ë¦¬": category_name,
                    "ê¸°ìëª…": reporter,
                    "ë³¸ë¬¸": content,
                }
            )
            time.sleep(random.uniform(0.5, 1.0))
        return articles

    def save_to_csv(self, all_articles, filename=None):
        """ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì„ CSV íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = filename or f"results/ì „ë¶ë„ë¯¼ì¼ë³´_ì „ì²´_{timestamp}.csv"
        with open(filename, "w", newline="", encoding="utf-8-sig") as csvfile:
            fieldnames = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            for art in all_articles:
                writer.writerow(art)
        print(f"ğŸ“„ CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
        return filename

    def test_connection(self):
        """ë„ë©”ì¸ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            headers = {"User-Agent": self.get_random_user_agent()}
            response = self.session.get(f"{self.base_url}/rss/allArticle.xml", headers=headers, timeout=10)
            print(f"âœ… ë„ë©”ì¸ ì—°ê²° ì„±ê³µ: {self.base_url}")
            return True
        except Exception as e:
            print(f"âŒ ë„ë©”ì¸ ì—°ê²° ì‹¤íŒ¨: {e}")
            print("âš ï¸  ë„ë©”ì¸ ì£¼ì†Œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return False

    def collect_all_categories(self, selected_categories=None):
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë˜ëŠ” ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì˜ RSS ìˆ˜ì§‘"""
        if selected_categories is None:
            selected_categories = list(self.rss_categories.keys())

        print("ğŸ“° ì „ë¶ë„ë¯¼ì¼ë³´ RSS ìˆ˜ì§‘ê¸° ì‹œì‘")
        print("=" * 50)

        # ë„ë©”ì¸ ì—°ê²° í…ŒìŠ¤íŠ¸
        print("ğŸ” ë„ë©”ì¸ ì—°ê²° ìƒíƒœ í™•ì¸ ì¤‘...")
        if not self.test_connection():
            print("âŒ ë„ë©”ì¸ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ë‚˜ ë„ë©”ì¸ ì£¼ì†Œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return []

        all_articles = []

        for category in selected_categories:
            if category in self.rss_categories:
                rss_file = self.rss_categories[category]
                articles = self.collect_rss_feed(category, rss_file)
                all_articles.extend(articles)

                # ìš”ì²­ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(random.uniform(1.0, 2.0))
            else:
                print(f"âš ï¸  ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬: {category}")

        print("=" * 50)
        print(f"ğŸ“Š ì´ ìˆ˜ì§‘ ê¸°ì‚¬: {len(all_articles)}ê°œ")

        if all_articles:
            saved_file = self.save_to_csv(all_articles)
            if saved_file:
                print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ! íŒŒì¼: {saved_file}")

        return all_articles


def main():
    collector = DominRSSCollector()
    all_articles = []
    for category, rss_file in collector.rss_categories.items():
        articles = collector.collect_rss_feed(category, rss_file)
        all_articles.extend(articles)
        time.sleep(random.uniform(1.0, 2.0))
    if all_articles:
        saved = collector.save_to_csv(all_articles)
        print(f"âœ… ì „ë¶ë„ë¯¼ì¼ë³´ ì „ì²´ {len(all_articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ, íŒŒì¼: {saved}")
    else:
        print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
