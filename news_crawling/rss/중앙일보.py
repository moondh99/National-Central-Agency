import requests
import pandas as pd
from bs4 import BeautifulSoup
import time
import re
from datetime import datetime
import os


# ì„¹ì…˜ ì„¤ì •
SECTIONS = {
    "politics": {"url": "https://www.joongang.co.kr/politics", "name": "ì •ì¹˜"},
    "money": {"url": "https://www.joongang.co.kr/money", "name": "ê²½ì œ"},
    "society": {"url": "https://www.joongang.co.kr/society", "name": "ì‚¬íšŒ"},
    "world": {"url": "https://www.joongang.co.kr/world", "name": "êµ­ì œ"},
    "culture": {"url": "https://www.joongang.co.kr/culture", "name": "ë¬¸í™”"},
}


def get_article_urls_from_page(section_key, page_num=1):
    """
    ì¤‘ì•™ì¼ë³´ íŠ¹ì • ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ URLë“¤ì„ ìˆ˜ì§‘
    section_key: ì„¹ì…˜ í‚¤ ('politics', 'money', 'society', 'world', 'culture')
    page_num: í˜ì´ì§€ ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘)
    """
    section_info = SECTIONS[section_key]

    if page_num == 1:
        url = section_info["url"]
    else:
        url = f"{section_info['url']}?page={page_num}"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")

        # ê¸°ì‚¬ ë§í¬ë“¤ ìˆ˜ì§‘
        article_links = soup.find_all("a", href=re.compile(r"/article/\d+"))

        urls = []
        for link in article_links:
            href = link.get("href")
            if href:
                if href.startswith("/"):
                    href = "https://www.joongang.co.kr" + href
                urls.append(href)

        # ì¤‘ë³µ ì œê±°
        unique_urls = list(set(urls))

        print(f"[{section_info['name']}] í˜ì´ì§€ {page_num}ì—ì„œ {len(unique_urls)}ê°œì˜ ê¸°ì‚¬ URL ìˆ˜ì§‘ ì™„ë£Œ")
        return unique_urls

    except Exception as e:
        print(f"[{section_info['name']}] í˜ì´ì§€ {page_num} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


def extract_article_info(article_url, section_name):
    """
    ê°œë³„ ê¸°ì‚¬ URLì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œ
    article_url: ê¸°ì‚¬ URL
    section_name: ì„¹ì…˜ëª… (ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ, êµ­ì œ, ë¬¸í™”)
    ë°˜í™˜ê°’: dict (ì–¸ë¡ ì‚¬ëª…, ì œëª©, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, ê¸°ìëª…, ë³¸ë¬¸)
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        response = requests.get(article_url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")

        # ì–¸ë¡ ì‚¬ëª… - ê³ ì •ê°’
        media_name = "ì¤‘ì•™ì¼ë³´"

        # ì œëª© ì¶”ì¶œ
        title = ""
        title_elem = soup.find("h1")
        if title_elem:
            title = title_elem.get_text().strip()

        # ë‚ ì§œ ì¶”ì¶œ (ë©”íƒ€ íƒœê·¸ì—ì„œ)
        date = ""
        date_meta = soup.find("meta", {"property": "article:published_time"})
        if date_meta:
            date = date_meta.get("content", "")
            # ISO í˜•ì‹ì„ í•œêµ­ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if date:
                try:
                    dt = datetime.fromisoformat(date.replace("Z", "+00:00"))
                    date = dt.strftime("%Y-%m-%d %H:%M:%S")
                except:
                    pass

        # ì¹´í…Œê³ ë¦¬ - ì„¹ì…˜ëª… ì‚¬ìš©
        category = section_name

        # ê¸°ìëª… ì¶”ì¶œ (ë©”íƒ€ íƒœê·¸ì—ì„œ)
        author = ""
        author_meta = soup.find("meta", {"name": "author"})
        if author_meta:
            author = author_meta.get("content", "")

        # ë³¸ë¬¸ ì¶”ì¶œ
        content = ""
        article_body = soup.find("div", {"id": "article_body"}) or soup.find("div", class_="article_body")
        if article_body:
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ë“¤ ì œê±° í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for script in article_body(["script", "style", "aside", "nav"]):
                script.decompose()
            content = article_body.get_text().strip()
            # ì—¬ëŸ¬ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆì„ ì •ë¦¬
            content = re.sub(r"\s+", " ", content)

        article_data = {
            "ì–¸ë¡ ì‚¬ëª…": media_name,
            "ì œëª©": title,
            "ë‚ ì§œ": date,
            "ì¹´í…Œê³ ë¦¬": category,
            "ê¸°ìëª…": author,
            "ë³¸ë¬¸": content,
        }

        return article_data

    except Exception as e:
        print(f"[{section_name}] ê¸°ì‚¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({article_url}): {e}")
        return {"ì–¸ë¡ ì‚¬ëª…": "ì¤‘ì•™ì¼ë³´", "ì œëª©": "", "ë‚ ì§œ": "", "ì¹´í…Œê³ ë¦¬": section_name, "ê¸°ìëª…": "", "ë³¸ë¬¸": ""}


def crawl_section(section_key, max_pages=5):
    """
    íŠ¹ì • ì„¹ì…˜ì„ í¬ë¡¤ë§
    section_key: ì„¹ì…˜ í‚¤
    max_pages: í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
    """
    section_info = SECTIONS[section_key]
    section_name = section_info["name"]

    print(f"\n{'='*20} [{section_name}] ì„¹ì…˜ í¬ë¡¤ë§ ì‹œì‘ {'='*20}")

    all_articles = []
    all_urls = set()  # ì¤‘ë³µ URL ë°©ì§€

    for page in range(1, max_pages + 1):
        print(f"\n=== [{section_name}] í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘ ===")

        # ê¸°ì‚¬ URLë“¤ ìˆ˜ì§‘
        urls = get_article_urls_from_page(section_key, page)

        if not urls:
            print(f"[{section_name}] í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        # ìƒˆë¡œìš´ URLë§Œ ì²˜ë¦¬
        new_urls = [url for url in urls if url not in all_urls]
        print(f"[{section_name}] ìƒˆë¡œìš´ ê¸°ì‚¬ {len(new_urls)}ê°œ ë°œê²¬")

        if not new_urls:
            print(f"[{section_name}] ë” ì´ìƒ ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            break

        # ê° ê¸°ì‚¬ì˜ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        for i, url in enumerate(new_urls):
            print(f"  [{section_name}] ê¸°ì‚¬ {i+1}/{len(new_urls)} ì²˜ë¦¬ ì¤‘...")
            article_data = extract_article_info(url, section_name)

            if article_data["ì œëª©"]:  # ì œëª©ì´ ì¶”ì¶œëœ ê²½ìš°ë§Œ ì¶”ê°€
                all_articles.append(article_data)
                all_urls.add(url)

            # ìš”ì²­ ê°„ê²© ì¡°ì ˆ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(0.5)

        print(f"[{section_name}] í˜ì´ì§€ {page} ì™„ë£Œ: {len(new_urls)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

        # í˜ì´ì§€ ê°„ ê°„ê²©
        time.sleep(1)

    print(f"\n[{section_name}] ì„¹ì…˜ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
    return all_articles


def crawl_all_sections(sections_to_crawl=None, max_pages=5):
    """
    ì—¬ëŸ¬ ì„¹ì…˜ì„ í¬ë¡¤ë§
    sections_to_crawl: í¬ë¡¤ë§í•  ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ì„¹ì…˜)
    max_pages: ê° ì„¹ì…˜ì—ì„œ í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
    """
    if sections_to_crawl is None:
        sections_to_crawl = list(SECTIONS.keys())

    print("=" * 60)
    print("ì¤‘ì•™ì¼ë³´ ë‹¤ì¤‘ ì„¹ì…˜ í¬ë¡¤ë§ í”„ë¡œê·¸ë¨")
    print(f"í¬ë¡¤ë§ ëŒ€ìƒ ì„¹ì…˜: {', '.join([SECTIONS[s]['name'] for s in sections_to_crawl])}")
    print("=" * 60)

    all_articles = []
    section_results = {}

    for section_key in sections_to_crawl:
        if section_key not in SECTIONS:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì„¹ì…˜: {section_key}")
            continue

        try:
            articles = crawl_section(section_key, max_pages)
            all_articles.extend(articles)
            section_results[SECTIONS[section_key]["name"]] = len(articles)

            # ì„¹ì…˜ ê°„ ê°„ê²©
            time.sleep(2)

        except Exception as e:
            print(f"âŒ [{SECTIONS[section_key]['name']}] ì„¹ì…˜ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            section_results[SECTIONS[section_key]["name"]] = 0

    return all_articles, section_results


def save_to_csv(articles_data, filename=None, split_by_section=False):
    """
    ê¸°ì‚¬ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
    articles_data: ê¸°ì‚¬ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    filename: íŒŒì¼ëª… (Noneì´ë©´ ìë™ ìƒì„±)
    split_by_section: ì„¹ì…˜ë³„ë¡œ íŒŒì¼ì„ ë‚˜ëˆ„ì–´ ì €ì¥í• ì§€ ì—¬ë¶€
    """
    # ê°•ì œ ë‹¨ì¼ íŒŒì¼ ì €ì¥ (split_by_section ë¬´ì‹œ)
    split_by_section = False

    if not articles_data:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # DataFrame ìƒì„±
    df = pd.DataFrame(articles_data)

    # ì—´ ìˆœì„œëŠ” ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ: ì–¸ë¡ ì‚¬ëª…, ì œëª©, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, ê¸°ìëª…, ë³¸ë¬¸
    column_order = ["ì–¸ë¡ ì‚¬ëª…", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
    df = df[column_order]

    # results ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("results", exist_ok=True)

    saved_files = []

    if split_by_section:
        # ì„¹ì…˜ë³„ë¡œ íŒŒì¼ ì €ì¥
        for category in df["ì¹´í…Œê³ ë¦¬"].unique():
            section_df = df[df["ì¹´í…Œê³ ë¦¬"] == category]
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            section_filename = f"results/ì¤‘ì•™ì¼ë³´_{category}_{timestamp}.csv"

            # CSV íŒŒì¼ë¡œ ì €ì¥ (UTF-8 with BOM for Excel compatibility)
            section_df.to_csv(section_filename, index=False, encoding="utf-8-sig")

            print(f"âœ“ [{category}] CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {section_filename}")
            print(f"  - {len(section_df)}ê°œ ê¸°ì‚¬, {os.path.getsize(section_filename):,} bytes")
            saved_files.append(section_filename)
    else:
        # í†µí•© íŒŒì¼ë¡œ ì €ì¥
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"results/ì¤‘ì•™ì¼ë³´_ì „ì²´_{timestamp}.csv"

        # CSV íŒŒì¼ë¡œ ì €ì¥ (UTF-8 with BOM for Excel compatibility)
        df.to_csv(filename, index=False, encoding="utf-8-sig")

        print(f"âœ“ í†µí•© CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"  - ì´ {len(df)}ê°œ ê¸°ì‚¬, {os.path.getsize(filename):,} bytes")
        saved_files.append(filename)

    return saved_files


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("ì¤‘ì•™ì¼ë³´ ë‹¤ì¤‘ ì„¹ì…˜ í¬ë¡¤ë§ í”„ë¡œê·¸ë¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("\nì‚¬ìš© ê°€ëŠ¥í•œ ì„¹ì…˜:")
    for key, info in SECTIONS.items():
        print(f"  - {key}: {info['name']}")

    # ì‚¬ìš©ì ì„¤ì • (ì—¬ê¸°ì„œ ìˆ˜ì • ê°€ëŠ¥)
    # ëª¨ë“  ì„¹ì…˜ í¬ë¡¤ë§: None
    # íŠ¹ì • ì„¹ì…˜ë§Œ í¬ë¡¤ë§: ['politics', 'money'] ë“±
    sections_to_crawl = None  # ëª¨ë“  ì„¹ì…˜
    max_pages = 5  # ê° ì„¹ì…˜ë‹¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜
    split_by_section = False  # í†µí•© íŒŒì¼ë§Œ ì €ì¥

    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        articles, section_results = crawl_all_sections(sections_to_crawl, max_pages)

        if articles:
            # CSV íŒŒì¼ë¡œ ì €ì¥
            saved_files = save_to_csv(articles, split_by_section=split_by_section)

            # ê²°ê³¼ ìš”ì•½
            print("\n" + "=" * 60)
            print("í¬ë¡¤ë§ ì™„ë£Œ ê²°ê³¼")
            print("=" * 60)
            print(f"âœ“ ì´ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜: {len(articles):,}ê°œ")

            print(f"\nğŸ“Š ì„¹ì…˜ë³„ ìˆ˜ì§‘ ê²°ê³¼:")
            for section, count in section_results.items():
                print(f"  - {section}: {count:,}ê°œ")

            print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
            for file in saved_files:
                print(f"  - {file}")

            # ìƒ˜í”Œ ë°ì´í„° í‘œì‹œ
            df = pd.DataFrame(articles)
            print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
            category_counts = df["ì¹´í…Œê³ ë¦¬"].value_counts()
            for category, count in category_counts.items():
                print(f"  - {category}: {count}ê°œ")

            print(f"\nğŸ“° ìµœì‹  ê¸°ì‚¬ 5ê°œ (ì „ì²´ ì„¹ì…˜):")
            # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬
            df_sorted = df.sort_values("ë‚ ì§œ", ascending=False)
            for i in range(min(5, len(df_sorted))):
                row = df_sorted.iloc[i]
                print(f"{i+1}. [{row['ì¹´í…Œê³ ë¦¬']}] {row['ì œëª©']}")
                print(f"   ê¸°ì: {row['ê¸°ìëª…']}, ë‚ ì§œ: {row['ë‚ ì§œ']}")
                print()

            return saved_files
        else:
            print("âŒ í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

    except Exception as e:
        print(f"âŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def crawl_specific_sections(section_list, max_pages=5, split_files=True):

    articles, section_results = crawl_all_sections(section_list, max_pages)

    if articles:
        saved_files = save_to_csv(articles, split_by_section=split_files)
        print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        return saved_files
    else:
        print("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
        return None


if __name__ == "__main__":
    # ë©”ì¸ í”„ë¡œê·¸ë¨ ì‹¤í–‰
    result_files = main()
