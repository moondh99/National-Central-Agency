import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import pandas as pd
import time
import random
import re
from datetime import datetime
import csv
from urllib.parse import urljoin, urlparse
import logging
import os


class KoreaCommitteeRSSCrawler:
    def __init__(self):
        """ì •ì±…ë¸Œë¦¬í•‘ ì •ë¶€ ìœ„ì›íšŒë³„ RSS í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.base_url = "https://www.korea.kr"

        # 6ê°œ ì •ë¶€ ìœ„ì›íšŒ RSS í”¼ë“œ
        self.committee_feeds = {
            "ë°©ì†¡í†µì‹ ìœ„ì›íšŒ": "https://www.korea.kr/rss/dept_kcc.xml",
            "ì›ìë ¥ì•ˆì „ìœ„ì›íšŒ": "https://www.korea.kr/rss/dept_nssc.xml",
            "ê³µì •ê±°ë˜ìœ„ì›íšŒ": "https://www.korea.kr/rss/dept_ftc.xml",
            "ê¸ˆìœµìœ„ì›íšŒ": "https://www.korea.kr/rss/dept_fsc.xml",
            "êµ­ë¯¼ê¶Œìµìœ„ì›íšŒ": "https://www.korea.kr/rss/dept_acrc.xml",
            "ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ": "https://www.korea.kr/rss/dept_pipc.xml",
        }

        # ìœ„ì›íšŒë³„ ì£¼ìš” ì—…ë¬´ ë¶„ì•¼ (ë¶„ì„ìš©)
        self.committee_areas = {
            "ë°©ì†¡í†µì‹ ìœ„ì›íšŒ": "ë°©ì†¡ì •ì±…, í†µì‹ ì •ì±…, ë¯¸ë””ì–´ê·œì œ, ì¸í„°ë„·ì •ì±…",
            "ì›ìë ¥ì•ˆì „ìœ„ì›íšŒ": "ì›ìë ¥ì•ˆì „, ë°©ì‚¬ëŠ¥ë°©ì¬, ì›ì „ì•ˆì „ê·œì œ",
            "ê³µì •ê±°ë˜ìœ„ì›íšŒ": "ê³µì •ê±°ë˜, ë…ì ê·œì œ, ì†Œë¹„ìë³´í˜¸, ê²½ìŸì •ì±…",
            "ê¸ˆìœµìœ„ì›íšŒ": "ê¸ˆìœµì •ì±…, ê¸ˆìœµê°ë…, ìë³¸ì‹œì¥, ê¸ˆìœµì†Œë¹„ìë³´í˜¸",
            "êµ­ë¯¼ê¶Œìµìœ„ì›íšŒ": "ë¶€íŒ¨ë°©ì§€, êµ­ë¯¼ì‹ ë¬¸ê³ , í–‰ì •ì‹¬íŒ, ê°ˆë“±ì¡°ì •",
            "ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ": "ê°œì¸ì •ë³´ë³´í˜¸, í”„ë¼ì´ë²„ì‹œì •ì±…, ì •ë³´ë³´ì•ˆ",
        }

        # ìœ„ì›íšŒ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        self.committee_categories = {
            "ë°©ì†¡Â·í†µì‹ ": ["ë°©ì†¡í†µì‹ ìœ„ì›íšŒ"],
            "ì›ìë ¥Â·ì•ˆì „": ["ì›ìë ¥ì•ˆì „ìœ„ì›íšŒ"],
            "ê²½ì œÂ·ê³µì •ê±°ë˜": ["ê³µì •ê±°ë˜ìœ„ì›íšŒ", "ê¸ˆìœµìœ„ì›íšŒ"],
            "ê¶ŒìµÂ·ì •ë³´ë³´í˜¸": ["êµ­ë¯¼ê¶Œìµìœ„ì›íšŒ", "ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ"],
        }

        # ìœ„ì›íšŒë³„ ì£¼ìš” í‚¤ì›Œë“œ
        self.committee_keywords = {
            "ë°©ì†¡í†µì‹ ìœ„ì›íšŒ": ["ë°©ì†¡", "í†µì‹ ", "ë¯¸ë””ì–´", "ICT", "ì¸í„°ë„·", "ë°©ì†¡ë²•", "í†µì‹ ë²•", "í”Œë«í¼"],
            "ì›ìë ¥ì•ˆì „ìœ„ì›íšŒ": ["ì›ìë ¥", "ì›ì „", "ë°©ì‚¬ëŠ¥", "ì•ˆì „ê·œì œ", "í•µì•ˆì „", "ë°©ì‚¬ì„ "],
            "ê³µì •ê±°ë˜ìœ„ì›íšŒ": ["ê³µì •ê±°ë˜", "ë…ì ", "ë‹´í•©", "ì†Œë¹„ì", "ê²½ìŸ", "ì¹´ë¥´í…”", "ì‹œì¥ì§€ë°°ë ¥"],
            "ê¸ˆìœµìœ„ì›íšŒ": ["ê¸ˆìœµ", "ì€í–‰", "ì¦ê¶Œ", "ë³´í—˜", "ìë³¸ì‹œì¥", "ê¸ˆìœµì†Œë¹„ì", "í•€í…Œí¬"],
            "êµ­ë¯¼ê¶Œìµìœ„ì›íšŒ": ["ë¶€íŒ¨ë°©ì§€", "ì‹ ë¬¸ê³ ", "í–‰ì •ì‹¬íŒ", "ê°ˆë“±ì¡°ì •", "ê³µìµì‹ ê³ ", "ì˜´ë¶€ì¦ˆë§Œ"],
            "ê°œì¸ì •ë³´ë³´í˜¸ìœ„ì›íšŒ": ["ê°œì¸ì •ë³´", "í”„ë¼ì´ë²„ì‹œ", "ì •ë³´ë³´í˜¸", "GDPR", "ë°ì´í„°", "ë™ì˜"],
        }

        # User-Agent ë¦¬ìŠ¤íŠ¸
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
        ]

        self.articles = []
        self.session = requests.Session()

        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
        self.logger = logging.getLogger(__name__)

    def get_random_headers(self):
        """ëœë¤ í—¤ë” ìƒì„±"""
        return {
            "User-Agent": random.choice(self.user_agents),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }

    def random_delay(self, min_delay=1, max_delay=3):
        """ëœë¤ ë”œë ˆì´"""
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)

    def fetch_rss_feed(self, rss_url, max_retries=3):
        """RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°"""
        for attempt in range(max_retries):
            try:
                headers = self.get_random_headers()
                response = self.session.get(rss_url, headers=headers, timeout=30)
                response.raise_for_status()
                response.encoding = "utf-8"
                return response.text
            except Exception as e:
                self.logger.warning(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    self.random_delay(2, 5)
                else:
                    self.logger.error(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ìµœì¢… ì‹¤íŒ¨: {rss_url}")
                    return None

    def parse_rss_feed(self, rss_content):
        """RSS í”¼ë“œ íŒŒì‹±"""
        try:
            root = ET.fromstring(rss_content)
            items = []

            for item in root.findall(".//item"):
                article_info = {}

                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title_elem = item.find("title")
                article_info["title"] = title_elem.text.strip() if title_elem is not None else ""

                link_elem = item.find("link")
                article_info["link"] = link_elem.text.strip() if link_elem is not None else ""

                pubdate_elem = item.find("pubDate")
                article_info["pub_date"] = pubdate_elem.text.strip() if pubdate_elem is not None else ""

                guid_elem = item.find("guid")
                article_info["guid"] = guid_elem.text.strip() if guid_elem is not None else ""

                # dc:creator ì¶”ì¶œ (ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê³ ë ¤)
                creator_elem = item.find(".//{http://purl.org/dc/elements/1.1/}creator")
                article_info["creator"] = creator_elem.text.strip() if creator_elem is not None else ""

                # descriptionì—ì„œ ì „ì²´ ë‚´ìš© ì¶”ì¶œ
                desc_elem = item.find("description")
                if desc_elem is not None:
                    desc_text = desc_elem.text or ""
                    # CDATA ì²˜ë¦¬
                    if desc_text.startswith("<![CDATA[") and desc_text.endswith("]]>"):
                        desc_text = desc_text[9:-3]
                    # HTML íƒœê·¸ ì œê±°í•˜ì—¬ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                    soup = BeautifulSoup(desc_text, "html.parser")
                    article_info["description"] = soup.get_text().strip()
                else:
                    article_info["description"] = ""

                items.append(article_info)

            return items
        except Exception as e:
            self.logger.error(f"RSS íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []

    def extract_article_content(self, article_url, max_retries=3):
        """ê°œë³„ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ - ìœ„ì›íšŒ í˜ì´ì§€ ìµœì í™”"""
        for attempt in range(max_retries):
            try:
                headers = self.get_random_headers()
                response = self.session.get(article_url, headers=headers, timeout=30)
                response.raise_for_status()
                response.encoding = "utf-8"

                soup = BeautifulSoup(response.text, "html.parser")

                # ìœ„ì›íšŒë³„ í˜ì´ì§€ êµ¬ì¡°ì— ìµœì í™”ëœ ë³¸ë¬¸ ì¶”ì¶œ ì…€ë ‰í„°
                content_selectors = [
                    ".committee_cont",  # ìœ„ì›íšŒ ì½˜í…ì¸ 
                    ".press_cont",  # ë³´ë„ìë£Œ ì½˜í…ì¸ 
                    ".decision_cont",  # ì˜ê²°ì‚¬í•­ ì½˜í…ì¸ 
                    ".article_body",  # ì¼ë°˜ ê¸°ì‚¬
                    ".rbody",  # ë¸Œë¦¬í•‘ í˜ì´ì§€
                    ".view_cont",  # ë·° í˜ì´ì§€
                    ".cont_body",  # ì½˜í…ì¸  ë³¸ë¬¸
                    ".policy_body",  # ì •ì±… ë³¸ë¬¸
                    ".briefing_cont",  # ë¸Œë¦¬í•‘ ë‚´ìš©
                    ".news_cont",  # ë‰´ìŠ¤ ë‚´ìš©
                    ".notice_cont",  # ê³µì§€ì‚¬í•­ ë‚´ìš©
                    ".regulation_cont",  # ê·œì œ ë‚´ìš©
                ]

                content = ""
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text().strip()
                        break

                # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„
                if not content:
                    # í—¤ë”, í‘¸í„°, ì‚¬ì´ë“œë°” ë“± ì œê±°
                    for elem in soup.find_all(["header", "footer", "nav", "aside", "script", "style"]):
                        elem.decompose()

                    main_content = soup.find("main") or soup.find("div", class_="content") or soup.find("body")
                    if main_content:
                        content = main_content.get_text().strip()

                # ìœ„ì›íšŒë³„ íŠ¹í™” ì •ë³´ ì¶”ì¶œ
                contact_info = self.extract_committee_contact_info(content)
                regulation_keywords = self.extract_regulation_keywords(content)
                decision_type = self.extract_decision_type(content)

                # í…ìŠ¤íŠ¸ ì •ë¦¬
                content = re.sub(r"\s+", " ", content).strip()

                return {
                    "content": content[:3500] + "..." if len(content) > 3500 else content,
                    "contact_info": contact_info,
                    "regulation_keywords": regulation_keywords,
                    "decision_type": decision_type,
                }

            except Exception as e:
                self.logger.warning(f"ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    self.random_delay(1, 3)
                else:
                    return {"content": "ì¶”ì¶œ ì‹¤íŒ¨", "contact_info": "", "regulation_keywords": "", "decision_type": ""}

    def extract_committee_contact_info(self, content):
        """ìœ„ì›íšŒ ì—°ë½ì²˜/ë‹´ë‹¹ì ì •ë³´ ì¶”ì¶œ"""
        # ìœ„ì›íšŒë³„ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ íŒ¨í„´
        patterns = [
            r"ë¬¸ì˜\s*:\s*([^(]+?)(?:\(([^)]+)\))?",
            r"ë‹´ë‹¹\s*:\s*([^(]+?)(?:\(([^)]+)\))?",
            r"ì—°ë½ì²˜\s*:\s*([^(]+?)(?:\(([^)]+)\))?",
            r"ë¬¸ì˜ì²˜\s*:\s*([^(]+?)(?:\(([^)]+)\))?",
            r"ë‹´ë‹¹ë¶€ì„œ\s*:\s*([^(]+?)(?:\(([^)]+)\))?",
            r"ë‹´ë‹¹ì\s*:\s*([^(]+?)(?:\(([^)]+)\))?",
            r"ìœ„ì›íšŒ\s+([ê°€-í£]+ê³¼|[ê°€-í£]+íŒ€|[ê°€-í£]+êµ­)\s*(?:\(([^)]+)\))?",
            r"í™ˆí˜ì´ì§€\s*:\s*(https?://[^\s]+)",
        ]

        contact_info = {}

        for pattern in patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if isinstance(match, tuple):
                    if len(match) >= 2:
                        dept = match[0].strip() if match[0] else ""
                        contact = match[1].strip() if match[1] else ""

                        if dept and len(dept) > 1 and len(dept) < 100:
                            contact_info["department"] = dept
                        if contact:
                            if "http" in contact:
                                contact_info["website"] = contact
                            elif any(prefix in contact for prefix in ["02-", "044-", "070-"]):
                                contact_info["phone"] = contact
                else:
                    if match and len(match) > 1 and len(match) < 100:
                        contact_info["department"] = match.strip()

        return "; ".join([f"{k}: {v}" for k, v in contact_info.items()])

    def extract_regulation_keywords(self, content):
        """ê·œì œ/ì •ì±… í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ìœ„ì›íšŒë³„ ê·œì œ/ì •ì±… í‚¤ì›Œë“œ íŒ¨í„´
        regulation_patterns = [
            r"(ê·œì œ|ì œì¬|ì²˜ë¶„|ì¡°ì¹˜|ëª…ë ¹|ê¶Œê³ |ê°œì„ |ì‹œì •)",
            r"(ì‹¬ì˜|ì˜ê²°|ê²°ì •|ìŠ¹ì¸|í—ˆê°€|ì¸ê°€|ë“±ë¡)",
            r"(ë²•ë¥ |ë²•ë ¹|ê·œì •|ê¸°ì¤€|ê°€ì´ë“œë¼ì¸|ì§€ì¹¨)",
            r"(ì¡°ì‚¬|ì ê²€|ê°ì‚¬|ê°ë…|ëª¨ë‹ˆí„°ë§|í‰ê°€)",
            r"(ê³¼ì§•ê¸ˆ|ê³¼íƒœë£Œ|ê²½ê³ |ì£¼ì˜|ì‹œì •ëª…ë ¹)",
            r"(ê³µì²­íšŒ|ê°„ë‹´íšŒ|í† ë¡ íšŒ|ì„¤ëª…íšŒ|ì˜ê²¬ìˆ˜ë ´)",
            r"(ê°œì •|ì œì •|íì§€|ì‹ ì„¤|ê°•í™”|ì™„í™”)",
        ]

        keywords = set()
        for pattern in regulation_patterns:
            matches = re.findall(pattern, content)
            keywords.update(matches)

        return ", ".join(list(keywords)[:12])  # ìµœëŒ€ 12ê°œ í‚¤ì›Œë“œ

    def extract_decision_type(self, content):
        """ì˜ê²°/ê²°ì • ìœ í˜• ì¶”ì¶œ"""
        decision_patterns = [
            r"(ë³´ë„ìë£Œ|ì–¸ë¡ ë°°í¬|ë°œí‘œ)",
            r"(ì˜ê²°|ê²°ì •|ìŠ¹ì¸)",
            r"(ê³ ì‹œ|ê³µê³ |ê³µì‹œ)",
            r"(ê·œì¹™|ê³ ì‹œ|í›ˆë ¹)",
            r"(ì •ì±…|ì œë„|ë°©ì•ˆ)",
            r"(ì¡°ì‚¬ê²°ê³¼|ê°ì‚¬ê²°ê³¼|ì ê²€ê²°ê³¼)",
        ]

        for pattern in decision_patterns:
            if re.search(pattern, content):
                matches = re.findall(pattern, content)
                if matches:
                    return matches[0]

        return ""

    def get_committee_category(self, committee_name):
        """ìœ„ì›íšŒì˜ ì¹´í…Œê³ ë¦¬ ë°˜í™˜"""
        for category, committees in self.committee_categories.items():
            if committee_name in committees:
                return category
        return "ê¸°íƒ€"

    def get_relevant_keywords(self, committee_name, content):
        """ìœ„ì›íšŒë³„ ê´€ë ¨ í‚¤ì›Œë“œ ë§¤ì¹­"""
        if committee_name in self.committee_keywords:
            keywords = self.committee_keywords[committee_name]
            found_keywords = []
            for keyword in keywords:
                if keyword in content:
                    found_keywords.append(keyword)
            return ", ".join(found_keywords[:8])  # ìµœëŒ€ 8ê°œ
        return ""

    def crawl_committee_feed(self, committee, rss_url, max_items=30):
        """ê°œë³„ ìœ„ì›íšŒ RSS í”¼ë“œ í¬ë¡¤ë§"""
        self.logger.info(f"ìœ„ì›íšŒ í¬ë¡¤ë§ ì‹œì‘: {committee}")

        # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
        rss_content = self.fetch_rss_feed(rss_url)
        if not rss_content:
            return

        # RSS íŒŒì‹±
        rss_items = self.parse_rss_feed(rss_content)
        if not rss_items:
            self.logger.warning(f"RSS ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤: {committee}")
            return

        # ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ë§Œ ì²˜ë¦¬
        items_to_process = rss_items[:max_items]

        for i, item in enumerate(items_to_process, 1):
            try:
                self.logger.info(f"{committee} ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘: {i}/{len(items_to_process)} - {item['title'][:50]}...")

                # ê¸°ì‚¬ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
                if item["link"]:
                    article_detail = self.extract_article_content(item["link"])

                    article_data = {
                        "committee": committee,
                        "committee_category": self.get_committee_category(committee),
                        "business_area": self.committee_areas.get(committee, ""),
                        "title": item["title"],
                        "link": item["link"],
                        "pub_date": item["pub_date"],
                        "creator": item["creator"],
                        "description": item["description"],
                        "content": article_detail["content"],
                        "contact_info": article_detail["contact_info"],
                        "regulation_keywords": article_detail["regulation_keywords"],
                        "decision_type": article_detail["decision_type"],
                        "relevant_keywords": self.get_relevant_keywords(committee, article_detail["content"]),
                        "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    }

                    self.articles.append(article_data)

                # ë”œë ˆì´
                self.random_delay(1, 3)

            except Exception as e:
                self.logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue

        self.logger.info(f"{committee} í¬ë¡¤ë§ ì™„ë£Œ: {len(items_to_process)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬")

    def crawl_all_committees(self, max_items_per_committee=30):
        """ëª¨ë“  ìœ„ì›íšŒ RSS í”¼ë“œ í¬ë¡¤ë§"""
        total_committees = len(self.committee_feeds)
        self.logger.info(f"ì „ì²´ {total_committees}ê°œ ìœ„ì›íšŒ RSS í”¼ë“œ í¬ë¡¤ë§ ì‹œì‘")

        for i, (committee, rss_url) in enumerate(self.committee_feeds.items(), 1):
            try:
                self.logger.info(f"[{i}/{total_committees}] {committee} í”¼ë“œ í¬ë¡¤ë§ ì¤‘...")
                self.crawl_committee_feed(committee, rss_url, max_items_per_committee)

                # ìœ„ì›íšŒ ê°„ ë”œë ˆì´
                if i < total_committees:
                    self.random_delay(3, 6)

            except Exception as e:
                self.logger.error(f"{committee} ìœ„ì›íšŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                continue

        self.logger.info(f"ì „ì²´ ìœ„ì›íšŒ í¬ë¡¤ë§ ì™„ë£Œ: {len(self.articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        self.print_statistics()

    def crawl_specific_committees(self, committee_names, max_items_per_committee=30):
        """íŠ¹ì • ìœ„ì›íšŒë“¤ë§Œ í¬ë¡¤ë§"""
        for committee_name in committee_names:
            if committee_name in self.committee_feeds:
                self.crawl_committee_feed(committee_name, self.committee_feeds[committee_name], max_items_per_committee)
            else:
                self.logger.warning(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ìœ„ì›íšŒ: {committee_name}")
                available_committees = list(self.committee_feeds.keys())
                self.logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ìœ„ì›íšŒ: {available_committees}")

    def crawl_by_category(self, categories, max_items_per_committee=25):
        """ì¹´í…Œê³ ë¦¬ë³„ ìœ„ì›íšŒ í¬ë¡¤ë§"""
        target_committees = []

        for category in categories:
            if category in self.committee_categories:
                target_committees.extend(self.committee_categories[category])
            else:
                self.logger.warning(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬: {category}")

        if target_committees:
            self.logger.info(f"ì¹´í…Œê³ ë¦¬ '{', '.join(categories)}'ì— í•´ë‹¹í•˜ëŠ” ìœ„ì›íšŒ: {target_committees}")
            self.crawl_specific_committees(target_committees, max_items_per_committee)
        else:
            self.logger.warning(f"í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ë§ëŠ” ìœ„ì›íšŒë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {categories}")

    def save_to_csv(self, filename=None):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"results/ìœ„ì›íšŒë³„_RSS_{timestamp}.csv"

        # ê³ ì • ì»¬ëŸ¼ ìˆœì„œë¡œ CSV ì €ì¥
        fieldnames = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        try:
            with open(filename, "w", newline="", encoding="utf-8-sig") as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                for art in self.articles:
                    writer.writerow(
                        {
                            "ì–¸ë¡ ì‚¬": "ì •ì±…í¬í„¸_ìœ„ì›íšŒë³„",
                            "ì œëª©": art.get("title", ""),
                            "ë‚ ì§œ": art.get("pub_date", ""),
                            "ì¹´í…Œê³ ë¦¬": art.get("committee_category", ""),
                            "ê¸°ìëª…": "ì •ì±…í¬í„¸",
                            "ë³¸ë¬¸": art.get("description", ""),
                        }
                    )
            self.logger.info(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            self.logger.info(f"ì´ {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥")
        except Exception as e:
            self.logger.error(f"CSV ì €ì¥ ì˜¤ë¥˜: {e}")

    def save_by_committee(self):
        """ìœ„ì›íšŒë³„ë¡œ ê°œë³„ CSV íŒŒì¼ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        df = pd.DataFrame(self.articles)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        for committee in df["committee"].unique():
            committee_df = df[df["committee"] == committee]
            filename = f"results/ìœ„ì›íšŒ_{committee}_{timestamp}.csv"
            committee_df.to_csv(filename, index=False, encoding="utf-8-sig")
            self.logger.info(f"{committee} ì €ì¥ ì™„ë£Œ: {filename} ({len(committee_df)}ê°œ ê¸°ì‚¬)")

    def save_by_category(self):
        """ì¹´í…Œê³ ë¦¬ë³„ë¡œ CSV íŒŒì¼ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        df = pd.DataFrame(self.articles)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        for category in df["committee_category"].unique():
            category_df = df[df["committee_category"] == category]
            filename = f"results/ìœ„ì›íšŒì¹´í…Œê³ ë¦¬_{category}_{timestamp}.csv"
            category_df.to_csv(filename, index=False, encoding="utf-8-sig")
            self.logger.info(f"{category} ì¹´í…Œê³ ë¦¬ ì €ì¥ ì™„ë£Œ: {filename} ({len(category_df)}ê°œ ê¸°ì‚¬)")

    def print_statistics(self):
        """í¬ë¡¤ë§ í†µê³„ ì¶œë ¥"""
        if not self.articles:
            return

        df = pd.DataFrame(self.articles)

        print("\n" + "=" * 60)
        print("ì •ë¶€ ìœ„ì›íšŒë³„ RSS í¬ë¡¤ë§ í†µê³„")
        print("=" * 60)

        # ìœ„ì›íšŒë³„ í†µê³„
        committee_stats = df["committee"].value_counts()
        print(f"\nğŸ›ï¸ ìœ„ì›íšŒë³„ ê¸°ì‚¬ ìˆ˜:")
        for committee, count in committee_stats.items():
            business_area = self.committee_areas.get(committee, "")
            print(f"  â€¢ {committee} ({business_area}): {count}ê°œ")

        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = df["committee_category"].value_counts()
        print(f"\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜:")
        for category, count in category_stats.items():
            print(f"  â€¢ {category}: {count}ê°œ")

        # ì˜ê²°/ê²°ì • ìœ í˜•ë³„ í†µê³„
        decision_stats = df[df["decision_type"] != ""]["decision_type"].value_counts().head(8)
        if not decision_stats.empty:
            print(f"\nâš–ï¸ ì£¼ìš” ì˜ê²°/ê²°ì • ìœ í˜•ë³„ ê¸°ì‚¬ ìˆ˜:")
            for decision_type, count in decision_stats.items():
                print(f"  â€¢ {decision_type}: {count}ê°œ")

        # ê·œì œ í‚¤ì›Œë“œ í†µê³„
        regulation_available = len(df[df["regulation_keywords"] != ""])
        print(f"\nğŸ“‹ ê·œì œ/ì •ì±… í‚¤ì›Œë“œ:")
        print(f"  â€¢ í‚¤ì›Œë“œ ì¶”ì¶œ ì„±ê³µ: {regulation_available}ê°œ")
        print(f"  â€¢ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {len(df) - regulation_available}ê°œ")

        # ì—°ë½ì²˜ ì •ë³´ í†µê³„
        contact_available = len(df[df["contact_info"] != ""])
        print(f"\nğŸ“ ì—°ë½ì²˜ ì •ë³´:")
        print(f"  â€¢ ì—°ë½ì²˜ ì¶”ì¶œ ì„±ê³µ: {contact_available}ê°œ")
        print(f"  â€¢ ì—°ë½ì²˜ ì¶”ì¶œ ì‹¤íŒ¨: {len(df) - contact_available}ê°œ")

        print(f"\nğŸ“ˆ ì „ì²´ ìš”ì•½:")
        print(f"  â€¢ ì´ ê¸°ì‚¬ ìˆ˜: {len(self.articles)}ê°œ")
        print(f"  â€¢ í¬ë¡¤ë§ ìœ„ì›íšŒ ìˆ˜: {len(committee_stats)}ê°œ")
        print(f"  â€¢ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {len(df[df['content'] != 'ì¶”ì¶œ ì‹¤íŒ¨'])}ê°œ")
        print(f"  â€¢ ê´€ë ¨ í‚¤ì›Œë“œ ë§¤ì¹­: {len(df[df['relevant_keywords'] != ''])}ê°œ")
        print("=" * 60)

    def get_available_committees(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ìœ„ì›íšŒ ëª©ë¡ ë°˜í™˜"""
        return list(self.committee_feeds.keys())

    def get_categories(self):
        """ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë°˜í™˜"""
        return list(self.committee_categories.keys())


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ì •ì±…ë¸Œë¦¬í•‘ ì •ë¶€ ìœ„ì›íšŒë³„ RSS í¬ë¡¤ëŸ¬")
    print("=" * 50)

    crawler = KoreaCommitteeRSSCrawler()

    # ìë™ ì „ì²´ ìœ„ì›íšŒ í¬ë¡¤ë§ (ê° ìœ„ì›íšŒë‹¹ 20ê°œì”©)
    print("ì „ì²´ ìœ„ì›íšŒ RSS í”¼ë“œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ê° ìœ„ì›íšŒë‹¹ 20ê±´)")
    crawler.crawl_all_committees(max_items_per_committee=20)

    # CSV ì €ì¥
    crawler.save_to_csv()

    print("\nìœ„ì›íšŒë³„ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
