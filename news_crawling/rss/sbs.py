import requests
import xml.etree.ElementTree as ET
import csv
import re
from datetime import datetime
from bs4 import BeautifulSoup
import time


def extract_sbs_article_content(url):
    """SBS ë‰´ìŠ¤ ê¸°ì‚¬ URLì—ì„œ ì „ì²´ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        response.encoding = "utf-8"

        soup = BeautifulSoup(response.text, "html.parser")

        # SBS ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„)
        content_selectors = [
            "div.news-content",  # ë‰´ìŠ¤ ì»¨í…ì¸ 
            "div.article-content",  # ê¸°ì‚¬ ì»¨í…ì¸ 
            "div.content",  # ì»¨í…ì¸ 
            "div.article-body",  # ê¸°ì‚¬ ë³¸ë¬¸
            "div.news-text",  # ë‰´ìŠ¤ í…ìŠ¤íŠ¸
            ".news_txt",  # ë‰´ìŠ¤ í…ìŠ¤íŠ¸ í´ë˜ìŠ¤
            "div.view-content",  # ë·° ì»¨í…ì¸ 
            "#content",  # ID ê¸°ë°˜ ì»¨í…ì¸ 
            "div.text_area",  # í…ìŠ¤íŠ¸ ì˜ì—­
            "section.article-content",  # ì„¹ì…˜ ê¸°ì‚¬ ì»¨í…ì¸ 
            "div.detail-content",  # ìƒì„¸ ì»¨í…ì¸ 
        ]

        full_content = ""

        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                for element in elements:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ ì œê±°
                    for unwanted in element.find_all(
                        [
                            "script",
                            "style",
                            "iframe",
                            "ins",
                            "div.ad",
                            ".advertisement",
                            ".related-articles",
                            ".tags",
                            ".share",
                            ".comment",
                            ".footer",
                            "div.reporter",
                            ".reporter_info",
                            ".social",
                            ".video",
                            ".photo",
                            ".image",
                            ".btn",
                        ]
                    ):
                        unwanted.decompose()

                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text = element.get_text(separator="\n", strip=True)
                    if text and len(text) > len(full_content):
                        full_content = text
                        break

                if full_content:
                    break

        # ë³¸ë¬¸ì´ ì—¬ì „íˆ ì§§ë‹¤ë©´ ì „ì²´ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if len(full_content) < 100:
            # SBS ê¸°ì‚¬ì˜ ê²½ìš° ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ êµ¬ì¡°
            page_text = soup.get_text(separator="\n", strip=True)
            lines = page_text.split("\n")

            # ë³¸ë¬¸ ì‹œì‘ê³¼ ëì„ ì°¾ê¸°
            content_lines = []
            start_found = False

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                # ë³¸ë¬¸ ì‹œì‘ ì¡°ê±´
                if not start_found:
                    if (
                        len(line) > 20
                        and not line.startswith("[")
                        and not line.startswith("Copyright")
                        and not line.startswith("â–¶")
                        and "SBS" not in line
                        and "ì œë³´" not in line
                    ):
                        start_found = True
                        content_lines.append(line)
                    continue

                # ë³¸ë¬¸ ë ì¡°ê±´
                if (
                    line.startswith("(ì‚¬ì§„=")
                    or line.startswith("Copyright")
                    or line.startswith("â–¶")
                    or "SBS" in line
                    and ("ì œë³´" in line or "ì•±" in line)
                    or "ë¬´ë‹¨ ì „ì¬" in line
                    or "AIí•™ìŠµ" in line
                ):
                    break

                # ìœ íš¨í•œ ë³¸ë¬¸ ë¼ì¸ ì¶”ê°€
                if len(line) > 3:
                    content_lines.append(line)

            if content_lines:
                full_content = "\n".join(content_lines)

        # í…ìŠ¤íŠ¸ ì •ë¦¬
        if full_content:
            # SBS ê´€ë ¨ ì •ë³´ ì œê±°
            full_content = re.sub(r"Copyright.*?SBS.*?ê¸ˆì§€", "", full_content, flags=re.DOTALL)
            full_content = re.sub(r"â–¶.*?SBS.*?$", "", full_content, flags=re.MULTILINE)
            full_content = re.sub(r"\(ì‚¬ì§„=.*?\)", "", full_content)
            # ê¸°ìëª… ë¼ì¸ ì œê±° (ë§ˆì§€ë§‰ì— ìˆëŠ” ê²½ìš°)
            full_content = re.sub(r"\n[ê°€-í£]{2,4}\s*(ê¸°ì|íŠ¹íŒŒì›).*?$", "", full_content, flags=re.MULTILINE)
            # ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
            full_content = re.sub(r"\n+", "\n", full_content)
            full_content = re.sub(r"\s+", " ", full_content)
            full_content = full_content.strip()

        return full_content

    except Exception as e:
        print(f"ë³¸ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""


def extract_sbs_reporter_name(soup, article_text):
    """SBS ë‰´ìŠ¤ ê¸°ìëª…ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        # SBS ë‰´ìŠ¤ì˜ ê¸°ìëª… ì¶”ì¶œ íŒ¨í„´
        reporter_patterns = [
            # HTMLì—ì„œ ê¸°ìëª… ì¶”ì¶œ
            r"<span[^>]*class[^>]*reporter[^>]*>([^<]+)</span>",
            r"<div[^>]*class[^>]*reporter[^>]*>([^<]+)</div>",
            # í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìëª… ì¶”ì¶œ (SBS íŠ¹ì„±ì— ë§ê²Œ)
            r"([ê°€-í£]{2,4})\s*(ê¸°ì|íŠ¹íŒŒì›)(?:\s*=|\s*âˆ™|\s*Â·|\s*ì…ë ¥|\s*ìˆ˜ì •|\s*ì‘ì„±)",
            r"ê¸°ì\s*([ê°€-í£]{2,4})(?:\s*=|\s*âˆ™|\s*Â·)",
            r"([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›",
            r"([ê°€-í£]{2,4})\s*ì•µì»¤",
            r"([ê°€-í£]{2,4})\s*ë…¼ì„¤ìœ„ì›",
            r"([ê°€-í£]{2,4})\s*í¸ì§‘ìœ„ì›",
            r"/\s*([ê°€-í£]{2,4})\s*ê¸°ì",
            r"=\s*([ê°€-í£]{2,4})\s*ê¸°ì",
            r"âˆ™\s*([ê°€-í£]{2,4})\s*ê¸°ì",
            r"Â·\s*([ê°€-í£]{2,4})\s*ê¸°ì",
            r"ê¸°ì\s*:\s*([ê°€-í£]{2,4})",
            r"\[([ê°€-í£]{2,4})\s*ê¸°ì\]",
            r"ì·¨ì¬\s*:\s*([ê°€-í£]{2,4})",  # ì·¨ì¬: ê¸°ìëª…
            r"ì˜ìƒí¸ì§‘\s*:\s*([ê°€-í£]{2,4})",  # ì˜ìƒí¸ì§‘: ê¸°ìëª…
            r"\(ì·¨ì¬:\s*([ê°€-í£]{2,4})",  # (ì·¨ì¬: ê¸°ìëª…
        ]

        # BeautifulSoup ê°ì²´ì—ì„œ ê¸°ìëª… ì°¾ê¸°
        if soup:
            # ê¸°ìëª…ì´ í¬í•¨ë  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ìš”ì†Œë“¤ ì°¾ê¸°
            reporter_elements = soup.find_all(["span", "div", "p"], string=re.compile(r"ê¸°ì|íŠ¹íŒŒì›|ì·¨ì¬"))
            for element in reporter_elements:
                text = element.get_text(strip=True)
                if "ê¸°ì" in text or "íŠ¹íŒŒì›" in text or "ì·¨ì¬" in text:
                    match = re.search(r"([ê°€-í£]{2,4})", text)
                    if match:
                        name = match.group(1)
                        if "ê¸°ì" in text:
                            return name + " ê¸°ì"
                        elif "íŠ¹íŒŒì›" in text:
                            return name + " íŠ¹íŒŒì›"
                        elif "ì·¨ì¬" in text:
                            return name + " ê¸°ì"

        # ê¸°ì‚¬ í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìëª… ì°¾ê¸°
        full_text = str(soup) + "\n" + article_text if soup else article_text

        for pattern in reporter_patterns:
            matches = re.findall(pattern, full_text)
            if matches:
                if isinstance(matches[0], tuple):
                    reporter = matches[0][0].strip()
                    role = matches[0][1] if len(matches[0]) > 1 else "ê¸°ì"
                else:
                    reporter = matches[0].strip()
                    role = "ê¸°ì"

                if reporter and len(reporter) >= 2:
                    return reporter + f" {role}" if role not in reporter else reporter

        return "ê¸°ìëª… ì—†ìŒ"

    except Exception as e:
        print(f"ê¸°ìëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return "ê¸°ìëª… ì—†ìŒ"


def parse_sbs_rss_full_content(category="all"):
    """SBS RSSë¥¼ íŒŒì‹±í•˜ì—¬ ì „ì²´ ë³¸ë¬¸ê³¼ í•¨ê»˜ CSVë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""

    # SBS RSS URL ëª©ë¡
    category_urls = {
        "all": "https://news.sbs.co.kr/news/newsflashRssFeed.do?plink=RSSREADER",  # ìµœì‹  ë‰´ìŠ¤
        "politics": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01&plink=RSSREADER",  # ì •ì¹˜
        "economy": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=02&plink=RSSREADER",  # ê²½ì œ
        "society": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=03&plink=RSSREADER",  # ì‚¬íšŒ
        "culture": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=04&plink=RSSREADER",  # ë¬¸í™”
        "international": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=05&plink=RSSREADER",  # êµ­ì œ
    }

    if category not in category_urls:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤.")
        print(f"âœ… ì§€ì› ì¹´í…Œê³ ë¦¬: {', '.join(category_urls.keys())}")
        return None

    rss_url = category_urls[category]

    try:
        print(f"ğŸ“¡ SBS {category} RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        response = requests.get(rss_url, headers=headers, timeout=30)
        response.raise_for_status()
        response.encoding = "utf-8"

        # XML íŒŒì‹±
        root = ET.fromstring(response.content)
        items = root.findall(".//item")
        news_data = []

        print(f"ì´ {len(items)}ê°œì˜ ë‰´ìŠ¤ í•­ëª©ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        print("ê° ê¸°ì‚¬ì˜ ì „ì²´ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” ì¤‘... (ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

        for i, item in enumerate(items):
            try:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = item.find("title").text if item.find("title") is not None else "ì œëª© ì—†ìŒ"
                title = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", title)
                title = re.sub(r"<[^>]+>", "", title).strip()

                # ë§í¬ ì¶”ì¶œ
                link = item.find("link").text if item.find("link") is not None else ""

                # ë‚ ì§œ ì¶”ì¶œ
                pub_date = item.find("pubDate").text if item.find("pubDate") is not None else ""
                formatted_date = ""
                if pub_date:
                    try:
                        # SBS RSS ë‚ ì§œ í˜•ì‹: Sat, 28 Jun 2025 16:41:00 +0900
                        date_obj = datetime.strptime(pub_date.split(" +")[0], "%a, %d %b %Y %H:%M:%S")
                        formatted_date = date_obj.strftime("%Y-%m-%d %H:%M:%S")
                    except:
                        formatted_date = pub_date
                else:
                    formatted_date = datetime.now().strftime("%Y-%m-%d %H:%M")

                # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                category_elem = item.find("category")
                category_text = category_elem.text if category_elem is not None else ""

                print(f"[{i+1}/{len(items)}] ì²˜ë¦¬ ì¤‘: {title[:60]}...")

                if link:
                    # ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ
                    try:
                        article_response = requests.get(link, headers=headers, timeout=20)
                        article_response.encoding = "utf-8"
                        soup = BeautifulSoup(article_response.text, "html.parser")

                        # ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ
                        full_content = extract_sbs_article_content(link)

                        # ê¸°ìëª… ì¶”ì¶œ
                        reporter_name = extract_sbs_reporter_name(soup, full_content)

                        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš° RSS descriptionë„ í¬í•¨
                        if len(full_content) < 200:
                            rss_description = (
                                item.find("description").text if item.find("description") is not None else ""
                            )
                            rss_description = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", rss_description)
                            rss_description = re.sub(r"<[^>]+>", "", rss_description).strip()

                            if rss_description:
                                full_content = (
                                    rss_description + "\n\n" + full_content if full_content else rss_description
                                )

                        # ë°ì´í„° ì €ì¥
                        if full_content.strip():  # ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš°ë§Œ ì €ì¥
                            news_data.append(
                                {"ì œëª©": title, "ë‚ ì§œ": formatted_date, "ê¸°ìëª…": reporter_name, "ë³¸ë¬¸": full_content}
                            )
                        else:
                            print(f"  â¤ ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")

                        # ì„œë²„ ë¶€í•˜ ë°©ì§€
                        time.sleep(1)

                    except Exception as e:
                        print(f"  â¤ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ RSS ê¸°ë³¸ ì •ë³´ëŠ” ì €ì¥
                        description = item.find("description").text if item.find("description") is not None else ""
                        description = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", description)
                        description = re.sub(r"<[^>]+>", "", description).strip()

                        news_data.append(
                            {"ì œëª©": title, "ë‚ ì§œ": formatted_date, "ê¸°ìëª…": "ê¸°ìëª… ì—†ìŒ", "ë³¸ë¬¸": description}
                        )
                        continue

            except Exception as e:
                print(f"RSS í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        # CSV íŒŒì¼ë¡œ ì €ì¥
        if news_data:
            filename = f"results/SBSë‰´ìŠ¤_{category}_ì „ì²´ë³¸ë¬¸_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

            with open(filename, "w", newline="", encoding="utf-8-sig") as csvfile:
                fieldnames = ["ì œëª©", "ë‚ ì§œ", "ê¸°ìëª…", "ë³¸ë¬¸"]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

                writer.writeheader()
                writer.writerows(news_data)

            print(f"\nâœ… ì„±ê³µì ìœ¼ë¡œ {len(news_data)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“ íŒŒì¼ëª…: {filename}")

            # í†µê³„ ì •ë³´ ì¶œë ¥
            total_chars = sum(len(item["ë³¸ë¬¸"]) for item in news_data)
            avg_chars = total_chars // len(news_data) if news_data else 0
            print(f"ğŸ“Š í‰ê·  ë³¸ë¬¸ ê¸¸ì´: {avg_chars:,}ì")
            print(f"ğŸ“Š ì´ ë³¸ë¬¸ ê¸¸ì´: {total_chars:,}ì")

            return filename
        else:
            print("âŒ ì¶”ì¶œëœ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

    except Exception as e:
        print(f"âŒ RSS íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def scrape_sbs_multiple_categories(categories=["all"], max_articles_per_category=20):
    """ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ì˜ SBS ë‰´ìŠ¤ë¥¼ ë™ì‹œì— ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜"""

    print("ğŸ—ï¸  SBS ë‰´ìŠ¤ ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘")
    print("=" * 60)

    total_collected = 0

    for category in categories:
        print(f"\nğŸ“° {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¤‘...")

        try:
            result = parse_sbs_rss_full_content(category)

            if result:
                print(f"âœ… {category} ì¹´í…Œê³ ë¦¬ ì™„ë£Œ")
                total_collected += 1
            else:
                print(f"âŒ {category} ì¹´í…Œê³ ë¦¬ ì‹¤íŒ¨")

        except Exception as e:
            print(f"âŒ {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue

    print(f"\nğŸ‰ ì´ {total_collected}ê°œ ì¹´í…Œê³ ë¦¬ì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
    return total_collected


if __name__ == "__main__":
    # ìë™ìœ¼ë¡œ ê° ì¹´í…Œê³ ë¦¬ë³„ 20ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ í›„ í•˜ë‚˜ì˜ CSVë¡œ ì €ì¥
    categories = ["all", "politics", "economy", "society", "culture", "international"]
    # ì¹´í…Œê³ ë¦¬ë³„ RSS URL ì¬ì‚¬ìš©
    category_urls = {
        "all": "https://news.sbs.co.kr/news/newsflashRssFeed.do?plink=RSSREADER",
        "politics": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01&plink=RSSREADER",
        "economy": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=02&plink=RSSREADER",
        "society": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=03&plink=RSSREADER",
        "culture": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=04&plink=RSSREADER",
        "international": "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=05&plink=RSSREADER",
    }
    all_articles = []
    for category in categories:
        rss_url = category_urls.get(category)
        # XML íŒŒì‹±
        response = requests.get(rss_url, headers={"User-Agent": "Mozilla/5.0"}, timeout=20)
        root = ET.fromstring(response.content)
        items = root.findall(".//item")[:20]
        for item in items:
            title = item.find("title").text or ""
            title = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", title)
            title = re.sub(r"<[^>]+>", "", title).strip()
            pub_date = item.find("pubDate").text or ""
            try:
                date_obj = datetime.strptime(pub_date.split(" +")[0], "%a, %d %b %Y %H:%M:%S")
                formatted_date = date_obj.strftime("%Y-%m-%d %H:%M:%S")
            except:
                formatted_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            link = item.find("link").text or ""
            # ë³¸ë¬¸ ì¶”ì¶œ
            full_content = extract_sbs_article_content(link) if link else ""
            # ê¸°ìëª… ì¶”ì¶œ
            soup = BeautifulSoup(requests.get(link).text, "html.parser") if link else None
            reporter_name = extract_sbs_reporter_name(soup, full_content)
            all_articles.append(
                {
                    "ì œëª©": title,
                    "ë‚ ì§œ": formatted_date,
                    "ì¹´í…Œê³ ë¦¬": category,
                    "ê¸°ìëª…": reporter_name,
                    "ë³¸ë¬¸": full_content,
                }
            )
            time.sleep(1)
    # CSV ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"results/SBS_ì „ì²´_{timestamp}.csv"
    with open(filename, "w", newline="", encoding="utf-8-sig") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"])
        for art in all_articles:
            writer.writerow(["SBS", art["ì œëª©"], art["ë‚ ì§œ"], art["ì¹´í…Œê³ ë¦¬"], art["ê¸°ìëª…"], art["ë³¸ë¬¸"]])
    print(f"\nâœ… SBS ë‰´ìŠ¤ {len(all_articles)}ê°œ ìë™ ì €ì¥ ì™„ë£Œ: {filename}")
