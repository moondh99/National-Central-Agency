import feedparser
import csv
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import time
import random
import os


def get_random_user_agent():
    """ëœë¤ User-Agent ë°˜í™˜"""
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    ]
    return random.choice(user_agents)


def extract_kado_article_content(url, rss_summary="", rss_author=""):
    """ê°•ì›ë„ë¯¼ì¼ë³´ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ (ê¸°ìëª…ì€ RSSì—ì„œ ê°€ì ¸ì˜´)"""
    try:
        session = requests.Session()

        headers = {
            "User-Agent": get_random_user_agent(),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Referer": "https://www.kado.net/",
            "Cache-Control": "no-cache",
        }

        print(f"    ì ‘ì† ì‹œë„: {url[:80]}...")

        # ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ í›„ ì‹¤ì œ ê¸°ì‚¬ ì ‘ê·¼
        try:
            session.get("https://www.kado.net/", headers=headers, timeout=5)
            time.sleep(0.5)

            response = session.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            if len(response.content) < 3000:  # 3KB ë¯¸ë§Œì´ë©´ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ
                print(f"    âš  ì‘ë‹µ í¬ê¸°ê°€ ì‘ìŒ (í¬ê¸°: {len(response.content)} bytes)")

        except Exception as e:
            print(f"    âš  ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return rss_author, rss_summary if rss_summary else "ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨"

        soup = BeautifulSoup(response.content, "html.parser")

        # ë³¸ë¬¸ ì¶”ì¶œ - ì œê³µëœ XPath ê²½ë¡œë¥¼ CSS ì„ íƒìë¡œ ë³€í™˜
        # XPath: /html/body/div[1]/div/section/div[4]/div/section/article/div[2]/div/article[1]/p
        content = ""

        try:
            # ì •í™•í•œ CSS ì„ íƒìë¡œ ë³¸ë¬¸ ì¶”ì¶œ
            content_selectors = [
                "body > div:nth-child(1) > div > section > div:nth-child(4) > div > section > article > div:nth-child(2) > div > article:nth-child(1) p",
                "section article div:nth-child(2) div article:first-child p",  # ì¡°ê¸ˆ ë” ìœ ì—°í•œ ì„ íƒì
                "article div:nth-child(2) div article p",  # ë” ê°„ë‹¨í•œ ì„ íƒì
                "div[class*='article'] p",  # article í´ë˜ìŠ¤ê°€ í¬í•¨ëœ div ë‚´ì˜ p íƒœê·¸
                "article p",  # article íƒœê·¸ ë‚´ì˜ ëª¨ë“  p íƒœê·¸
            ]

            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content_parts = []
                    for p in elements:
                        text = p.get_text().strip()
                        if len(text) > 10:  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ë§Œ
                            content_parts.append(text)

                    if content_parts:
                        content = " ".join(content_parts)
                        print(f"    ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ (ì„ íƒì: {selector[:50]}...)")
                        break

            # ìœ„ ë°©ë²•ì´ ì‹¤íŒ¨í•˜ë©´ ëª¨ë“  p íƒœê·¸ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ
            if len(content) < 100:
                print("    ê¸°ë³¸ ì„ íƒìë¡œ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„...")
                paragraphs = soup.find_all("p")
                content_parts = []

                for p in paragraphs:
                    text = p.get_text().strip()
                    if (
                        len(text) > 20
                        and not re.search(r"ì…ë ¥\s*\d{4}|ìˆ˜ì •\s*\d{4}|Copyright|ì €ì‘ê¶Œ|ê°•ì›ë„ë¯¼ì¼ë³´|kado", text)
                        and not text.startswith(("â–¶", "â˜", "â€»", "â– ", "â–²", "[", "â€»", "â—†", "â—‹", "â–³"))
                        and "@kado.net" not in text
                        and "ë¬´ë‹¨ ì „ì¬" not in text
                        and "ì¬ë°°í¬ ê¸ˆì§€" not in text
                        and "ê¸°ì‚¬ì œë³´" not in text
                    ):
                        content_parts.append(text)

                if content_parts:
                    content = " ".join(content_parts)

        except Exception as e:
            print(f"    ë³¸ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")

        # ë³¸ë¬¸ ì •ì œ
        content = clean_kado_content(content)

        # RSS ìš”ì•½ì´ ë” ì¢‹ìœ¼ë©´ RSS ìš”ì•½ ì‚¬ìš©
        if rss_summary and (len(content) < 100 or len(rss_summary) > len(content)):
            content = rss_summary
            print(f"    RSS ìš”ì•½ ì±„íƒ (ê¸¸ì´: {len(rss_summary)})")

        print(f"    ìµœì¢… ë³¸ë¬¸ ê¸¸ì´: {len(content)}")
        return rss_author, content

    except Exception as e:
        print(f"    âŒ ì—ëŸ¬: {e}")
        return rss_author, rss_summary if rss_summary else f"ì˜¤ë¥˜: {str(e)}"


def clean_kado_content(content):
    """ê°•ì›ë„ë¯¼ì¼ë³´ ê¸°ì‚¬ ë³¸ë¬¸ ì •ì œ"""
    if not content:
        return ""

    # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ë“¤ ì œê±° - ê°•ì›ë„ë¯¼ì¼ë³´ íŠ¹ì„±ì— ë§ê²Œ ìˆ˜ì •
    remove_patterns = [
        r"ì…ë ¥\s*\d{4}[-./]\d{2}[-./]\d{2}.*?\d{2}:\d{2}",
        r"ìˆ˜ì •\s*\d{4}[-./]\d{2}[-./]\d{2}.*?\d{2}:\d{2}",
        r"ì—…ë°ì´íŠ¸\s*\d{4}[-./]\d{2}[-./]\d{2}.*?\d{2}:\d{2}",
        r"ê°•ì›ë„ë¯¼ì¼ë³´.*ë¬´ë‹¨.*ì „ì¬.*ê¸ˆì§€",
        r"ë¬´ë‹¨.*ì „ì¬.*ì¬ë°°í¬.*ê¸ˆì§€",
        r"ì €ì‘ê¶Œ.*ê°•ì›ë„ë¯¼ì¼ë³´",
        r"ê´€ë ¨ê¸°ì‚¬.*ë”ë³´ê¸°",
        r"í˜ì´ìŠ¤ë¶.*íŠ¸ìœ„í„°.*ì¹´ì¹´ì˜¤",
        r"êµ¬ë….*ì‹ ì²­",
        r"ê´‘ê³ ",
        r"[ê°€-í£]{2,4}\s*ê¸°ì\s*[a-zA-Z0-9_.+-]+@kado\.net",  # ê¸°ì ì´ë©”ì¼ ì œê±°
        r"ì—°í•©ë‰´ìŠ¤.*ì œê³µ",  # ë‰´ìŠ¤ ì¶œì²˜ ì œê±°
        r"ë‰´ì‹œìŠ¤.*ì œê³µ",  # ë‰´ìŠ¤ ì¶œì²˜ ì œê±°
        r"ê°•ì›ë„ë¯¼ì¼ë³´.*ì œê³µ",  # ì‚¬ì§„ ì¶œì²˜ ì œê±°
        r"â“’.*ê°•ì›ë„ë¯¼ì¼ë³´",
        r"kado\.net",
        r"ê¸°ì‚¬ì œë³´.*ë¬¸ì˜",
        r"ë…ìíˆ¬ê³ .*ë¬¸ì˜",
        r"ì²­ì†Œë…„.*ë³´í˜¸.*ì±…ì„ì",
        r"ê°œì¸ì •ë³´.*ì²˜ë¦¬.*ë°©ì¹¨",
        r"ì´ë©”ì¼.*ë¬´ë‹¨.*ìˆ˜ì§‘.*ê±°ë¶€",
        r"Copyright.*\d{4}.*ê°•ì›ë„ë¯¼ì¼ë³´",
        r"ê°•ì›.*ì¶˜ì²œ.*ì›ì£¼.*ì†ì´ˆ",  # ì§€ì—­ ê´€ë ¨ ë°˜ë³µ ë¬¸êµ¬
        r"ë„ë¯¼ì¼ë³´.*NEWS",
    ]

    for pattern in remove_patterns:
        content = re.sub(pattern, "", content, flags=re.IGNORECASE)

    # ê³µë°± ì •ë¦¬
    content = re.sub(r"\s+", " ", content).strip()

    # ê¸¸ì´ ì œí•œ
    if len(content) > 1800:
        content = content[:1800] + "..."

    return content


def fetch_kado_rss_to_csv(rss_url, category_name, writer, max_articles=30):
    """ê°•ì›ë„ë¯¼ì¼ë³´ RSSë¥¼ íŒŒì‹±í•˜ì—¬ CSVì— ì €ì¥ (ë‹¨ì¼ íŒŒì¼ì— ì¶”ê°€)"""

    print(f"ê°•ì›ë„ë¯¼ì¼ë³´ RSS í”¼ë“œ íŒŒì‹± ì¤‘: {rss_url}")

    # RSS íŒŒì‹±
    try:
        headers = {"User-Agent": get_random_user_agent()}
        response = requests.get(rss_url, headers=headers, timeout=10)
        # ê°•ì›ë„ë¯¼ì¼ë³´ RSSëŠ” UTF-8 ì¸ì½”ë”© ì‚¬ìš©
        response.encoding = "utf-8"
        feed = feedparser.parse(response.content)
    except:
        feed = feedparser.parse(rss_url)

    if not feed.entries:
        print("âŒ RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    print(f"âœ… RSSì—ì„œ {len(feed.entries)}ê°œ ê¸°ì‚¬ ë°œê²¬")

    success_count = 0
    total_count = min(len(feed.entries), max_articles)

    print(f"ì´ {total_count}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘...\n")

    for i, entry in enumerate(feed.entries[:max_articles]):
        try:
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            title = entry.title.strip()
            title = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", title)

            link = entry.link

            # RSS ìš”ì•½ ì •ë³´ ì¶”ì¶œ
            summary = ""
            if hasattr(entry, "description"):
                summary = entry.description.strip()
                # HTML íƒœê·¸ì™€ CDATA ì œê±°
                summary = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", summary, flags=re.DOTALL)
                summary = re.sub(r"<[^>]+>", "", summary)  # HTML íƒœê·¸ ì œê±°
                summary = clean_kado_content(summary)

            # RSSì—ì„œ ê¸°ìëª… ì¶”ì¶œ
            author = ""
            if hasattr(entry, "author"):
                author = entry.author.strip()
                # ê¸°ìëª… ì •ì œ (ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°)
                author = re.sub(r"ê¸°ì|íŠ¹íŒŒì›|í¸ì§‘ìœ„ì›|íŒ€ì¥|ì„ ì„ê¸°ì|ìˆ˜ì„ê¸°ì", "", author).strip()

            # ë‚ ì§œ í˜•ì‹ ë³€í™˜
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S")
            else:
                date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            print(f"[{i+1}/{total_count}] {title[:50]}...")

            # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ê¸°ìëª…ì€ RSSì—ì„œ ê°€ì ¸ì˜´)
            reporter, content = extract_kado_article_content(link, summary, author)

            # ìµœì†Œ ì¡°ê±´ í™•ì¸
            if len(content.strip()) < 20:
                print(f"    âš  ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ ê±´ë„ˆëœ€ (ê¸¸ì´: {len(content)})\n")
                continue

            # CSVì— ì“°ê¸° (ì–¸ë¡ ì‚¬ëª… ì¶”ê°€)
            writer.writerow(
                {
                    "ì–¸ë¡ ì‚¬": "ê°•ì›ë„ë¯¼ì¼ë³´",
                    "ì œëª©": title,
                    "ë‚ ì§œ": date,
                    "ì¹´í…Œê³ ë¦¬": category_name,
                    "ê¸°ìëª…": reporter if reporter else "ë¯¸ìƒ",
                    "ë³¸ë¬¸": content,
                }
            )

            success_count += 1
            print(
                f"    âœ… ì„±ê³µ! (ì¹´í…Œê³ ë¦¬: {category_name}, ê¸°ì: {reporter if reporter else 'ë¯¸ìƒ'}, ë³¸ë¬¸: {len(content)}ì)"
            )

            # ì§„í–‰ë¥  í‘œì‹œ
            if (i + 1) % 5 == 0:
                print(f"\nğŸ“Š ì§„í–‰ë¥ : {i+1}/{total_count} ({(i+1)/total_count*100:.1f}%)")
                print(f"ğŸ“ˆ ì„±ê³µë¥ : {success_count}/{i+1} ({success_count/(i+1)*100:.1f}%)\n")

            # ëœë¤ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            delay = random.uniform(1.5, 3.0)
            time.sleep(delay)

        except KeyboardInterrupt:
            print("\nâš  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"    âŒ ì˜¤ë¥˜: {e}")
            continue

    print(f"\n{'='*50}")
    print(f"ğŸ‰ {category_name} ì¹´í…Œê³ ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“Š ê²°ê³¼: {success_count}/{total_count}ê°œ ì„±ê³µ ({success_count/total_count*100:.1f}%)")
    print(f"{'='*50}")

    return success_count


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ê°•ì›ë„ë¯¼ì¼ë³´ RSS URL ì˜µì…˜ë“¤ (ì§€ì •ëœ ì¹´í…Œê³ ë¦¬ë§Œ)
    kado_rss_options = {
        "ì „ì²´ê¸°ì‚¬": "https://www.kado.net/rss/allArticle.xml",
        "ì •ì¹˜": "https://www.kado.net/rss/S1N1.xml",
        "ê²½ì œ": "https://www.kado.net/rss/S1N2.xml",
        "ì‚¬íšŒ": "https://www.kado.net/rss/S1N3.xml",
        "ë¬¸í™”": "https://www.kado.net/rss/S1N4.xml",
        "ì§€ì—­": "https://www.kado.net/rss/S1N6.xml",
        "ì˜¤í”¼ë‹ˆì–¸": "https://www.kado.net/rss/S1N8.xml",
    }

    print("ê°•ì›ë„ë¯¼ì¼ë³´ RSS ìˆ˜ì§‘ê¸° (ê°•ì›ë„ ì§€ì—­ì–¸ë¡ )")
    print("=" * 60)

    # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ 20ê°œì”© ìë™ ìˆ˜ì§‘
    max_articles = 20
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")

    total_categories = len(kado_rss_options)
    current_category = 0
    total_success = 0

    print(f"ì´ {total_categories}ê°œ ì¹´í…Œê³ ë¦¬ì—ì„œ ê°ê° {max_articles}ê°œì”© ê¸°ì‚¬ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("ëª¨ë“  ê¸°ì‚¬ëŠ” í•˜ë‚˜ì˜ CSV íŒŒì¼ì— ì €ì¥ë©ë‹ˆë‹¤.\n")

    # results ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("results", exist_ok=True)

    # ë‹¨ì¼ CSV íŒŒì¼ ìƒì„±
    output_file = f"results/ê°•ì›ë„ë¯¼ì¼ë³´_ì „ì²´_{timestamp}.csv"

    with open(output_file, "w", newline="", encoding="utf-8-sig") as csvfile:
        fieldnames = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        for category, rss_url in kado_rss_options.items():
            current_category += 1

            print(f"ğŸš€ [{current_category}/{total_categories}] {category} ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì‹œì‘!")
            print(f"ğŸ”— RSS URL: {rss_url}\n")

            # ì‹¤í–‰
            success_count = fetch_kado_rss_to_csv(rss_url, category, writer, max_articles)
            total_success += success_count

            # ì¹´í…Œê³ ë¦¬ ê°„ íœ´ì‹ ì‹œê°„
            if current_category < total_categories:
                print(f"\nâ° ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ê¹Œì§€ 5ì´ˆ ëŒ€ê¸°...\n")
                time.sleep(5)

    print(f"\nğŸ‰ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {total_categories}ê°œ ì¹´í…Œê³ ë¦¬ì—ì„œ {total_success}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì„±ê³µ")
    print(f"ğŸ“ ì €ì¥ íŒŒì¼: {output_file}")
    print(
        f"ğŸ“ˆ ì „ì²´ ì„±ê³µë¥ : {total_success}/{total_categories * max_articles}ê°œ ({total_success/(total_categories * max_articles)*100:.1f}%)"
    )
