#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì—ì´ë¸”ë‰´ìŠ¤(ablenews.co.kr) RSS í¬ë¡¤ë§ ì½”ë“œ
13ê°œ ì¹´í…Œê³ ë¦¬ RSS í”¼ë“œ ì§€ì›í•˜ëŠ” ì¥ì• ì¸ ì „ë¬¸ ì–¸ë¡ ì‚¬ í¬ë¡¤ëŸ¬

ì‘ì„±ì¼: 2025-08-02
"""

import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import pandas as pd
import time
import random
import re
from datetime import datetime
import csv
from urllib.parse import urljoin, urlparse
import logging

class AbleNewsRSSCrawler:
    def __init__(self):
        """ì—ì´ë¸”ë‰´ìŠ¤ RSS í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.base_url = "https://www.ablenews.co.kr"
        
        # 13ê°œ ì—ì´ë¸”ë‰´ìŠ¤ RSS í”¼ë“œ
        self.rss_feeds = {
            'ì „ì²´ê¸°ì‚¬': 'https://www.ablenews.co.kr/rss/allArticle.xml',
            'ì¸ê¸°ê¸°ì‚¬': 'https://www.ablenews.co.kr/rss/clickTop.xml',
            'ì •ë³´ì„¸ìƒ': 'https://www.ablenews.co.kr/rss/S1N1.xml',
            'ì˜¤í”¼ë‹ˆì–¸': 'https://www.ablenews.co.kr/rss/S1N2.xml',
            'ì¸ê¶Œ': 'https://www.ablenews.co.kr/rss/S1N3.xml',
            'ë…¸ë™': 'https://www.ablenews.co.kr/rss/S1N4.xml',
            'êµìœ¡': 'https://www.ablenews.co.kr/rss/S1N5.xml',
            'ë³µì§€': 'https://www.ablenews.co.kr/rss/S1N6.xml',
            'ìë¦½ìƒí™œ': 'https://www.ablenews.co.kr/rss/S1N7.xml',
            'ë¬¸í™”/ì²´ìœ¡': 'https://www.ablenews.co.kr/rss/S1N8.xml',
            'ë‹¨ì²´': 'https://www.ablenews.co.kr/rss/S1N9.xml',
            'ì „êµ­ë„·': 'https://www.ablenews.co.kr/rss/S1N10.xml',
            'ì •ì±…': 'https://www.ablenews.co.kr/rss/S1N11.xml',
            'ë™ì˜ìƒ': 'https://www.ablenews.co.kr/rss/S1N12.xml',
            'ê¸°íšíŠ¹ì§‘': 'https://www.ablenews.co.kr/rss/S1N14.xml'
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì„¤ëª…
        self.category_descriptions = {
            'ì „ì²´ê¸°ì‚¬': 'ì—ì´ë¸”ë‰´ìŠ¤ ì „ì²´ ê¸°ì‚¬',
            'ì¸ê¸°ê¸°ì‚¬': 'ì¡°íšŒìˆ˜ê°€ ë†’ì€ ì¸ê¸° ê¸°ì‚¬',
            'ì •ë³´ì„¸ìƒ': 'ì¥ì• ì¸ ìƒí™œ ì •ë³´ ë° ìœ ìš©í•œ ì •ë³´',
            'ì˜¤í”¼ë‹ˆì–¸': 'ì¹¼ëŸ¼, ì‚¬ì„¤, ë…¼í‰ ë“± ì˜ê²¬ê¸€',
            'ì¸ê¶Œ': 'ì¥ì• ì¸ ì¸ê¶Œ ê´€ë ¨ ì´ìŠˆ',
            'ë…¸ë™': 'ì¥ì• ì¸ ê³ ìš© ë° ë…¸ë™ ê´€ë ¨ ì†Œì‹',
            'êµìœ¡': 'ì¥ì• ì¸ êµìœ¡ ì •ì±… ë° í˜„í™©',
            'ë³µì§€': 'ì¥ì• ì¸ ë³µì§€ ì œë„ ë° ì„œë¹„ìŠ¤',
            'ìë¦½ìƒí™œ': 'ì¥ì• ì¸ ìë¦½ìƒí™œ ì§€ì› ë° ì‚¬ë¡€',
            'ë¬¸í™”/ì²´ìœ¡': 'ì¥ì• ì¸ ë¬¸í™”í™œë™ ë° ì²´ìœ¡ ì†Œì‹',
            'ë‹¨ì²´': 'ì¥ì• ì¸ ë‹¨ì²´ ë° ê¸°ê´€ í™œë™',
            'ì „êµ­ë„·': 'ì „êµ­ ê° ì§€ì—­ì˜ ì¥ì• ì¸ ê´€ë ¨ ì†Œì‹',
            'ì •ì±…': 'ì¥ì• ì¸ ê´€ë ¨ ì •ë¶€ ì •ì±…',
            'ë™ì˜ìƒ': 'ë™ì˜ìƒ ë‰´ìŠ¤ ë° ì½˜í…ì¸ ',
            'ê¸°íšíŠ¹ì§‘': 'ì‹¬ì¸µ ë³´ë„ ë° íŠ¹ì§‘ ê¸°ì‚¬'
        }
        
        # ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ ë¶„ë¥˜
        self.category_groups = {
            'ê¶ŒìµÂ·ì¸ê¶Œ': ['ì¸ê¶Œ', 'ì •ì±…'],
            'ìƒí™œÂ·ë³µì§€': ['ì •ë³´ì„¸ìƒ', 'ë³µì§€', 'ìë¦½ìƒí™œ'],
            'ì‚¬íšŒÂ·ë…¸ë™': ['ë…¸ë™', 'êµìœ¡'],
            'ë¬¸í™”Â·í™œë™': ['ë¬¸í™”/ì²´ìœ¡', 'ë‹¨ì²´', 'ì „êµ­ë„·'],
            'íŠ¹ë³„Â·ê¸°íš': ['ì˜¤í”¼ë‹ˆì–¸', 'ë™ì˜ìƒ', 'ê¸°íšíŠ¹ì§‘'],
            'ì¢…í•©': ['ì „ì²´ê¸°ì‚¬', 'ì¸ê¸°ê¸°ì‚¬']
        }
        
        # ì¥ì•  ê´€ë ¨ í‚¤ì›Œë“œ
        self.disability_keywords = [
            'ì‹œê°ì¥ì• ', 'ì²­ê°ì¥ì• ', 'ì§€ì²´ì¥ì• ', 'ë‡Œë³‘ë³€ì¥ì• ', 'ë°œë‹¬ì¥ì• ', 'ì§€ì ì¥ì• ',
            'ì •ì‹ ì¥ì• ', 'ì‹ ì¥ì¥ì• ', 'ì‹¬ì¥ì¥ì• ', 'í˜¸í¡ê¸°ì¥ì• ', 'ê°„ì¥ì• ', 'ì•ˆë©´ì¥ì• ',
            'ì¥ë£¨Â·ìš”ë£¨ì¥ì• ', 'ë‡Œì „ì¦ì¥ì• ', 'ìíì„±ì¥ì• ', 'ì¤‘ì¦ì¥ì• ', 'ê²½ì¦ì¥ì• '
        ]
        
        # User-Agent ë¦¬ìŠ¤íŠ¸
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15'
        ]
        
        self.articles = []
        self.session = requests.Session()
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)

    def get_random_headers(self):
        """ëœë¤ í—¤ë” ìƒì„±"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Referer': 'https://www.ablenews.co.kr/'
        }

    def random_delay(self, min_delay=1, max_delay=3):
        """ëœë¤ ë”œë ˆì´"""
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)

    def fetch_rss_feed(self, rss_url, max_retries=3):
        """RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°"""
        for attempt in range(max_retries):
            try:
                headers = self.get_random_headers()
                response = self.session.get(rss_url, headers=headers, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except Exception as e:
                self.logger.warning(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    self.random_delay(2, 5)
                else:
                    self.logger.error(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ìµœì¢… ì‹¤íŒ¨: {rss_url}")
                    return None

    def parse_rss_feed(self, rss_content):
        """RSS í”¼ë“œ íŒŒì‹±"""
        try:
            root = ET.fromstring(rss_content)
            items = []
            
            for item in root.findall('.//item'):
                article_info = {}
                
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title_elem = item.find('title')
                article_info['title'] = title_elem.text.strip() if title_elem is not None else ''
                
                link_elem = item.find('link')
                article_info['link'] = link_elem.text.strip() if link_elem is not None else ''
                
                pubdate_elem = item.find('pubDate')
                article_info['pub_date'] = pubdate_elem.text.strip() if pubdate_elem is not None else ''
                
                # descriptionì—ì„œ ê°„ë‹¨í•œ ë‚´ìš© ì¶”ì¶œ
                desc_elem = item.find('description')
                if desc_elem is not None:
                    desc_text = desc_elem.text or ''
                    # CDATA ì²˜ë¦¬
                    if desc_text.startswith('<![CDATA[') and desc_text.endswith(']]>'):
                        desc_text = desc_text[9:-3]
                    
                    # HTML íƒœê·¸ ì œê±°í•˜ì—¬ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                    soup = BeautifulSoup(desc_text, 'html.parser')
                    article_info['description'] = soup.get_text().strip()[:300] + '...' if len(soup.get_text().strip()) > 300 else soup.get_text().strip()
                else:
                    article_info['description'] = ''
                
                items.append(article_info)
            
            return items
        except Exception as e:
            self.logger.error(f"RSS íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []

    def extract_article_content(self, article_url, max_retries=3):
        """ê°œë³„ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ - ì—ì´ë¸”ë‰´ìŠ¤ í˜ì´ì§€ ìµœì í™”"""
        for attempt in range(max_retries):
            try:
                headers = self.get_random_headers()
                response = self.session.get(article_url, headers=headers, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ì—ì´ë¸”ë‰´ìŠ¤ í˜ì´ì§€ êµ¬ì¡°ì— ìµœì í™”ëœ ë³¸ë¬¸ ì¶”ì¶œ ì…€ë ‰í„°
                content_selectors = [
                    '.news_content',        # ë‰´ìŠ¤ ë³¸ë¬¸
                    '.article_content',     # ê¸°ì‚¬ ë‚´ìš©
                    '.view_content',        # ë·° ë‚´ìš©
                    '.detail_content',      # ìƒì„¸ ë‚´ìš©
                    '.news_text',          # ë‰´ìŠ¤ í…ìŠ¤íŠ¸
                    '.content_area',       # ì½˜í…ì¸  ì˜ì—­
                    '.article_body',       # ê¸°ì‚¬ ë³¸ë¬¸
                    '.news_body',          # ë‰´ìŠ¤ ë³¸ë¬¸
                    '#article-view-content-div'  # íŠ¹ì • ID
                ]
                
                content = ""
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text().strip()
                        break
                
                # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„
                if not content:
                    # í—¤ë”, í‘¸í„°, ì‚¬ì´ë“œë°” ë“± ì œê±°
                    for elem in soup.find_all(['header', 'footer', 'nav', 'aside', 'script', 'style']):
                        elem.decompose()
                    
                    main_content = soup.find('main') or soup.find('div', class_='content') or soup.find('body')
                    if main_content:
                        content = main_content.get_text().strip()
                
                # ê¸°ìëª… ì¶”ì¶œ
                reporter = self.extract_reporter_name(soup, content)
                
                # ì¥ì•  ìœ í˜• ì¶”ì¶œ
                disability_types = self.extract_disability_types(content)
                
                # ê´€ë ¨ ê¸°ê´€/ë‹¨ì²´ ì¶”ì¶œ
                organizations = self.extract_organizations(content)
                
                # í…ìŠ¤íŠ¸ ì •ë¦¬
                content = re.sub(r'\s+', ' ', content).strip()
                
                return {
                    'content': content[:3000] + '...' if len(content) > 3000 else content,
                    'reporter': reporter,
                    'disability_types': disability_types,
                    'organizations': organizations
                }
                
            except Exception as e:
                self.logger.warning(f"ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    self.random_delay(1, 3)
                else:
                    return {'content': 'ì¶”ì¶œ ì‹¤íŒ¨', 'reporter': '', 'disability_types': '', 'organizations': ''}

    def extract_reporter_name(self, soup, content):
        """ê¸°ìëª… ì¶”ì¶œ"""
        # ì—ì´ë¸”ë‰´ìŠ¤ ê¸°ìëª… ì¶”ì¶œ íŒ¨í„´
        reporter_patterns = [
            r'ê¸°ì\s*([ê°€-í£]{2,4})\s*ê¸°ì',
            r'([ê°€-í£]{2,4})\s*ê¸°ì',
            r'ê¸°ì\s*:\s*([ê°€-í£]{2,4})',
            r'ì·¨ì¬\s*:\s*([ê°€-í£]{2,4})',
            r'ê¸€\s*:\s*([ê°€-í£]{2,4})',
            r'ì‘ì„±ì\s*:\s*([ê°€-í£]{2,4})'
        ]
        
        # HTMLì—ì„œ ê¸°ìëª… ì°¾ê¸°
        reporter_selectors = [
            '.reporter_name',
            '.author_name',
            '.writer_name',
            '.byline'
        ]
        
        for selector in reporter_selectors:
            reporter_elem = soup.select_one(selector)
            if reporter_elem:
                return reporter_elem.get_text().strip()
        
        # í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìëª… íŒ¨í„´ ë§¤ì¹­
        for pattern in reporter_patterns:
            match = re.search(pattern, content)
            if match:
                return match.group(1).strip()
        
        return ''

    def extract_disability_types(self, content):
        """ì¥ì•  ìœ í˜• ì¶”ì¶œ"""
        found_types = []
        for disability_type in self.disability_keywords:
            if disability_type in content:
                found_types.append(disability_type)
        
        return ', '.join(found_types[:5])  # ìµœëŒ€ 5ê°œ

    def extract_organizations(self, content):
        """ê´€ë ¨ ê¸°ê´€/ë‹¨ì²´ ì¶”ì¶œ"""
        org_patterns = [
            r'([ê°€-í£]+ì¥ì• ì¸[ê°€-í£]*ë‹¨ì²´|[ê°€-í£]+ì¥ì• ì¸[ê°€-í£]*í˜‘íšŒ)',
            r'([ê°€-í£]+ë³µì§€ê´€|[ê°€-í£]+ì¬í™œì›)',
            r'(ë³´ê±´ë³µì§€ë¶€|êµìœ¡ë¶€|ê³ ìš©ë…¸ë™ë¶€)',
            r'([ê°€-í£]+ì‹œ|[ê°€-í£]+êµ¬|[ê°€-í£]+êµ°)\s*(ì²­|ì²­ì‚¬)',
            r'([ê°€-í£]+ëŒ€í•™êµ|[ê°€-í£]+ëŒ€í•™)',
            r'([ê°€-í£]*ì¥ì• ì¸[ê°€-í£]*ì„¼í„°)'
        ]
        
        organizations = set()
        for pattern in org_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if isinstance(match, tuple):
                    organizations.add(match[0])
                else:
                    organizations.add(match)
        
        return ', '.join(list(organizations)[:8])  # ìµœëŒ€ 8ê°œ

    def get_category_group(self, category_name):
        """ì¹´í…Œê³ ë¦¬ì˜ ê·¸ë£¹ ë°˜í™˜"""
        for group, categories in self.category_groups.items():
            if category_name in categories:
                return group
        return 'ê¸°íƒ€'

    def crawl_category_feed(self, category, rss_url, max_items=30):
        """ê°œë³„ ì¹´í…Œê³ ë¦¬ RSS í”¼ë“œ í¬ë¡¤ë§"""
        self.logger.info(f"ì—ì´ë¸”ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘: {category}")
        
        # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
        rss_content = self.fetch_rss_feed(rss_url)
        if not rss_content:
            return
        
        # RSS íŒŒì‹±
        rss_items = self.parse_rss_feed(rss_content)
        if not rss_items:
            self.logger.warning(f"RSS ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤: {category}")
            return
        
        # ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ë§Œ ì²˜ë¦¬
        items_to_process = rss_items[:max_items]
        
        for i, item in enumerate(items_to_process, 1):
            try:
                self.logger.info(f"{category} ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘: {i}/{len(items_to_process)} - {item['title'][:50]}...")
                
                # ê¸°ì‚¬ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
                if item['link']:
                    article_detail = self.extract_article_content(item['link'])
                    
                    article_data = {
                        'category': category,
                        'category_group': self.get_category_group(category),
                        'category_description': self.category_descriptions.get(category, ''),
                        'title': item['title'],
                        'link': item['link'],
                        'pub_date': item['pub_date'],
                        'description': item['description'],
                        'content': article_detail['content'],
                        'reporter': article_detail['reporter'],
                        'disability_types': article_detail['disability_types'],
                        'organizations': article_detail['organizations'],
                        'collected_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    
                    self.articles.append(article_data)
                
                # ë”œë ˆì´
                self.random_delay(1, 3)
                
            except Exception as e:
                self.logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        self.logger.info(f"{category} í¬ë¡¤ë§ ì™„ë£Œ: {len(items_to_process)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬")

    def crawl_all_categories(self, max_items_per_category=30):
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ RSS í”¼ë“œ í¬ë¡¤ë§"""
        total_categories = len(self.rss_feeds)
        self.logger.info(f"ì „ì²´ {total_categories}ê°œ ì—ì´ë¸”ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ RSS í”¼ë“œ í¬ë¡¤ë§ ì‹œì‘")
        
        for i, (category, rss_url) in enumerate(self.rss_feeds.items(), 1):
            try:
                self.logger.info(f"[{i}/{total_categories}] {category} í”¼ë“œ í¬ë¡¤ë§ ì¤‘...")
                self.crawl_category_feed(category, rss_url, max_items_per_category)
                
                # ì¹´í…Œê³ ë¦¬ ê°„ ë”œë ˆì´
                if i < total_categories:
                    self.random_delay(2, 4)
                    
            except Exception as e:
                self.logger.error(f"{category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                continue
        
        self.logger.info(f"ì „ì²´ ì—ì´ë¸”ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ: {len(self.articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        self.print_statistics()

    def crawl_specific_categories(self, category_names, max_items_per_category=30):
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ë“¤ë§Œ í¬ë¡¤ë§"""
        for category_name in category_names:
            if category_name in self.rss_feeds:
                self.crawl_category_feed(category_name, self.rss_feeds[category_name], max_items_per_category)
            else:
                self.logger.warning(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬: {category_name}")
                available_categories = list(self.rss_feeds.keys())
                self.logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬: {available_categories}")

    def crawl_by_group(self, groups, max_items_per_category=25):
        """ê·¸ë£¹ë³„ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§"""
        target_categories = []
        
        for group in groups:
            if group in self.category_groups:
                target_categories.extend(self.category_groups[group])
            else:
                self.logger.warning(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê·¸ë£¹: {group}")
        
        if target_categories:
            self.logger.info(f"ê·¸ë£¹ '{', '.join(groups)}'ì— í•´ë‹¹í•˜ëŠ” ì¹´í…Œê³ ë¦¬: {target_categories}")
            self.crawl_specific_categories(target_categories, max_items_per_category)
        else:
            self.logger.warning(f"í•´ë‹¹ ê·¸ë£¹ì— ë§ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {groups}")

    def save_to_csv(self, filename=None):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'results/ì—ì´ë¸”ë‰´ìŠ¤_RSS_{timestamp}.csv'
        
        try:
            df = pd.DataFrame(self.articles)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            self.logger.info(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            self.logger.info(f"ì´ {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥")
        except Exception as e:
            self.logger.error(f"CSV ì €ì¥ ì˜¤ë¥˜: {e}")

    def save_by_category(self):
        """ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê°œë³„ CSV íŒŒì¼ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        df = pd.DataFrame(self.articles)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        for category in df['category'].unique():
            category_df = df[df['category'] == category]
            filename = f'results/ì—ì´ë¸”ë‰´ìŠ¤_{category}_{timestamp}.csv'
            category_df.to_csv(filename, index=False, encoding='utf-8-sig')
            self.logger.info(f"{category} ì €ì¥ ì™„ë£Œ: {filename} ({len(category_df)}ê°œ ê¸°ì‚¬)")

    def save_by_group(self):
        """ê·¸ë£¹ë³„ë¡œ CSV íŒŒì¼ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        df = pd.DataFrame(self.articles)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        for group in df['category_group'].unique():
            group_df = df[df['category_group'] == group]
            filename = f'results/ì—ì´ë¸”ë‰´ìŠ¤ê·¸ë£¹_{group}_{timestamp}.csv'
            group_df.to_csv(filename, index=False, encoding='utf-8-sig')
            self.logger.info(f"{group} ê·¸ë£¹ ì €ì¥ ì™„ë£Œ: {filename} ({len(group_df)}ê°œ ê¸°ì‚¬)")

    def print_statistics(self):
        """í¬ë¡¤ë§ í†µê³„ ì¶œë ¥"""
        if not self.articles:
            return
        
        df = pd.DataFrame(self.articles)
        
        print("\n" + "="*60)
        print("ì—ì´ë¸”ë‰´ìŠ¤ RSS í¬ë¡¤ë§ í†µê³„")
        print("="*60)
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = df['category'].value_counts()
        print(f"\nğŸ“° ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜:")
        for category, count in category_stats.items():
            description = self.category_descriptions.get(category, '')
            print(f"  â€¢ {category} ({description}): {count}ê°œ")
        
        # ê·¸ë£¹ë³„ í†µê³„
        group_stats = df['category_group'].value_counts()
        print(f"\nğŸ“Š ê·¸ë£¹ë³„ ê¸°ì‚¬ ìˆ˜:")
        for group, count in group_stats.items():
            print(f"  â€¢ {group}: {count}ê°œ")
        
        # ì¥ì•  ìœ í˜•ë³„ í†µê³„
        disability_stats = df[df['disability_types'] != '']['disability_types'].str.split(', ').explode().value_counts().head(10)
        if not disability_stats.empty:
            print(f"\nâ™¿ ì£¼ìš” ì¥ì•  ìœ í˜•ë³„ ì–¸ê¸‰ ìˆ˜:")
            for disability_type, count in disability_stats.items():
                print(f"  â€¢ {disability_type}: {count}íšŒ")
        
        # ê¸°ìë³„ í†µê³„
        reporter_stats = df[df['reporter'] != '']['reporter'].value_counts().head(10)
        if not reporter_stats.empty:
            print(f"\nâœï¸ ì£¼ìš” ê¸°ìë³„ ê¸°ì‚¬ ìˆ˜:")
            for reporter, count in reporter_stats.items():
                print(f"  â€¢ {reporter}: {count}ê°œ")
        
        # ê´€ë ¨ ê¸°ê´€/ë‹¨ì²´ í†µê³„
        org_stats = df[df['organizations'] != '']['organizations'].str.split(', ').explode().value_counts().head(8)
        if not org_stats.empty:
            print(f"\nğŸ¢ ì£¼ìš” ê´€ë ¨ ê¸°ê´€/ë‹¨ì²´:")
            for org, count in org_stats.items():
                print(f"  â€¢ {org}: {count}íšŒ")
        
        print(f"\nğŸ“ˆ ì „ì²´ ìš”ì•½:")
        print(f"  â€¢ ì´ ê¸°ì‚¬ ìˆ˜: {len(self.articles)}ê°œ")
        print(f"  â€¢ í¬ë¡¤ë§ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(category_stats)}ê°œ")
        print(f"  â€¢ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {len(df[df['content'] != 'ì¶”ì¶œ ì‹¤íŒ¨'])}ê°œ")
        print(f"  â€¢ ê¸°ìëª… ì¶”ì¶œ: {len(df[df['reporter'] != ''])}ê°œ")
        print(f"  â€¢ ì¥ì•  ìœ í˜• ë§¤ì¹­: {len(df[df['disability_types'] != ''])}ê°œ")
        print(f"  â€¢ ê´€ë ¨ ê¸°ê´€ ì¶”ì¶œ: {len(df[df['organizations'] != ''])}ê°œ")
        print("="*60)

    def get_available_categories(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë°˜í™˜"""
        return list(self.rss_feeds.keys())

    def get_groups(self):
        """ê·¸ë£¹ ëª©ë¡ ë°˜í™˜"""
        return list(self.category_groups.keys())


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ì—ì´ë¸”ë‰´ìŠ¤ RSS í¬ë¡¤ëŸ¬")
    print("="*50)
    
    crawler = AbleNewsRSSCrawler()
    
    # ì‚¬ìš© ì˜ˆì‹œ 1: ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ (ê° ì¹´í…Œê³ ë¦¬ë‹¹ 20ê°œì”©)
    print("ì „ì²´ ì—ì´ë¸”ë‰´ìŠ¤ RSS í”¼ë“œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    crawler.crawl_all_categories(max_items_per_category=20)
    
    # CSV ì €ì¥
    crawler.save_to_csv()
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê°œë³„ íŒŒì¼ ì €ì¥
    # crawler.save_by_category()
    
    # ê·¸ë£¹ë³„ íŒŒì¼ ì €ì¥
    # crawler.save_by_group()
    
    # ì‚¬ìš© ì˜ˆì‹œ 2: íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ í¬ë¡¤ë§
    # crawler.crawl_specific_categories(['ì¸ê¶Œ', 'ë³µì§€', 'ì •ì±…'])
    
    # ì‚¬ìš© ì˜ˆì‹œ 3: ê·¸ë£¹ë³„ í¬ë¡¤ë§
    # crawler.crawl_by_group(['ê¶ŒìµÂ·ì¸ê¶Œ', 'ìƒí™œÂ·ë³µì§€'], max_items_per_category=25)
    
    print("\nì—ì´ë¸”ë‰´ìŠ¤ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
