import feedparser
import csv
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import time
import random
import os

NEWS_OUTLET = "í•œêµ­ê²½ì œ"


def get_random_user_agent():
    """ëœë¤ User-Agent ë°˜í™˜"""
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    ]
    return random.choice(user_agents)


def extract_hankyung_article_content(url, rss_summary=""):
    """í•œêµ­ê²½ì œ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ê³¼ ê¸°ìëª…ì„ ì¶”ì¶œ"""
    try:
        session = requests.Session()

        headers = {
            "User-Agent": get_random_user_agent(),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Referer": "https://www.hankyung.com/",
            "Cache-Control": "no-cache",
        }

        print(f"    ì ‘ì† ì‹œë„: {url[:80]}...")

        # ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ í›„ ì‹¤ì œ ê¸°ì‚¬ ì ‘ê·¼
        try:
            session.get("https://www.hankyung.com/", headers=headers, timeout=5)
            time.sleep(0.5)

            response = session.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            # ì‘ë‹µ í¬ê¸° ì²´í¬
            if len(response.content) < 5000:  # 5KB ë¯¸ë§Œì´ë©´ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ
                print(f"    âš  ì‘ë‹µ í¬ê¸°ê°€ ì‘ìŒ (í¬ê¸°: {len(response.content)} bytes)")

        except Exception as e:
            print(f"    âš  ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return "", rss_summary if rss_summary else "ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨"

        soup = BeautifulSoup(response.content, "html.parser")
        full_text = soup.get_text()

        # ê¸°ìëª… ì¶”ì¶œ - í•œêµ­ê²½ì œ íŒ¨í„´ì— ë§ê²Œ ìˆ˜ì •
        reporter = ""
        reporter_patterns = [
            r"([ê°€-í£]{2,4})\s*ê¸°ì\s*([a-zA-Z0-9_.+-]+@hankyung\.com)",  # ê¸°ìëª… ê¸°ì ì´ë©”ì¼@hankyung.com
            r"([ê°€-í£]{2,4})\s*ê¸°ì\s*[a-zA-Z0-9_.+-]+@hankyung\.com",  # ê¸°ìëª… ê¸°ì ì´ë©”ì¼
            r"([ê°€-í£]{2,4})\s*ê¸°ì",  # ê¸°ìëª… ê¸°ì
            r"ê¸°ì\s*([ê°€-í£]{2,4})",  # ê¸°ì ê¸°ìëª…
            r"([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›",  # ê¸°ìëª… íŠ¹íŒŒì›
            r"([ê°€-í£]{2,4})\s*í¸ì§‘ìœ„ì›",  # ê¸°ìëª… í¸ì§‘ìœ„ì›
            r"([ê°€-í£]{2,4})\s*íŒ€ì¥",  # ê¸°ìëª… íŒ€ì¥
            r"([ê°€-í£]{2,4})\s*ê¸°ì\s*=",  # ê¸°ìëª… ê¸°ì =
        ]

        # ê¸°ì‚¬ ë³¸ë¬¸ ë ë¶€ë¶„ì—ì„œ ê¸°ìëª…ì„ ì°¾ëŠ” ê²ƒì´ ë” ì •í™•
        article_end = full_text[-1000:]  # ë§ˆì§€ë§‰ 1000ìì—ì„œ ì°¾ê¸°

        for pattern in reporter_patterns:
            match = re.search(pattern, article_end)
            if match:
                reporter = match.group(1)
                reporter = re.sub(r"ê¸°ì|íŠ¹íŒŒì›|í¸ì§‘ìœ„ì›|íŒ€ì¥", "", reporter).strip()
                if 2 <= len(reporter) <= 4:
                    break

        # ë³¸ë¬¸ ì¶”ì¶œ - ì›ë¬¸ í˜ì´ì§€ì˜ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆì—ì„œë§Œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        content = ""

        # ìš°ì„ ìˆœìœ„: ì •í™•í•œ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì„ íƒ
        container = soup.select_one('div.article-body#articletxt[itemprop="articleBody"]')
        if not container:
            container = soup.select_one("div.article-body")
        if not container:
            container = soup.select_one("article")

        def _has_ad_class(el):
            cls = el.get("class") or []
            if isinstance(cls, str):
                cls = [cls]
            combined = " ".join([c.lower() for c in cls])
            ad_keys = [
                "ad",
                "ad-area",
                "ad_wrap",
                "ad-wrap",
                "ad-box",
                "promotion",
                "promo",
                "sns",
                "share",
                "tag",
                "related",
                "recommend",
                "subscribe",
                "banner",
                "thumb",
            ]
            return any(k in combined for k in ad_keys)

        if container:
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for el in container.find_all(["figure", "script", "style", "noscript", "iframe", "aside"]):
                el.decompose()
            for el in container.find_all(True):
                if _has_ad_class(el):
                    el.decompose()

            # ì»¨í…Œì´ë„ˆ ë‚´ë¶€ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (br êµ¬ë¶„ ë³´ì¡´)
            content = container.get_text(separator=" ", strip=True)
            # ê¸°ìëª… íƒìƒ‰ì€ ë³¸ë¬¸ ëë‹¨ì„ ìš°ì„  ì‚¬ìš©
            container_text = container.get_text(separator=" ", strip=True)
            article_end = container_text[-800:] if len(container_text) > 0 else full_text[-1000:]
        else:
            # í´ë°±: ê¸°ì¡´ì˜ ë²”ìš© ì„ íƒì ì‚¬ìš©
            content_selectors = [
                "div.article-body",
                'div[class*="article"]',
                'div[class*="content"]',
                'div[class*="news"]',
                "div.wrap_cont",
                "article",
                "main",
                'div[id*="article"]',
                "div.text",
            ]
            for selector in content_selectors:
                elements = soup.select(selector)
                for element in elements:
                    text = element.get_text(separator=" ", strip=True)
                    if len(text) > len(content):
                        content = text
            article_end = full_text[-1000:]

        # ë³¸ë¬¸ ì •ì œ
        content = clean_hankyung_content(content)

        print(f"    ìµœì¢… ë³¸ë¬¸ ê¸¸ì´: {len(content)}")
        return reporter, content

    except Exception as e:
        print(f"    âŒ ì—ëŸ¬: {e}")
        return "", rss_summary if rss_summary else f"ì˜¤ë¥˜: {str(e)}"


def clean_hankyung_content(content):
    """í•œêµ­ê²½ì œ ê¸°ì‚¬ ë³¸ë¬¸ ì •ì œ"""
    if not content:
        return ""

    # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ë“¤ ì œê±° - í•œêµ­ê²½ì œ íŠ¹ì„±ì— ë§ê²Œ ìˆ˜ì •
    remove_patterns = [
        r"ì…ë ¥\s*\d{4}\.\d{2}\.\d{2}.*?\d{2}:\d{2}",
        r"ìˆ˜ì •\s*\d{4}\.\d{2}\.\d{2}.*?\d{2}:\d{2}",
        r"í•œêµ­ê²½ì œ.*ë¬´ë‹¨.*ì „ì¬.*ê¸ˆì§€",
        r"ë¬´ë‹¨.*ì „ì¬.*ì¬ë°°í¬.*ê¸ˆì§€",
        r"ì €ì‘ê¶Œ.*í•œêµ­ê²½ì œ",
        r"ê´€ë ¨ê¸°ì‚¬.*ë”ë³´ê¸°",
        r"í˜ì´ìŠ¤ë¶.*íŠ¸ìœ„í„°.*ì¹´ì¹´ì˜¤",
        r"êµ¬ë….*ì‹ ì²­",
        r"ê´‘ê³ ",
        r"[ê°€-í£]{2,4}\s*ê¸°ì\s*[a-zA-Z0-9_.+-]+@hankyung\.com",  # ê¸°ì ì´ë©”ì¼ ì œê±°
        r"ì—°í•©ë‰´ìŠ¤.*ì œê³µ",  # ë‰´ìŠ¤ ì¶œì²˜ ì œê±°
        r"í•œêµ­ê²½ì œ.*ì œê³µ",  # ì‚¬ì§„ ì¶œì²˜ ì œê±°
        r"í•œê²½ë‹·ì»´",
        r"â“’.*í•œêµ­ê²½ì œ",
        r"ê²½ì œTV.*ì¦ê¶Œ.*ë¶€ë™ì‚°",  # í•œêµ­ê²½ì œ ë©”ë‰´ ì œê±°
    ]

    for pattern in remove_patterns:
        content = re.sub(pattern, "", content, flags=re.IGNORECASE)

    # ê³µë°± ì •ë¦¬
    content = re.sub(r"\s+", " ", content).strip()

    # ê¸¸ì´ ì œí•œ
    if len(content) > 1500:
        content = content[:1500] + "..."

    return content


def append_hankyung_rss_to_writer(rss_url, writer, max_articles=30, category_hint: str | None = None):
    """í•œêµ­ê²½ì œ RSSë¥¼ íŒŒì‹±í•˜ì—¬ ì£¼ì–´ì§„ writerì— í–‰ì„ ì¶”ê°€"""

    print(f"í•œêµ­ê²½ì œ RSS í”¼ë“œ íŒŒì‹± ì¤‘: {rss_url}")

    # RSS íŒŒì‹±
    try:
        headers = {"User-Agent": get_random_user_agent()}
        response = requests.get(rss_url, headers=headers, timeout=10)
        response.encoding = "utf-8"
        feed = feedparser.parse(response.content)
    except Exception:
        feed = feedparser.parse(rss_url)

    if not feed.entries:
        print("âŒ RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0, 0

    print(f"âœ… RSSì—ì„œ {len(feed.entries)}ê°œ ê¸°ì‚¬ ë°œê²¬")

    success_count = 0
    total_count = min(len(feed.entries), max_articles)

    print(f"ì´ {total_count}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘...\n")

    for i, entry in enumerate(feed.entries[:max_articles]):
        try:
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            title = entry.title.strip()
            title = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", title)

            link = entry.link

            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            category = ""
            if hasattr(entry, "category") and entry.category:
                category = entry.category.strip()
            elif hasattr(entry, "tags") and entry.tags:
                try:
                    category = entry.tags[0].term or ""
                except Exception:
                    category = ""
            # URLì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì‹œë„
            if not category and isinstance(link, str):
                url_category_map = {
                    "/economy/": "ê²½ì œ",
                    "/finance/": "ì¦ê¶Œ",
                    "/realestate/": "ë¶€ë™ì‚°",
                    "/politics/": "ì •ì¹˜",
                    "/society/": "ì‚¬íšŒ",
                    "/international/": "êµ­ì œ",
                    "/life/": "ìƒí™œ",
                    "/sports/": "ìŠ¤í¬ì¸ ",
                    "/it/": "IT",
                    "/video/": "VIDEO",
                    "/opinion/": "ì˜¤í”¼ë‹ˆì–¸",
                    "/entertainment/": "ì—°ì˜ˆ",
                }
                for url_part, cat_name in url_category_map.items():
                    if url_part in link:
                        category = cat_name
                        break
            # íŒíŠ¸ ì‚¬ìš©
            if not category and category_hint:
                category = category_hint

            # RSS ìš”ì•½ ì •ë³´ ì¶”ì¶œ (ë³¸ë¬¸ ì •ì œì— ì‚¬ìš©ë  ìˆ˜ ìˆìœ¼ë‚˜, ìš°ì„  ì›ë¬¸ ë³¸ë¬¸ ì‚¬ìš©)
            summary = ""
            if hasattr(entry, "description") and entry.description:
                summary = entry.description.strip()
                summary = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", summary, flags=re.DOTALL)
                summary = re.sub(r"<[^>]+>", "", summary)
                summary = clean_hankyung_content(summary)

            # ë‚ ì§œ í˜•ì‹ ë³€í™˜
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S")
            else:
                date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            # ê¸°ìëª…ì€ RSS authorì—ì„œ ì¶”ì¶œ
            reporter = ""
            if hasattr(entry, "author") and entry.author:
                reporter = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", str(entry.author)).strip()

            print(f"[{i+1}/{total_count}] {title[:60]}...")

            # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ì›ë¬¸ í˜ì´ì§€)
            _ignored_reporter, content = extract_hankyung_article_content(link, summary)

            # ìµœì†Œ ì¡°ê±´ í™•ì¸
            if len(content.strip()) < 20:
                print(f"    âš  ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ ê±´ë„ˆëœ€ (ê¸¸ì´: {len(content)})\n")
                continue

            # CSVì— ì“°ê¸° (ì—´ ìˆœì„œ: ì–¸ë¡ ì‚¬, ì œëª©, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, ê¸°ìëª…, ë³¸ë¬¸)
            writer.writerow(
                {
                    "ì–¸ë¡ ì‚¬": NEWS_OUTLET,
                    "ì œëª©": title,
                    "ë‚ ì§œ": date,
                    "ì¹´í…Œê³ ë¦¬": category if category else "ë¯¸ë¶„ë¥˜",
                    "ê¸°ìëª…": reporter if reporter else "ë¯¸ìƒ",
                    "ë³¸ë¬¸": content,
                }
            )

            success_count += 1
            print(
                f"    âœ… ì„±ê³µ! (ì¹´í…Œê³ ë¦¬: {category}, ê¸°ì: {reporter if reporter else 'ë¯¸ìƒ'}, ë³¸ë¬¸: {len(content)}ì)"
            )

            # ì§„í–‰ë¥  í‘œì‹œ
            if (i + 1) % 5 == 0:
                print(f"\nğŸ“Š ì§„í–‰ë¥ : {i+1}/{total_count} ({(i+1)/total_count*100:.1f}%)")
                print(f"ğŸ“ˆ ì„±ê³µë¥ : {success_count}/{i+1} ({success_count/(i+1)*100:.1f}%)\n")

            # ëœë¤ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            delay = random.uniform(1.0, 2.5)
            time.sleep(delay)

        except KeyboardInterrupt:
            print("\nâš  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"    âŒ ì˜¤ë¥˜: {e}")
            continue

    return success_count, total_count


# ìë™ ìˆ˜ì§‘ ì‹¤í–‰ ì˜ˆì‹œ (ëª¨ë“  ì¹´í…Œê³ ë¦¬ë¥¼ 20ê°œì”© ìˆ˜ì§‘)
if __name__ == "__main__":
    # í•œêµ­ê²½ì œ RSS URL ì˜µì…˜ë“¤
    hankyung_rss_options = {
        "ì „ì²´ë‰´ìŠ¤": "https://www.hankyung.com/feed/all-news",
        "ê²½ì œ": "https://www.hankyung.com/feed/economy",
        "ì¦ê¶Œ": "https://www.hankyung.com/feed/finance",
        "ë¶€ë™ì‚°": "https://www.hankyung.com/feed/realestate",
        "ì •ì¹˜": "https://www.hankyung.com/feed/politics",
        "ì‚¬íšŒ": "https://www.hankyung.com/feed/society",
        "êµ­ì œ": "https://www.hankyung.com/feed/international",
        "IT": "https://www.hankyung.com/feed/it",
        "ìƒí™œ": "https://www.hankyung.com/feed/life",
        "ì˜¤í”¼ë‹ˆì–¸": "https://www.hankyung.com/feed/opinion",
    }

    max_articles = 20
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")

    print("í•œêµ­ê²½ì œ RSS ìë™ ìˆ˜ì§‘ê¸° (ì¹´í…Œê³ ë¦¬ë³„ 20ê°œ â†’ ë‹¨ì¼ CSV)\n" + "=" * 50)

    # ë‹¨ì¼ CSV íŒŒì¼ ì¤€ë¹„
    output_file = f"results/{NEWS_OUTLET}_ì „ì²´_{timestamp}.csv"
    out_dir = os.path.dirname(output_file)
    if out_dir:
        os.makedirs(out_dir, exist_ok=True)

    with open(output_file, "w", newline="", encoding="utf-8-sig") as csvfile:
        fieldnames = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        total_success = 0
        total_expected = 0

        for category_name, rss_url in hankyung_rss_options.items():
            print(f"\nğŸš€ [{category_name}] ì¹´í…Œê³ ë¦¬ì—ì„œ {max_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘!")
            print(f"ğŸ“ ì €ì¥ íŒŒì¼: {output_file}\n")
            success, expected = append_hankyung_rss_to_writer(
                rss_url, writer, max_articles, category_hint=category_name
            )
            total_success += success
            total_expected += expected
            # ì¹´í…Œê³ ë¦¬ ê°„ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(random.uniform(1.5, 3.0))

    print(f"\n{'='*70}")
    print(f"ğŸ‰ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥: {output_file}")
    if total_expected:
        print(
            f"ğŸ“Š ìµœì¢… ê²°ê³¼: {total_success}/{total_expected*len(hankyung_rss_options)}ê°œ ì‹œë„ ì¤‘ {total_success}ê±´ ì„±ê³µ"
        )
    print(f"{'='*70}")
