import feedparser
import requests
from bs4 import BeautifulSoup
import csv
import time
import random
from datetime import datetime
import re
import urllib.parse


class DaeguShinmunRSSCollector:
    def __init__(self):
        self.base_url = "https://www.idaegu.co.kr"
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
        ]

        # ëŒ€êµ¬ì‹ ë¬¸ RSS í”¼ë“œ ì¹´í…Œê³ ë¦¬
        self.rss_categories = {
            "ì „ì²´ê¸°ì‚¬": "allArticle.xml",
            "ì¸ê¸°ê¸°ì‚¬": "clickTop.xml",
            "ì •ì¹˜": "S1N1.xml",
            "ê²½ì œ": "S1N2.xml",
            "ì‚¬íšŒ": "S1N3.xml",
            "ê²½ë¶": "S1N4.xml",
            "ë¬¸í™”": "S1N5.xml",
            "ìŠ¤í¬ì¸ ": "S1N6.xml",
            "ì˜¤í”¼ë‹ˆì–¸": "S1N7.xml",
            "í¬í† ë‰´ìŠ¤": "S1N8.xml",
            "ì‚¬ëŒë“¤": "S1N9.xml",
            "ì—¬ëŸ¬ì´ëŠ”ë¯¸ë˜ë‹¤": "S1N10.xml",
            "ë…ìë§ˆë‹¹": "S1N11.xml",
            "ê¸°íšíŠ¹ì§‘": "S1N12.xml",
            "ì¢…í•©": "S1N13.xml",
        }

        self.session = requests.Session()

    def get_random_user_agent(self):
        """ëœë¤ User-Agent ë°˜í™˜"""
        return random.choice(self.user_agents)

    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text:
            return ""

        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r"<[^>]+>", "", text)
        # íŠ¹ìˆ˜ë¬¸ì ì •ì œ
        text = re.sub(r"[\r\n\t]+", " ", text)
        text = re.sub(r"\s+", " ", text)
        # ë”°ì˜´í‘œ ì²˜ë¦¬
        text = text.replace('"', '""')

        return text.strip()

    def extract_reporter_name(self, article_url):
        """ê¸°ì‚¬ URLì—ì„œ ê¸°ìëª… ì¶”ì¶œ"""
        try:
            headers = {"User-Agent": self.get_random_user_agent()}
            response = self.session.get(article_url, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            # ëŒ€êµ¬ì‹ ë¬¸ ê¸°ìëª… íŒ¨í„´ ì°¾ê¸°
            reporter_patterns = [
                # ê¸°ì‚¬ ë³¸ë¬¸ ë ê¸°ìëª… íŒ¨í„´: "ê¹€ì§„ì˜¤ê¸°ì kimjo@idaegu.co.kr"
                r"([ê°€-í£]{2,4})ê¸°ì\s+[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
                # ê¸°ìëª…ë§Œ: "ê¹€ì§„ì˜¤"
                r"([ê°€-í£]{2,4})ê¸°ì",
                # ê¸°ì‚¬ ë‚´ ê¸°ìëª… íƒœê·¸
                r'<[^>]*class="reporter"[^>]*>([ê°€-í£]{2,4})',
                r"<[^>]*ê¸°ì[^>]*>([ê°€-í£]{2,4})",
            ]

            article_text = soup.get_text()

            for pattern in reporter_patterns:
                match = re.search(pattern, article_text)
                if match:
                    return match.group(1).strip()

            # ì¶”ê°€ íŒ¨í„´: ê¸°ì‚¬ ë‚´ìš©ì—ì„œ "â—‹â—‹â—‹ê¸°ì" í˜•íƒœ ì°¾ê¸°
            reporter_match = re.search(r"([ê°€-í£]{2,4})ê¸°ì(?:\s|$)", article_text)
            if reporter_match:
                return reporter_match.group(1)

        except Exception as e:
            print(f"ê¸°ìëª… ì¶”ì¶œ ì˜¤ë¥˜ ({article_url}): {e}")

        return "ì •ë³´ì—†ìŒ"

    def collect_rss_feed(self, category_name, rss_file):
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ RSS í”¼ë“œ ìˆ˜ì§‘"""
        rss_url = f"{self.base_url}/rss/{rss_file}"

        try:
            print(f"{category_name} ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì¤‘: {rss_url}")

            headers = {"User-Agent": self.get_random_user_agent()}
            response = self.session.get(rss_url, headers=headers, timeout=15)
            response.raise_for_status()

            # RSS íŒŒì‹±
            feed = feedparser.parse(response.content)

            if not feed.entries:
                print(f"âŒ {category_name}: RSS í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
                return []

            articles = []

            for entry in feed.entries:
                if len(articles) >= 20:
                    break
                try:
                    # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                    title = self.clean_text(entry.title)
                    link = entry.link

                    # ë°œí–‰ì¼ ì²˜ë¦¬
                    pub_date = ""
                    if hasattr(entry, "published"):
                        try:
                            pub_date = datetime.strptime(entry.published, "%Y-%m-%d %H:%M:%S").strftime(
                                "%Y-%m-%d %H:%M:%S"
                            )
                        except:
                            pub_date = entry.published

                    # ìš”ì•½ ë‚´ìš©
                    summary = ""
                    if hasattr(entry, "summary"):
                        summary = self.clean_text(entry.summary)
                    try:
                        # ë³¸ë¬¸ ì¶”ì¶œ: ì§€ì •ëœ xpath ì´ìš©
                        resp = self.session.get(link, headers=headers, timeout=15)
                        resp.raise_for_status()
                        article_soup = BeautifulSoup(resp.content, "html.parser")
                        content_elem = article_soup.select_one(
                            "body > div:nth-of-type(2) > div > div:nth-of-type(1) > main > div:nth-of-type(4) > div > section > article > div:nth-of-type(1) > div > article:nth-of-type(1) > div > div > font"
                        )
                        if content_elem:
                            summary = self.clean_text(content_elem.get_text())
                    except Exception as e:
                        print(f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜ ({link}): {e}")

                    # ê¸°ìëª… ì¶”ì¶œ (rssì˜ author ë¶€ë¶„ ì‚¬ìš©)
                    reporter = self.clean_text(entry.author) if "author" in entry else "ì •ë³´ì—†ìŒ"

                    article_data = {
                        "category": category_name,
                        "title": title,
                        "link": link,
                        "published": pub_date,
                        "summary": summary,
                        "reporter": reporter,
                        "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    }

                    articles.append(article_data)

                except Exception as e:
                    print(f"ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue

            print(f"âœ… {category_name}: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
            return articles

        except Exception as e:
            print(f"âŒ {category_name} RSS ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    def save_to_csv(self, all_articles, filename=None):
        """ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì„ CSV íŒŒì¼ë¡œ ì €ì¥ (ì–¸ë¡ ì‚¬, ì œëª©, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, ê¸°ìëª…, ë³¸ë¬¸ ìˆœ)"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"results/ëŒ€êµ¬ì‹ ë¬¸_ì „ì²´_{timestamp}.csv"

        try:
            with open(filename, "w", newline="", encoding="utf-8-sig") as csvfile:
                fieldnames = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

                writer.writeheader()
                for article in all_articles:
                    writer.writerow(
                        {
                            "ì–¸ë¡ ì‚¬": "ëŒ€êµ¬ì‹ ë¬¸",
                            "ì œëª©": article.get("title", ""),
                            "ë‚ ì§œ": article.get("published", ""),
                            "ì¹´í…Œê³ ë¦¬": article.get("category", ""),
                            "ê¸°ìëª…": article.get("reporter", ""),
                            "ë³¸ë¬¸": article.get("summary", ""),
                        }
                    )

            print(f"ğŸ“„ CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            return filename

        except Exception as e:
            print(f"âŒ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

    def collect_all_categories(self, selected_categories=None):
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë˜ëŠ” ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì˜ RSS ìˆ˜ì§‘"""
        if selected_categories is None:
            selected_categories = list(self.rss_categories.keys())

        print("ğŸ—ï¸  ëŒ€êµ¬ì‹ ë¬¸ RSS ìˆ˜ì§‘ê¸° ì‹œì‘")
        print("=" * 50)

        all_articles = []

        for category in selected_categories:
            if category in self.rss_categories:
                rss_file = self.rss_categories[category]
                articles = self.collect_rss_feed(category, rss_file)
                all_articles.extend(articles)

                # ìš”ì²­ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(random.uniform(1.0, 2.0))
            else:
                print(f"âš ï¸  ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬: {category}")

        print("=" * 50)
        print(f"ğŸ“Š ì´ ìˆ˜ì§‘ ê¸°ì‚¬: {len(all_articles)}ê°œ")

        if all_articles:
            saved_file = self.save_to_csv(all_articles)
            if saved_file:
                print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ! íŒŒì¼: {saved_file}")

        return all_articles


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    collector = DaeguShinmunRSSCollector()

    # ê³ ì •ëœ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì„¤ì •
    selected_categories = ["ì „ì²´ê¸°ì‚¬", "ì •ì¹˜", "ê²½ì œ", "ì‚¬íšŒ", "ê²½ë¶", "ë¬¸í™”", "ì˜¤í”¼ë‹ˆì–¸"]
    print("ğŸ”„ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤: " + ", ".join(selected_categories))

    # ê° ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ 20ê°œì”© ìˆ˜ì§‘
    articles = collector.collect_all_categories(selected_categories)

    if articles:
        print(f"\nğŸ‰ ëŒ€êµ¬ì‹ ë¬¸ RSS ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ˆ ì´ {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
