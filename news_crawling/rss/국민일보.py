import feedparser
import csv
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import time
import random
import os


def get_random_user_agent():
    """ëœë¤ User-Agent ë°˜í™˜"""
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    ]
    return random.choice(user_agents)


def extract_kmib_article_content(url, rss_summary=""):
    """êµ­ë¯¼ì¼ë³´ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ê³¼ ê¸°ìëª…ì„ ì¶”ì¶œ"""
    try:
        session = requests.Session()

        headers = {
            "User-Agent": get_random_user_agent(),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Referer": "https://www.kmib.co.kr/",
            "Cache-Control": "no-cache",
        }

        print(f"    ì ‘ì† ì‹œë„: {url[:80]}...")

        # ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ í›„ ì‹¤ì œ ê¸°ì‚¬ ì ‘ê·¼
        try:
            session.get("https://www.kmib.co.kr/", headers=headers, timeout=5)
            time.sleep(0.5)

            response = session.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            # êµ­ë¯¼ì¼ë³´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì ‘ê·¼ ì œí•œì´ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ í¬ê¸° ì²´í¬
            if len(response.content) < 5000:  # 5KB ë¯¸ë§Œì´ë©´ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ
                print(f"    âš  ì‘ë‹µ í¬ê¸°ê°€ ì‘ìŒ (í¬ê¸°: {len(response.content)} bytes)")

        except Exception as e:
            print(f"    âš  ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return "", rss_summary if rss_summary else "ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨"

        soup = BeautifulSoup(response.content, "html.parser")
        full_text = soup.get_text()

        # ê¸°ìëª… ì¶”ì¶œ - êµ­ë¯¼ì¼ë³´ íŒ¨í„´ì— ë§ê²Œ ìˆ˜ì •
        reporter = ""
        reporter_patterns = [
            r"([ê°€-í£]{2,4})\s*ê¸°ì\s*([a-zA-Z0-9_.+-]+@kmib\.co\.kr)",  # ê¸°ìëª… ê¸°ì ì´ë©”ì¼@kmib.co.kr
            r"([ê°€-í£]{2,4})\s*ê¸°ì\s*[a-zA-Z0-9_.+-]+@kmib\.co\.kr",  # ê¸°ìëª… ê¸°ì ì´ë©”ì¼
            r"([ê°€-í£]{2,4})\s*ê¸°ì",  # ê¸°ìëª… ê¸°ì
            r"ê¸°ì\s*([ê°€-í£]{2,4})",  # ê¸°ì ê¸°ìëª…
            r"([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›",  # ê¸°ìëª… íŠ¹íŒŒì›
            r"([ê°€-í£]{2,4})\s*íŒ€ì¥",  # ê¸°ìëª… íŒ€ì¥
        ]

        # ê¸°ì‚¬ ë³¸ë¬¸ ë ë¶€ë¶„ì—ì„œ ê¸°ìëª…ì„ ì°¾ëŠ” ê²ƒì´ ë” ì •í™•
        article_end = full_text[-1000:]  # ë§ˆì§€ë§‰ 1000ìì—ì„œ ì°¾ê¸°

        for pattern in reporter_patterns:
            match = re.search(pattern, article_end)
            if match:
                reporter = match.group(1)
                reporter = re.sub(r"ê¸°ì|íŠ¹íŒŒì›|íŒ€ì¥", "", reporter).strip()
                if len(reporter) >= 2 and len(reporter) <= 4:
                    break

        # ë³¸ë¬¸ ì¶”ì¶œ - êµ­ë¯¼ì¼ë³´ HTML êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
        content = ""

        # ë°©ë²• 1: ì§€ì •ëœ XPath(/html/body/div[1]/div/section/article/div[1]/div[1])ì— í•´ë‹¹í•˜ëŠ” ë³¸ë¬¸ ì¶”ì¶œ
        xpath_like_selectors = [
            "html > body > div:nth-of-type(1) > div > section > article > div:nth-of-type(1) > div:nth-of-type(1)",
            "body > div:nth-of-type(1) > div > section > article > div:nth-of-type(1) > div:nth-of-type(1)",
        ]

        for sel in xpath_like_selectors:
            target_element = soup.select_one(sel)
            if target_element and len(target_element.get_text(strip=True)) > 50:
                content = target_element.get_text().strip()
                break

        # ë°©ë²• 2: ë°±ì—…ìš© - ê¸°ì¡´ ì„ íƒìë“¤
        if len(content) < 200:
            content_selectors = [
                "div.article_txt",  # êµ­ë¯¼ì¼ë³´ ê¸°ì‚¬ ë³¸ë¬¸ í´ë˜ìŠ¤
                'div[class*="article"]',  # article ê´€ë ¨ í´ë˜ìŠ¤
                'div[class*="content"]',  # content ê´€ë ¨ í´ë˜ìŠ¤
                'div[class*="news"]',  # news ê´€ë ¨ í´ë˜ìŠ¤
                "article",  # article íƒœê·¸
                "main",  # main íƒœê·¸
            ]

            for selector in content_selectors:
                elements = soup.select(selector)
                for element in elements:
                    text = element.get_text().strip()
                    if len(text) > len(content):
                        content = text

        # ë°©ë²• 2: P íƒœê·¸ ê¸°ë°˜ ì¶”ì¶œ (êµ­ë¯¼ì¼ë³´ íŠ¹ì„±ì— ë§ê²Œ ìˆ˜ì •)
        if len(content) < 200:
            paragraphs = soup.find_all("p")
            content_parts = []

            for p in paragraphs:
                text = p.get_text().strip()
                if (
                    len(text) > 20
                    and not re.search(r"ì…ë ¥\s*\d{4}|ì—…ë°ì´íŠ¸\s*\d{4}|Copyright|ì €ì‘ê¶Œ|êµ­ë¯¼ì¼ë³´|GoodNews paper", text)
                    and not text.startswith(("â–¶", "â˜", "â€»", "â– ", "â–²", "[", "â€»"))
                    and "@kmib.co.kr" not in text
                ):
                    content_parts.append(text)

            if content_parts:
                content = " ".join(content_parts)

        # ë³¸ë¬¸ ì •ì œ
        content = clean_kmib_content(content)

        # RSS ìš”ì•½ì´ ë” ì¢‹ìœ¼ë©´ RSS ìš”ì•½ ì‚¬ìš©
        if rss_summary and (len(content) < 100 or len(rss_summary) > len(content)):
            content = rss_summary
            print(f"    RSS ìš”ì•½ ì±„íƒ (ê¸¸ì´: {len(rss_summary)})")

        print(f"    ìµœì¢… ë³¸ë¬¸ ê¸¸ì´: {len(content)}")
        return reporter, content

    except Exception as e:
        print(f"    âŒ ì—ëŸ¬: {e}")
        return "", rss_summary if rss_summary else f"ì˜¤ë¥˜: {str(e)}"


def clean_kmib_content(content):
    """êµ­ë¯¼ì¼ë³´ ê¸°ì‚¬ ë³¸ë¬¸ ì •ì œ"""
    if not content:
        return ""

    # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ë“¤ ì œê±° - êµ­ë¯¼ì¼ë³´ íŠ¹ì„±ì— ë§ê²Œ ìˆ˜ì •
    remove_patterns = [
        r"ì…ë ¥\s*\d{4}\.\d{2}\.\d{2}.*?\d{2}:\d{2}",
        r"ì—…ë°ì´íŠ¸\s*\d{4}\.\d{2}\.\d{2}.*?\d{2}:\d{2}",
        r"GoodNews paper.*êµ­ë¯¼ì¼ë³´",
        r"ë¬´ë‹¨.*ì „ì¬.*ê¸ˆì§€",
        r"AIí•™ìŠµ.*ì´ìš©.*ê¸ˆì§€",
        r"ê´€ë ¨ê¸°ì‚¬.*ë”ë³´ê¸°",
        r"í˜ì´ìŠ¤ë¶.*íŠ¸ìœ„í„°.*ì¹´ì¹´ì˜¤",
        r"êµ¬ë….*ì‹ ì²­",
        r"ê´‘ê³ ",
        r"[ê°€-í£]{2,4}\s*ê¸°ì\s*[a-zA-Z0-9_.+-]+@kmib\.co\.kr",  # ê¸°ì ì´ë©”ì¼ ì œê±°
        r"LCK\s*ì œê³µ",  # ì‚¬ì§„ ì¶œì²˜ ì œê±°
        r"ì—°í•©ë‰´ìŠ¤.*ì œê³µ",  # ì‚¬ì§„ ì¶œì²˜ ì œê±°
    ]

    for pattern in remove_patterns:
        content = re.sub(pattern, "", content, flags=re.IGNORECASE)

    # ê³µë°± ì •ë¦¬
    content = re.sub(r"\s+", " ", content).strip()

    # ê¸¸ì´ ì œí•œ
    if len(content) > 1500:
        content = content[:1500] + "..."

    return content


def _is_valid_korean_name(name: str) -> bool:
    """2~4ì í•œê¸€ ì´ë¦„ì´ë©° ê¸ˆì§€ì–´ê°€ ì•„ë‹Œì§€ ê²€ì‚¬"""
    if not name:
        return False
    name = str(name).strip()
    invalid_terms = {
        "ì„œë¹„ìŠ¤",
        "êµ­ë¯¼ì¼ë³´",
        "ê´€ë¦¬ì",
        "ìš´ì˜ì",
        "ë°ìŠ¤í¬",
        "í¸ì§‘",
        "ì˜¨ë¼ì¸",
        "ë‰´ìŠ¤",
    }
    if name in invalid_terms:
        return False
    return re.fullmatch(r"[ê°€-í£]{2,4}", name) is not None


def parse_reporter_from_author(author_info: str) -> str:
    """RSS author ë¬¸ìì—´ì—ì„œ ê¸°ìëª…ë§Œ ì¶”ì¶œ

    ì˜ˆ) "ê¹€ì„¸í›ˆ ê¸°ì ksh3712@kyunghyang.com" -> "ê¹€ì„¸í›ˆ"
        "ì‘ì„±ì ê¹€ì„¸í›ˆ ê¸°ì" -> "ê¹€ì„¸í›ˆ"
        "ê¸°ì ê¹€ì„¸í›ˆ" -> "ê¹€ì„¸í›ˆ"
        "<![CDATA[ ê¹€ì„¸í›ˆ ê¸°ì ksh3712@kyunghyang.com ]]>" -> "ê¹€ì„¸í›ˆ" (CDATA ì²˜ë¦¬ ì¶”ê°€)
    """
    if not author_info:
        return ""

    # CDATA ì œê±° ë° ê³µë°± ì •ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼í•˜ì§€ë§Œ, ë” ê°•ë ¥í•˜ê²Œ ì²˜ë¦¬)
    text = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", author_info, flags=re.DOTALL).strip()
    text = re.sub(r"\s+", " ", text)

    # ì–¸ë¡ ì‚¬ëª…ë§Œ ìˆëŠ” ê²½ìš° ë°°ì œ
    if text in {"êµ­ë¯¼ì¼ë³´", "ì„œë¹„ìŠ¤"}:
        return ""

    # íŒ¨í„´ ëª©ë¡ (ê¸°ì¡´ íŒ¨í„´ ìœ ì§€, í•„ìš” ì‹œ ì¼ë°˜í™”)
    patterns = [
        r"([ê°€-í£]{2,4})\s*ê¸°ì\s*[a-zA-Z0-9_.+-]+@[a-zA-Z0-9.-]+",  # ì´ë¦„ ê¸°ì email (ì˜ˆ: ê¹€ì„¸í›ˆ ê¸°ì ksh3712@kyunghyang.com)
        r"([ê°€-í£]{2,4})\s*(?:ì¸í„´)?ê¸°ì",  # ì´ë¦„ (ì¸í„´)ê¸°ì
        r"ê¸°ì\s*([ê°€-í£]{2,4})",  # ê¸°ì ì´ë¦„
        r"([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›",  # ì´ë¦„ íŠ¹íŒŒì›
        r"([ê°€-í£]{2,4})\s*íŒ€ì¥",  # ì´ë¦„ íŒ€ì¥
        r"ì‘ì„±ì\s*([ê°€-í£]{2,4})\s*ê¸°ì",  # ì‘ì„±ì ì´ë¦„ ê¸°ì (ì¶”ê°€ íŒ¨í„´)
    ]

    for p in patterns:
        m = re.search(p, text)
        if m:
            name = m.group(1).strip()
            # ë°©ì–´ì  í›„ì²˜ë¦¬ (ê¸°ì, ì¸í„´ ë“± ì œê±°)
            name = re.sub(r"(ê¸°ì|ì¸í„´|íŠ¹íŒŒì›|íŒ€ì¥|ì‘ì„±ì)$", "", name).strip()
            if _is_valid_korean_name(name):
                return name

    # ì¼ë°˜ í…ìŠ¤íŠ¸ì— í¬í•¨ëœ "ì´ë¦„ ê¸°ì email" íŒ¨í„´ì„ í›„ìˆœìœ„ë¡œ í•œ ë²ˆ ë” ì‹œë„
    m = re.search(r"([ê°€-í£]{2,4})\s*ê¸°ì\s*[a-zA-Z0-9_.+-]+@[a-zA-Z0-9.-]+", text)
    if m:
        name = m.group(1).strip()
        if _is_valid_korean_name(name):
            return name

    return ""


def get_rss_reporter(entry) -> tuple[str, str]:
    """feedparser entryì—ì„œ ê¸°ìëª… í›„ë³´ ë¬¸ìì—´ì„ ëª¨ì•„ íŒŒì‹±í•œë‹¤.
    ë°˜í™˜: (ê¸°ìëª…, ì›ë³¸ë¬¸ìì—´) â€” ê¸°ìëª…ì„ ëª» ì°¾ìœ¼ë©´ ("", ë§ˆì§€ë§‰ ê²€ì‚¬ ì›ë³¸ or "").
    """
    candidates: list[str] = []
    # 0) description CDATA ë‚´ í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìëª… ìš°ì„  ì‹œë„
    try:
        desc = getattr(entry, "description", None)
        if desc:
            # CDATA ì œê±° í›„ íƒœê·¸ ì œê±°
            text = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", str(desc), flags=re.DOTALL)
            text = re.sub(r"<[^>]+>", " ", text)
            text = re.sub(r"\s+", " ", text).strip()
            name = parse_reporter_from_author(text)
            if _is_valid_korean_name(name):
                return name, "description"
    except Exception:
        pass
    # 1) author
    if hasattr(entry, "author") and entry.author:
        candidates.append(str(entry.author))
    # 2) author_detail.name
    if hasattr(entry, "author_detail") and entry.author_detail:
        try:
            name_val = getattr(entry.author_detail, "name", None)
        except Exception:
            name_val = None
        if name_val:
            candidates.append(str(name_val))
    # 3) dc:creator (feedparserëŠ” dc:creatorë¥¼ dc_creatorë¡œ ë§¤í•‘í•˜ëŠ” ê²½ìš°ê°€ ìˆìŒ)
    dc_val = getattr(entry, "dc_creator", None) or getattr(entry, "creator", None)
    if dc_val:
        candidates.append(str(dc_val))
    # 4) authors ë¦¬ìŠ¤íŠ¸
    if hasattr(entry, "authors") and entry.authors:
        for a in entry.authors:
            # dict-like or object with name
            try:
                nm = a.get("name") if isinstance(a, dict) else getattr(a, "name", None)
            except Exception:
                nm = None
            if nm:
                candidates.append(str(nm))

    # ì¤‘ë³µ ì œê±°, ê³µë°± ì •ë¦¬
    seen = set()
    normalized = []
    for c in candidates:
        s = re.sub(r"\s+", " ", c).strip()
        if s and s not in seen:
            seen.add(s)
            normalized.append(s)

    last_src = ""
    for src in normalized:
        last_src = src
        name = parse_reporter_from_author(src)
        if _is_valid_korean_name(name):
            return name, src
    return "", last_src


def fetch_kmib_rss_to_csv(rss_url, output_file, max_articles=30):
    """êµ­ë¯¼ì¼ë³´ RSSë¥¼ íŒŒì‹±í•˜ì—¬ CSVë¡œ ì €ì¥"""

    print(f"êµ­ë¯¼ì¼ë³´ RSS í”¼ë“œ íŒŒì‹± ì¤‘: {rss_url}")

    # RSS íŒŒì‹±
    try:
        headers = {"User-Agent": get_random_user_agent()}
        response = requests.get(rss_url, headers=headers, timeout=10)
        # êµ­ë¯¼ì¼ë³´ RSSëŠ” EUC-KR ì¸ì½”ë”© ì‚¬ìš©
        response.encoding = "euc-kr"
        feed = feedparser.parse(response.content)
    except:
        feed = feedparser.parse(rss_url)

    if not feed.entries:
        print("âŒ RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"âœ… RSSì—ì„œ {len(feed.entries)}ê°œ ê¸°ì‚¬ ë°œê²¬")

    success_count = 0
    total_count = min(len(feed.entries), max_articles)

    # CSV íŒŒì¼ ìƒì„±
    with open(output_file, "w", newline="", encoding="utf-8-sig") as csvfile:
        fieldnames = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        print(f"ì´ {total_count}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘...\n")

        for i, entry in enumerate(feed.entries[:max_articles]):
            try:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = entry.title.strip()
                title = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", title)

                link = entry.link

                # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
                category = ""
                if hasattr(entry, "category"):
                    category = entry.category.strip()

                # RSS ìš”ì•½ ì •ë³´ ì¶”ì¶œ
                summary = ""
                if hasattr(entry, "description"):
                    summary = entry.description.strip()
                    # HTML íƒœê·¸ì™€ CDATA ì œê±°
                    summary = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", summary, flags=re.DOTALL)
                    summary = re.sub(r"<[^>]+>", "", summary)  # HTML íƒœê·¸ ì œê±°
                    summary = clean_kmib_content(summary)

                # RSSì—ì„œ ê¸°ìëª… ì¶”ì¶œ (ì—¬ëŸ¬ í•„ë“œ ê²€ì‚¬)
                rss_reporter, rss_src = get_rss_reporter(entry)
                if rss_reporter:
                    print(f"    ğŸ” RSS ê¸°ìëª…: '{rss_reporter}' â† {rss_src}")
                else:
                    if rss_src:
                        print(f"    âš  RSS ê¸°ìëª… íŒŒì‹± ì‹¤íŒ¨. ì›ë³¸: {rss_src}")
                    else:
                        print("    âš  RSSì— ê¸°ì ê´€ë ¨ í•„ë“œ ì—†ìŒ")

                # ë‚ ì§œ í˜•ì‹ ë³€í™˜
                if hasattr(entry, "published_parsed") and entry.published_parsed:
                    date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S")
                else:
                    date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                print(f"[{i+1}/{total_count}] {title[:60]}...")

                # ê¸°ì‚¬ ë³¸ë¬¸ ë° ê¸°ìëª… ì¶”ì¶œ (ì›¹)
                reporter, content = extract_kmib_article_content(link, summary)

                # ìµœì¢… ê¸°ìëª… ê²°ì •: RSS > ì›¹ > ë¯¸ìƒ (ìœ íš¨ì„± ê²€ì¦)
                final_reporter = (
                    rss_reporter
                    if _is_valid_korean_name(rss_reporter)
                    else (reporter if _is_valid_korean_name(reporter) else "ë¯¸ìƒ")
                )

                # ìµœì†Œ ì¡°ê±´ í™•ì¸
                if len(content.strip()) < 20:
                    print(f"    âš  ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ ê±´ë„ˆëœ€ (ê¸¸ì´: {len(content)})\n")
                    continue

                # CSVì— ì“°ê¸°
                writer.writerow(
                    {
                        "ì–¸ë¡ ì‚¬": "êµ­ë¯¼ì¼ë³´",
                        "ì œëª©": title,
                        "ë‚ ì§œ": date,
                        "ì¹´í…Œê³ ë¦¬": category if category else "ë¯¸ë¶„ë¥˜",
                        "ê¸°ìëª…": final_reporter,
                        "ë³¸ë¬¸": content,
                    }
                )

                success_count += 1
                print(f"    âœ… ì„±ê³µ! (ì¹´í…Œê³ ë¦¬: {category}, ê¸°ì: {final_reporter}, ë³¸ë¬¸: {len(content)}ì)")

                # ì§„í–‰ë¥  í‘œì‹œ
                if (i + 1) % 5 == 0:
                    print(f"\nğŸ“Š ì§„í–‰ë¥ : {i+1}/{total_count} ({(i+1)/total_count*100:.1f}%)")
                    print(f"ğŸ“ˆ ì„±ê³µë¥ : {success_count}/{i+1} ({success_count/(i+1)*100:.1f}%)\n")

                # ëœë¤ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                delay = random.uniform(1.0, 2.5)
                time.sleep(delay)

            except KeyboardInterrupt:
                print("\nâš  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"    âŒ ì˜¤ë¥˜: {e}")
                continue

        print(f"\n{'='*70}")
        print(f"ğŸ‰ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥: {output_file}")
        print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {success_count}/{total_count}ê°œ ì„±ê³µ ({success_count/total_count*100:.1f}%)")
        print(f"{'='*70}")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # êµ­ë¯¼ì¼ë³´ RSS URL ì˜µì…˜ë“¤ (ì „ì²´ê¸°ì‚¬, ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ, êµ­ì œ, ë¬¸í™”ë§Œ)
    kmib_rss_options = {
        "ì „ì²´ê¸°ì‚¬": "https://www.kmib.co.kr/rss/data/kmibRssAll.xml",
        "ì •ì¹˜": "https://www.kmib.co.kr/rss/data/kmibPolRss.xml",
        "ê²½ì œ": "https://www.kmib.co.kr/rss/data/kmibEcoRss.xml",
        "ì‚¬íšŒ": "https://www.kmib.co.kr/rss/data/kmibSocRss.xml",
        "êµ­ì œ": "https://www.kmib.co.kr/rss/data/kmibIntRss.xml",
        "ë¬¸í™”": "https://www.kmib.co.kr/rss/data/kmibCulRss.xml",
    }

    print("êµ­ë¯¼ì¼ë³´ RSS ìë™ ìˆ˜ì§‘ê¸°")
    print("=" * 50)
    print("ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ 20ê°œì”© ìë™ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    print(f"ìˆ˜ì§‘ ì¹´í…Œê³ ë¦¬: {', '.join(kmib_rss_options.keys())}")
    print("=" * 50)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    max_articles = 20
    output_file = f"results/êµ­ë¯¼ì¼ë³´_ì „ì²´_{timestamp}.csv"

    print(f"\nğŸ“ í†µí•© ì €ì¥ íŒŒì¼: {output_file}")
    print("-" * 50)

    # í•˜ë‚˜ì˜ CSV íŒŒì¼ì— ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë°ì´í„° ì €ì¥
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, "w", newline="", encoding="utf-8-sig") as csvfile:
        fieldnames = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        total_success = 0
        total_processed = 0

        # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìˆ˜ì§‘í•˜ì—¬ ê°™ì€ íŒŒì¼ì— ì¶”ê°€
        for category, rss_url in kmib_rss_options.items():
            print(f"\nğŸš€ {category} ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì‹œì‘!")
            print("-" * 30)

            # RSS íŒŒì‹±
            try:
                headers = {"User-Agent": get_random_user_agent()}
                response = requests.get(rss_url, headers=headers, timeout=10)
                response.encoding = "euc-kr"
                feed = feedparser.parse(response.content)
            except:
                feed = feedparser.parse(rss_url)

            if not feed.entries:
                print(f"âŒ {category} RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue

            print(f"âœ… {category}ì—ì„œ {len(feed.entries)}ê°œ ê¸°ì‚¬ ë°œê²¬")

            success_count = 0
            total_count = min(len(feed.entries), max_articles)

            for i, entry in enumerate(feed.entries[:max_articles]):
                try:
                    # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                    title = entry.title.strip()
                    title = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", title)

                    link = entry.link

                    # RSSì—ì„œ ê¸°ìëª… ì¶”ì¶œ (í—¬í¼ ì‚¬ìš©, ì—¬ëŸ¬ í•„ë“œ ê²€ì‚¬)
                    rss_reporter, rss_src = get_rss_reporter(entry)
                    if rss_reporter:
                        print(f"    âœ… RSSì—ì„œ ê¸°ìëª… ì¶”ì¶œ: '{rss_reporter}' â† {rss_src}")
                    else:
                        if rss_src:
                            print(f"    âš  RSS ê¸°ìëª… íŒŒì‹± ì‹¤íŒ¨. ì›ë³¸: {rss_src}")
                        else:
                            print("    âš  RSSì— ê¸°ì ê´€ë ¨ í•„ë“œ ì—†ìŒ")

                    # RSS ìš”ì•½ ì •ë³´ ì¶”ì¶œ
                    summary = ""
                    if hasattr(entry, "description"):
                        summary = entry.description.strip()
                        summary = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", summary, flags=re.DOTALL)
                        summary = re.sub(r"<[^>]+>", "", summary)
                        summary = clean_kmib_content(summary)

                    # ë‚ ì§œ í˜•ì‹ ë³€í™˜
                    if hasattr(entry, "published_parsed") and entry.published_parsed:
                        date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S")
                    else:
                        date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                    print(f"  [{i+1}/{total_count}] {title[:50]}...")

                    # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ì›¹ í¬ë¡¤ë§ì—ì„œ ê¸°ìëª…ë„ í•¨ê»˜ ì¶”ì¶œ)
                    web_reporter, content = extract_kmib_article_content(link, summary)

                    # ìµœì¢… ê¸°ìëª… ê²°ì •: RSS > ì›¹ > ë¯¸ìƒ (ìœ íš¨ì„± ê²€ì¦)
                    final_reporter = (
                        rss_reporter
                        if _is_valid_korean_name(rss_reporter)
                        else (web_reporter if _is_valid_korean_name(web_reporter) else "ë¯¸ìƒ")
                    )

                    print(f"    ğŸ“° ìµœì¢… ê¸°ìëª…: '{final_reporter}' (RSS: '{rss_reporter}', ì›¹: '{web_reporter}')")

                    # ìµœì†Œ ì¡°ê±´ í™•ì¸
                    if len(content.strip()) < 20:
                        print(f"    âš  ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ ê±´ë„ˆëœ€")
                        continue

                    # CSVì— ì“°ê¸°
                    writer.writerow(
                        {
                            "ì–¸ë¡ ì‚¬": "êµ­ë¯¼ì¼ë³´",
                            "ì œëª©": title,
                            "ë‚ ì§œ": date,
                            "ì¹´í…Œê³ ë¦¬": category,
                            "ê¸°ìëª…": final_reporter,
                            "ë³¸ë¬¸": content,
                        }
                    )

                    success_count += 1
                    print(f"    âœ… ì„±ê³µ! (ê¸°ì: {final_reporter})")

                    # ëœë¤ ë”œë ˆì´
                    delay = random.uniform(1.0, 2.5)
                    time.sleep(delay)

                except KeyboardInterrupt:
                    print("\nâš  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                    break
                except Exception as e:
                    print(f"    âŒ ì˜¤ë¥˜: {e}")
                    continue

            total_success += success_count
            total_processed += total_count
            print(f"âœ… {category} ì™„ë£Œ: {success_count}/{total_count}ê°œ ì„±ê³µ\n")

    print(f"\n{'='*70}")
    print(f"ğŸ‰ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ íŒŒì¼: {output_file}")
    print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {total_success}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
    print(f"{'='*70}")
