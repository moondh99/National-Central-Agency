import feedparser
import csv
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import time
import random
from urllib.parse import urlparse, parse_qs

def get_random_user_agent():
    """ëœë¤ User-Agent ë°˜í™˜"""
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15'
    ]
    return random.choice(user_agents)

def extract_google_news_content(url, rss_summary=""):
    """êµ¬ê¸€ë‰´ìŠ¤ì—ì„œ ì‹¤ì œ ê¸°ì‚¬ URL ì¶”ì¶œ ë° ë³¸ë¬¸ ìˆ˜ì§‘"""
    try:
        # êµ¬ê¸€ë‰´ìŠ¤ URLì—ì„œ ì‹¤ì œ ê¸°ì‚¬ URL ì¶”ì¶œ
        actual_url = extract_actual_url_from_google_news(url)
        if not actual_url:
            print(f"    âš  ì‹¤ì œ ê¸°ì‚¬ URL ì¶”ì¶œ ì‹¤íŒ¨")
            return "", rss_summary if rss_summary else "URL ì¶”ì¶œ ì‹¤íŒ¨"
        
        print(f"    ì‹¤ì œ URL: {actual_url[:80]}...")
        
        session = requests.Session()
        headers = {
            'User-Agent': get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'no-cache'
        }
        
        try:
            response = session.get(actual_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            if len(response.content) < 1000:  # 1KB ë¯¸ë§Œì´ë©´ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ
                print(f"    âš  ì‘ë‹µ í¬ê¸°ê°€ ì‘ìŒ (í¬ê¸°: {len(response.content)} bytes)")
                
        except Exception as e:
            print(f"    âš  ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return "", rss_summary if rss_summary else "ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨"
        
        soup = BeautifulSoup(response.content, 'html.parser')
        full_text = soup.get_text()
        
        # ì–¸ë¡ ì‚¬ë³„ ê¸°ìëª… ì¶”ì¶œ íŒ¨í„´
        reporter = extract_reporter_from_content(full_text, actual_url)
        
        # ë³¸ë¬¸ ì¶”ì¶œ - ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ êµ¬ì¡°ì— ëŒ€ì‘
        content = extract_article_content(soup, actual_url)
        
        # ë³¸ë¬¸ ì •ì œ
        content = clean_news_content(content, actual_url)
        
        # RSS ìš”ì•½ì´ ë” ì¢‹ìœ¼ë©´ RSS ìš”ì•½ ì‚¬ìš©
        if rss_summary and (len(content) < 100 or len(rss_summary) > len(content)):
            content = rss_summary
            print(f"    RSS ìš”ì•½ ì±„íƒ (ê¸¸ì´: {len(rss_summary)})")
        
        print(f"    ìµœì¢… ë³¸ë¬¸ ê¸¸ì´: {len(content)}")
        return reporter, content
        
    except Exception as e:
        print(f"    âŒ ì—ëŸ¬: {e}")
        return "", rss_summary if rss_summary else f"ì˜¤ë¥˜: {str(e)}"

def extract_actual_url_from_google_news(google_news_url):
    """êµ¬ê¸€ë‰´ìŠ¤ URLì—ì„œ ì‹¤ì œ ê¸°ì‚¬ URL ì¶”ì¶œ"""
    try:
        # êµ¬ê¸€ë‰´ìŠ¤ URL íŒ¨í„´ ë¶„ì„
        if 'news.google.co.kr' in google_news_url:
            # URL íŒŒë¼ë¯¸í„°ì—ì„œ ì‹¤ì œ URL ì¶”ì¶œ ì‹œë„
            parsed_url = urlparse(google_news_url)
            params = parse_qs(parsed_url.query)
            
            # url íŒŒë¼ë¯¸í„°ì—ì„œ ì¶”ì¶œ
            if 'url' in params:
                return params['url'][0]
            
            # êµ¬ê¸€ ë¦¬ë‹¤ì´ë ‰íŠ¸ ì„œë¹„ìŠ¤ë¥¼ í†µí•œ ì¶”ì¶œ
            session = requests.Session()
            headers = {'User-Agent': get_random_user_agent()}
            
            try:
                response = session.get(google_news_url, headers=headers, timeout=5, allow_redirects=True)
                return response.url
            except:
                pass
        
        # ì´ë¯¸ ì‹¤ì œ URLì¸ ê²½ìš°
        return google_news_url
        
    except Exception as e:
        print(f"    URL ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return google_news_url

def extract_reporter_from_content(full_text, url):
    """ì–¸ë¡ ì‚¬ë³„ ê¸°ìëª… ì¶”ì¶œ"""
    reporter = ""
    
    # ì–¸ë¡ ì‚¬ë³„ ë„ë©”ì¸ í™•ì¸
    domain = urlparse(url).netloc.lower()
    
    # ê¸°ë³¸ ê¸°ìëª… íŒ¨í„´ë“¤
    reporter_patterns = [
        r'([ê°€-í£]{2,4})\s*ê¸°ì\s*([a-zA-Z0-9_.+-]+@[a-zA-Z0-9.-]+)',  # ê¸°ìëª… ê¸°ì ì´ë©”ì¼
        r'([ê°€-í£]{2,4})\s*ê¸°ì',                                        # ê¸°ìëª… ê¸°ì
        r'ê¸°ì\s*([ê°€-í£]{2,4})',                                        # ê¸°ì ê¸°ìëª…
        r'([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›',                                      # ê¸°ìëª… íŠ¹íŒŒì›
        r'([ê°€-í£]{2,4})\s*í¸ì§‘ìœ„ì›',                                    # ê¸°ìëª… í¸ì§‘ìœ„ì›
        r'([ê°€-í£]{2,4})\s*íŒ€ì¥',                                        # ê¸°ìëª… íŒ€ì¥
        r'([ê°€-í£]{2,4})\s*ê¸°ì\s*=',                                    # ê¸°ìëª… ê¸°ì =
        r'ì·¨ì¬\s*([ê°€-í£]{2,4})',                                        # ì·¨ì¬ ê¸°ìëª…
        r'ê¸€\s*([ê°€-í£]{2,4})',                                          # ê¸€ ê¸°ìëª…
    ]
    
    # ê¸°ì‚¬ ë³¸ë¬¸ ë ë¶€ë¶„ì—ì„œ ê¸°ìëª… ì°¾ê¸°
    article_end = full_text[-1500:]  # ë§ˆì§€ë§‰ 1500ìì—ì„œ ì°¾ê¸°
    
    for pattern in reporter_patterns:
        match = re.search(pattern, article_end)
        if match:
            reporter = match.group(1)
            reporter = re.sub(r'ê¸°ì|íŠ¹íŒŒì›|í¸ì§‘ìœ„ì›|íŒ€ì¥|ì·¨ì¬|ê¸€', '', reporter).strip()
            if len(reporter) >= 2 and len(reporter) <= 4:
                break
    
    return reporter

def extract_article_content(soup, url):
    """ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ì˜ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
    content = ""
    domain = urlparse(url).netloc.lower()
    
    # ì–¸ë¡ ì‚¬ë³„ ë§ì¶¤ ì…€ë ‰í„°
    content_selectors = []
    
    if 'chosun.com' in domain:
        content_selectors = ['div.article-body', 'div[class*="article"]']
    elif 'donga.com' in domain:
        content_selectors = ['div.article_txt', 'div[class*="article"]']
    elif 'joongang.co.kr' in domain:
        content_selectors = ['div.article_body', 'div[class*="article"]']
    elif 'hankyung.com' in domain:
        content_selectors = ['div.article-body', 'div.wrap_cont']
    elif 'mk.co.kr' in domain:
        content_selectors = ['div.article_content', 'div[class*="article"]']
    elif 'seoul.co.kr' in domain:
        content_selectors = ['div.article', 'div[class*="content"]']
    elif 'hani.co.kr' in domain:
        content_selectors = ['div.article-text', 'div[class*="article"]']
    elif 'khan.co.kr' in domain:
        content_selectors = ['div.art_body', 'div[class*="article"]']
    elif 'yna.co.kr' in domain:
        content_selectors = ['div.article', 'div[class*="content"]']
    
    # ê³µí†µ ì…€ë ‰í„° ì¶”ê°€
    content_selectors.extend([
        'div[class*="article"]',
        'div[class*="content"]',
        'div[class*="news"]',
        'div[class*="text"]',
        'article',
        'main',
        'div[id*="article"]',
        'div.story-body',
        'div.entry-content'
    ])
    
    # ì…€ë ‰í„°ë¡œ ë³¸ë¬¸ ì°¾ê¸°
    for selector in content_selectors:
        elements = soup.select(selector)
        for element in elements:
            text = element.get_text().strip()
            if len(text) > len(content):
                content = text
    
    # P íƒœê·¸ ê¸°ë°˜ ì¶”ì¶œ (ë°±ì—… ë°©ë²•)
    if len(content) < 200:
        paragraphs = soup.find_all('p')
        content_parts = []
        
        for p in paragraphs:
            text = p.get_text().strip()
            if (len(text) > 20 and 
                not re.search(r'ì…ë ¥\s*\d{4}|ìˆ˜ì •\s*\d{4}|Copyright|ì €ì‘ê¶Œ|ë¬´ë‹¨.*ì „ì¬', text) and
                not text.startswith(('â–¶', 'â˜', 'â€»', 'â– ', 'â–²', '[', 'â—†', 'â—‹')) and
                '@' not in text or len([x for x in text if x == '@']) <= 1):
                content_parts.append(text)
        
        if content_parts:
            content = ' '.join(content_parts)
    
    return content

def clean_news_content(content, url):
    """ë‰´ìŠ¤ ë³¸ë¬¸ ì •ì œ"""
    if not content:
        return ""
    
    domain = urlparse(url).netloc.lower()
    
    # ê³µí†µ ì œê±° íŒ¨í„´
    remove_patterns = [
        r'ì…ë ¥\s*\d{4}[-./]\d{2}[-./]\d{2}.*?\d{2}:\d{2}',
        r'ìˆ˜ì •\s*\d{4}[-./]\d{2}[-./]\d{2}.*?\d{2}:\d{2}',
        r'ì—…ë°ì´íŠ¸\s*\d{4}[-./]\d{2}[-./]\d{2}.*?\d{2}:\d{2}',
        r'ë¬´ë‹¨.*ì „ì¬.*ê¸ˆì§€',
        r'ì¬ë°°í¬.*ê¸ˆì§€',
        r'ì €ì‘ê¶Œ.*ë¬´ë‹¨.*ì „ì¬',
        r'ê´€ë ¨ê¸°ì‚¬.*ë”ë³´ê¸°',
        r'í˜ì´ìŠ¤ë¶.*íŠ¸ìœ„í„°.*ì¹´ì¹´ì˜¤',
        r'êµ¬ë….*ì‹ ì²­',
        r'ê´‘ê³ ',
        r'ì—°í•©ë‰´ìŠ¤.*ì œê³µ',
        r'ë‰´ì‹œìŠ¤.*ì œê³µ',
        r'[ê°€-í£]{2,4}\s*ê¸°ì\s*[a-zA-Z0-9_.+-]+@[a-zA-Z0-9.-]+',  # ê¸°ì ì´ë©”ì¼ ì œê±°
        r'â“’.*ë‰´ìŠ¤',
        r'Copyright.*\d{4}',
        r'All rights reserved',
    ]
    
    # ì–¸ë¡ ì‚¬ë³„ íŠ¹ìˆ˜ íŒ¨í„´
    if 'chosun.com' in domain:
        remove_patterns.extend([r'ì¡°ì„ ì¼ë³´.*ë¬´ë‹¨.*ì „ì¬', r'chosun\.com'])
    elif 'donga.com' in domain:
        remove_patterns.extend([r'ë™ì•„ì¼ë³´.*ë¬´ë‹¨.*ì „ì¬', r'donga\.com'])
    elif 'joongang.co.kr' in domain:
        remove_patterns.extend([r'ì¤‘ì•™ì¼ë³´.*ë¬´ë‹¨.*ì „ì¬', r'joongang\.co\.kr'])
    elif 'hankyung.com' in domain:
        remove_patterns.extend([r'í•œêµ­ê²½ì œ.*ë¬´ë‹¨.*ì „ì¬', r'hankyung\.com', r'í•œê²½ë‹·ì»´'])
    
    for pattern in remove_patterns:
        content = re.sub(pattern, '', content, flags=re.IGNORECASE)
    
    # ê³µë°± ì •ë¦¬
    content = re.sub(r'\s+', ' ', content).strip()
    
    # ê¸¸ì´ ì œí•œ
    if len(content) > 2000:
        content = content[:2000] + "..."
    
    return content

def fetch_google_news_rss_to_csv(rss_url, output_file, max_articles=30):
    """êµ¬ê¸€ë‰´ìŠ¤ RSSë¥¼ íŒŒì‹±í•˜ì—¬ CSVë¡œ ì €ì¥"""
    
    print(f"êµ¬ê¸€ë‰´ìŠ¤ RSS í”¼ë“œ íŒŒì‹± ì¤‘: {rss_url}")
    
    # RSS íŒŒì‹±
    try:
        headers = {'User-Agent': get_random_user_agent()}
        response = requests.get(rss_url, headers=headers, timeout=15)
        response.encoding = 'utf-8'
        feed = feedparser.parse(response.content)
    except:
        feed = feedparser.parse(rss_url)
    
    if not feed.entries:
        print("âŒ RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… RSSì—ì„œ {len(feed.entries)}ê°œ ê¸°ì‚¬ ë°œê²¬")
    
    success_count = 0
    total_count = min(len(feed.entries), max_articles)
    
    # CSV íŒŒì¼ ìƒì„±
    with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = ['ì œëª©', 'ë‚ ì§œ', 'ì–¸ë¡ ì‚¬', 'ê¸°ìëª…', 'ë³¸ë¬¸', 'ì›ë³¸URL']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        print(f"ì´ {total_count}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘...\n")
        
        for i, entry in enumerate(feed.entries[:max_articles]):
            try:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = entry.title.strip()
                title = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', title)
                
                link = entry.link
                
                # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
                source = ""
                if hasattr(entry, 'source') and hasattr(entry.source, 'title'):
                    source = entry.source.title.strip()
                
                # RSS ìš”ì•½ ì •ë³´ ì¶”ì¶œ
                summary = ""
                if hasattr(entry, 'description'):
                    summary = entry.description.strip()
                    summary = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', summary, flags=re.DOTALL)
                    summary = re.sub(r'<[^>]+>', '', summary)  # HTML íƒœê·¸ ì œê±°
                    summary = clean_news_content(summary, link)
                
                # ë‚ ì§œ í˜•ì‹ ë³€í™˜
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d %H:%M:%S')
                else:
                    date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
                print(f"[{i+1}/{total_count}] {title[:50]}... (ì¶œì²˜: {source})")
                
                # ê¸°ì‚¬ ë³¸ë¬¸ ë° ê¸°ìëª… ì¶”ì¶œ
                reporter, content = extract_google_news_content(link, summary)
                
                # ì‹¤ì œ URL ì¶”ì¶œ
                actual_url = extract_actual_url_from_google_news(link)
                
                # ìµœì†Œ ì¡°ê±´ í™•ì¸
                if len(content.strip()) < 30:
                    print(f"    âš  ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ ê±´ë„ˆëœ€ (ê¸¸ì´: {len(content)})\n")
                    continue
                
                # CSVì— ì“°ê¸°
                writer.writerow({
                    'ì œëª©': title,
                    'ë‚ ì§œ': date,
                    'ì–¸ë¡ ì‚¬': source if source else "ë¯¸ìƒ",
                    'ê¸°ìëª…': reporter if reporter else "ë¯¸ìƒ",
                    'ë³¸ë¬¸': content,
                    'ì›ë³¸URL': actual_url if actual_url else link
                })
                
                success_count += 1
                print(f"    âœ… ì„±ê³µ! (ì–¸ë¡ ì‚¬: {source}, ê¸°ì: {reporter if reporter else 'ë¯¸ìƒ'}, ë³¸ë¬¸: {len(content)}ì)")
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if (i + 1) % 3 == 0:
                    print(f"\nğŸ“Š ì§„í–‰ë¥ : {i+1}/{total_count} ({(i+1)/total_count*100:.1f}%)")
                    print(f"ğŸ“ˆ ì„±ê³µë¥ : {success_count}/{i+1} ({success_count/(i+1)*100:.1f}%)\n")
                
                # ëœë¤ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€) - êµ¬ê¸€ë‰´ìŠ¤ëŠ” ë” ê¸´ ë”œë ˆì´ í•„ìš”
                delay = random.uniform(2.0, 4.0)
                time.sleep(delay)
                
            except KeyboardInterrupt:
                print("\nâš  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"    âŒ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"\n{'='*70}")
        print(f"ğŸ‰ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥: {output_file}")
        print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {success_count}/{total_count}ê°œ ì„±ê³µ ({success_count/total_count*100:.1f}%)")
        print(f"{'='*70}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # êµ¬ê¸€ë‰´ìŠ¤ RSS URL ì˜µì…˜ë“¤ (ì²¨ë¶€ëœ ì´ë¯¸ì§€ ê¸°ë°˜)
    google_news_rss_options = {
        "ì „ì²´ë‰´ìŠ¤": "http://news.google.co.kr/news?pz=1&cf=all&ned=kr&hl=ko&output=rss",
        "ì£¼ìš”ë‰´ìŠ¤": "http://news.google.co.kr/news?pz=1&cf=all&ned=kr&hl=ko&topic=h&output=rss",
        "ì •ì¹˜": "http://news.google.co.kr/news?pz=1&cf=all&ned=kr&hl=ko&topic=p&output=rss",
        "ê²½ì œ": "http://news.google.co.kr/news?pz=1&cf=all&ned=kr&hl=ko&topic=b&output=rss",
        "ì‚¬íšŒ": "http://news.google.co.kr/news?pz=1&cf=all&ned=kr&hl=ko&topic=y&output=rss",
        "ë¬¸í™”/ìƒí™œ": "http://news.google.co.kr/news?pz=1&cf=all&ned=kr&hl=ko&topic=l&output=rss",
        "êµ­ì œ": "http://news.google.co.kr/news?pz=1&cf=all&ned=kr&hl=ko&topic=w&output=rss",
        "ì •ë³´ê³¼í•™": "http://news.google.co.kr/news?pz=1&cf=all&ned=kr&hl=ko&topic=t&output=rss",
        "ê±´ê°•": "http://news.google.co.kr/news?pz=1&cf=all&ned=kr&hl=ko&topic=m&output=rss",
        "ìŠ¤í¬ì¸ ": "http://news.google.co.kr/news?pz=1&cf=all&ned=kr&hl=ko&topic=s&output=rss"
    }
    
    # ì›í•˜ëŠ” ì¹´í…Œê³ ë¦¬ ì„ íƒ
    print("êµ¬ê¸€ë‰´ìŠ¤ RSS ìˆ˜ì§‘ê¸°")
    print("="*50)
    for key in google_news_rss_options.keys():
        print(f"- {key}")
    
    # ì¹´í…Œê³ ë¦¬ ì…ë ¥ ë°›ê¸°
    selected_category = input("\nìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: ì „ì²´ë‰´ìŠ¤): ").strip()
    if not selected_category or selected_category not in google_news_rss_options:
        selected_category = "ì „ì²´ë‰´ìŠ¤"
    
    # ê¸°ì‚¬ ìˆ˜ ì…ë ¥ ë°›ê¸°
    try:
        max_articles = int(input("ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 15): ").strip() or "15")
    except:
        max_articles = 15
    
    selected_rss = google_news_rss_options[selected_category]
    timestamp = datetime.now().strftime('%Y%m%d_%H%M')
    output_file = f"results/google_news_{selected_category}_{timestamp}.csv"
    
    print(f"\nğŸš€ {selected_category} ì¹´í…Œê³ ë¦¬ì—ì„œ {max_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘!")
    print(f"ğŸ“ ì €ì¥ íŒŒì¼: {output_file}\n")
    
    # ì‹¤í–‰
    fetch_google_news_rss_to_csv(selected_rss, output_file, max_articles)
