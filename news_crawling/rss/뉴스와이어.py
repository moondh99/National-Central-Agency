import feedparser
import csv
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import time
import random

def get_random_user_agent():
    """ëœë¤ User-Agent ë°˜í™˜"""
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15'
    ]
    return random.choice(user_agents)

def extract_newswire_article_content(url, rss_summary=""):
    """ë‰´ìŠ¤ì™€ì´ì–´ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ê³¼ ì‘ì„±ìëª…ì„ ì¶”ì¶œ"""
    try:
        session = requests.Session()
        
        headers = {
            'User-Agent': get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://api.newswire.co.kr/',
            'Cache-Control': 'no-cache'
        }
        
        print(f"    ì ‘ì† ì‹œë„: {url[:80]}...")
        
        # ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ í›„ ì‹¤ì œ ê¸°ì‚¬ ì ‘ê·¼
        try:
            session.get('https://api.newswire.co.kr/', headers=headers, timeout=5)
            time.sleep(0.5)
            
            response = session.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            if len(response.content) < 2000:  # 2KB ë¯¸ë§Œì´ë©´ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ
                print(f"    âš  ì‘ë‹µ í¬ê¸°ê°€ ì‘ìŒ (í¬ê¸°: {len(response.content)} bytes)")
                
        except Exception as e:
            print(f"    âš  ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return "", rss_summary if rss_summary else "ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨"
        
        soup = BeautifulSoup(response.content, 'html.parser')
        full_text = soup.get_text()
        
        # ì‘ì„±ìëª… ì¶”ì¶œ - ë‰´ìŠ¤ì™€ì´ì–´ íŒ¨í„´ì— ë§ê²Œ ìˆ˜ì • (ë³´ë„ìë£Œ íŠ¹ì„±ìƒ ê¸°ìë³´ë‹¤ëŠ” ë‹´ë‹¹ì)
        reporter = ""
        reporter_patterns = [
            r'([ê°€-í£]{2,4})\s*ê¸°ì\s*([a-zA-Z0-9_.+-]+@[a-zA-Z0-9.-]+)',          # ê¸°ìëª… ê¸°ì ì´ë©”ì¼
            r'([ê°€-í£]{2,4})\s*ê¸°ì',                                              # ê¸°ìëª… ê¸°ì
            r'([ê°€-í£]{2,4})\s*ë‹´ë‹¹ì',                                            # ë‹´ë‹¹ìëª… ë‹´ë‹¹ì
            r'([ê°€-í£]{2,4})\s*ëŒ€í‘œ',                                              # ëŒ€í‘œëª… ëŒ€í‘œ
            r'([ê°€-í£]{2,4})\s*ë§¤ë‹ˆì €',                                            # ë§¤ë‹ˆì €ëª… ë§¤ë‹ˆì €
            r'([ê°€-í£]{2,4})\s*íŒ€ì¥',                                              # íŒ€ì¥ëª… íŒ€ì¥
            r'([ê°€-í£]{2,4})\s*ì‹¤ì¥',                                              # ì‹¤ì¥ëª… ì‹¤ì¥
            r'([ê°€-í£]{2,4})\s*ë¶€ì¥',                                              # ë¶€ì¥ëª… ë¶€ì¥
            r'([ê°€-í£]{2,4})\s*ê³¼ì¥',                                              # ê³¼ì¥ëª… ê³¼ì¥
            r'([ê°€-í£]{2,4})\s*ì°¨ì¥',                                              # ì°¨ì¥ëª… ì°¨ì¥
            r'ë¬¸ì˜\s*:\s*([ê°€-í£]{2,4})',                                          # ë¬¸ì˜: ë‹´ë‹¹ìëª…
            r'ì—°ë½ì²˜\s*:\s*([ê°€-í£]{2,4})',                                        # ì—°ë½ì²˜: ë‹´ë‹¹ìëª…
            r'Contact\s*:\s*([ê°€-í£]{2,4})',                                      # Contact: ë‹´ë‹¹ìëª…
        ]
        
        # ê¸°ì‚¬ ë³¸ë¬¸ ë ë¶€ë¶„ì—ì„œ ë‹´ë‹¹ìëª…ì„ ì°¾ëŠ” ê²ƒì´ ë” ì •í™•
        article_end = full_text[-1500:]  # ë§ˆì§€ë§‰ 1500ìì—ì„œ ì°¾ê¸°
        
        for pattern in reporter_patterns:
            match = re.search(pattern, article_end)
            if match:
                reporter = match.group(1)
                reporter = re.sub(r'ê¸°ì|ë‹´ë‹¹ì|ëŒ€í‘œ|ë§¤ë‹ˆì €|íŒ€ì¥|ì‹¤ì¥|ë¶€ì¥|ê³¼ì¥|ì°¨ì¥|ë¬¸ì˜|ì—°ë½ì²˜|Contact', '', reporter).strip()
                if len(reporter) >= 2 and len(reporter) <= 4:
                    break
        
        # ë³¸ë¬¸ ì¶”ì¶œ - ë‰´ìŠ¤ì™€ì´ì–´ HTML êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
        content = ""
        
        # ë°©ë²• 1: ë‰´ìŠ¤ì™€ì´ì–´ ê¸°ì‚¬ ë³¸ë¬¸ êµ¬ì¡° ì°¾ê¸°
        content_selectors = [
            'div.article_content',       # ë‰´ìŠ¤ì™€ì´ì–´ ì£¼ìš” ê¸°ì‚¬ ë³¸ë¬¸ í´ë˜ìŠ¤
            'div[class*="article"]',     # article ê´€ë ¨ í´ë˜ìŠ¤
            'div[class*="content"]',     # content ê´€ë ¨ í´ë˜ìŠ¤
            'div[class*="news"]',        # news ê´€ë ¨ í´ë˜ìŠ¤
            'div[class*="text"]',        # text ê´€ë ¨ í´ë˜ìŠ¤
            'div.news_content',          # ë‰´ìŠ¤ ì»¨í…ì¸ 
            'div.view_content',          # ë·° ì»¨í…ì¸ 
            'article',                   # article íƒœê·¸
            'main',                      # main íƒœê·¸
            'div[id*="article"]',        # article ID ê´€ë ¨
            'div.bodycontent',           # ë°”ë”” ì»¨í…ì¸ 
            'div.story',                 # ìŠ¤í† ë¦¬ ì»¨í…ì¸ 
            'div.article_body',          # ê¸°ì‚¬ ë³¸ë¬¸
            'div.articleView',           # ê¸°ì‚¬ ë·°
            'div.press_content',         # ë³´ë„ìë£Œ ì»¨í…ì¸ 
            'div.release_content',       # ë¦´ë¦¬ì¦ˆ ì»¨í…ì¸ 
        ]
        
        for selector in content_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text().strip()
                if len(text) > len(content):
                    content = text
        
        # ë°©ë²• 2: P íƒœê·¸ ê¸°ë°˜ ì¶”ì¶œ (ë‰´ìŠ¤ì™€ì´ì–´ íŠ¹ì„±ì— ë§ê²Œ ìˆ˜ì •)
        if len(content) < 200:
            paragraphs = soup.find_all('p')
            content_parts = []
            
            for p in paragraphs:
                text = p.get_text().strip()
                if (len(text) > 20 and 
                    not re.search(r'ì…ë ¥\s*\d{4}|ìˆ˜ì •\s*\d{4}|Copyright|ì €ì‘ê¶Œ|ë‰´ìŠ¤ì™€ì´ì–´|newswire', text) and
                    not text.startswith(('â–¶', 'â˜', 'â€»', 'â– ', 'â–²', '[', 'â€»', 'â—†', 'â—‹', 'â–³')) and
                    'ë¬´ë‹¨ ì „ì¬' not in text and
                    'ì¬ë°°í¬ ê¸ˆì§€' not in text and
                    'ê¸°ì‚¬ì œë³´' not in text and
                    'ë¬¸ì˜ì‚¬í•­' not in text):
                    content_parts.append(text)
            
            if content_parts:
                content = ' '.join(content_parts)
        
        # ë³¸ë¬¸ ì •ì œ
        content = clean_newswire_content(content)
        
        # RSS ìš”ì•½ì´ ë” ì¢‹ìœ¼ë©´ RSS ìš”ì•½ ì‚¬ìš©
        if rss_summary and (len(content) < 100 or len(rss_summary) > len(content)):
            content = rss_summary
            print(f"    RSS ìš”ì•½ ì±„íƒ (ê¸¸ì´: {len(rss_summary)})")
        
        print(f"    ìµœì¢… ë³¸ë¬¸ ê¸¸ì´: {len(content)}")
        return reporter, content
        
    except Exception as e:
        print(f"    âŒ ì—ëŸ¬: {e}")
        return "", rss_summary if rss_summary else f"ì˜¤ë¥˜: {str(e)}"

def clean_newswire_content(content):
    """ë‰´ìŠ¤ì™€ì´ì–´ ê¸°ì‚¬ ë³¸ë¬¸ ì •ì œ"""
    if not content:
        return ""
    
    # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ë“¤ ì œê±° - ë‰´ìŠ¤ì™€ì´ì–´ íŠ¹ì„±ì— ë§ê²Œ ìˆ˜ì •
    remove_patterns = [
        r'ì…ë ¥\s*\d{4}[-./]\d{2}[-./]\d{2}.*?\d{2}:\d{2}',
        r'ìˆ˜ì •\s*\d{4}[-./]\d{2}[-./]\d{2}.*?\d{2}:\d{2}',
        r'ì—…ë°ì´íŠ¸\s*\d{4}[-./]\d{2}[-./]\d{2}.*?\d{2}:\d{2}',
        r'ë‰´ìŠ¤ì™€ì´ì–´.*ë¬´ë‹¨.*ì „ì¬.*ê¸ˆì§€',
        r'ë¬´ë‹¨.*ì „ì¬.*ì¬ë°°í¬.*ê¸ˆì§€',
        r'ì €ì‘ê¶Œ.*ë‰´ìŠ¤ì™€ì´ì–´',
        r'ê´€ë ¨ê¸°ì‚¬.*ë”ë³´ê¸°',
        r'í˜ì´ìŠ¤ë¶.*íŠ¸ìœ„í„°.*ì¹´ì¹´ì˜¤',
        r'êµ¬ë….*ì‹ ì²­',
        r'ê´‘ê³ ',
        r'ë³´ë„ìë£Œ.*ë¬¸ì˜',
        r'ê¸°ì‚¬.*ë¬¸ì˜',
        r'newswire\.co\.kr',
        r'â“’.*ë‰´ìŠ¤ì™€ì´ì–´',
        r'Newswire',
        r'NEWS.*WIRE',
        r'ë¬¸ì˜ì‚¬í•­.*ì—°ë½ì²˜',
        r'í™ˆí˜ì´ì§€.*ë°”ë¡œê°€ê¸°',
        r'Press.*Release',
        r'ë³´ë„ìë£Œ.*ë',
        r'ì´ìƒ.*ë',
        r'\*\*\*.*ë.*\*\*\*',
        r'---.*ë.*---',
        r'Copyright.*\d{4}.*ë‰´ìŠ¤ì™€ì´ì–´',
    ]
    
    for pattern in remove_patterns:
        content = re.sub(pattern, '', content, flags=re.IGNORECASE)
    
    # ê³µë°± ì •ë¦¬
    content = re.sub(r'\s+', ' ', content).strip()
    
    # ê¸¸ì´ ì œí•œ
    if len(content) > 2000:
        content = content[:2000] + "..."
    
    return content

def fetch_newswire_rss_to_csv(rss_url, output_file, max_articles=30):
    """ë‰´ìŠ¤ì™€ì´ì–´ RSSë¥¼ íŒŒì‹±í•˜ì—¬ CSVë¡œ ì €ì¥"""
    
    print(f"ë‰´ìŠ¤ì™€ì´ì–´ RSS í”¼ë“œ íŒŒì‹± ì¤‘: {rss_url}")
    
    # RSS íŒŒì‹±
    try:
        headers = {'User-Agent': get_random_user_agent()}
        response = requests.get(rss_url, headers=headers, timeout=10)
        # ë‰´ìŠ¤ì™€ì´ì–´ RSSëŠ” UTF-8 ì¸ì½”ë”© ì‚¬ìš©
        response.encoding = 'utf-8'
        feed = feedparser.parse(response.content)
    except:
        feed = feedparser.parse(rss_url)
    
    if not feed.entries:
        print("âŒ RSS í”¼ë“œì—ì„œ ë³´ë„ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… RSSì—ì„œ {len(feed.entries)}ê°œ ë³´ë„ìë£Œ ë°œê²¬")
    
    success_count = 0
    total_count = min(len(feed.entries), max_articles)
    
    # CSV íŒŒì¼ ìƒì„±
    with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = ['ì œëª©', 'ë‚ ì§œ', 'ì¹´í…Œê³ ë¦¬', 'ë‹´ë‹¹ìëª…', 'ë³¸ë¬¸']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        print(f"ì´ {total_count}ê°œ ë³´ë„ìë£Œ ì²˜ë¦¬ ì‹œì‘...\n")
        
        for i, entry in enumerate(feed.entries[:max_articles]):
            try:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = entry.title.strip()
                title = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', title)
                
                link = entry.link
                
                # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ë‰´ìŠ¤ì™€ì´ì–´ RSS êµ¬ì¡°ì— ë§ê²Œ)
                category = ""
                if hasattr(entry, 'category'):
                    category = entry.category.strip()
                elif hasattr(entry, 'tags') and entry.tags:
                    category = entry.tags[0].term if entry.tags else ""
                
                # URLì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì‹œë„ (ë‰´ìŠ¤ì™€ì´ì–´ URL êµ¬ì¡° ê¸°ë°˜)
                if not category:
                    url_category_map = {
                        # ì „ì²´ ë° ì¸ê¸°
                        'rss/all': 'ì „ì²´',
                        
                        # ì‚°ì—…ë³„
                        'industry/600': 'ê¸°ìˆ ',
                        'industry/400': 'ì‚°ì—…',
                        'industry/1200': 'í—¬ìŠ¤',
                        'industry/900': 'ìƒí™œ',
                        'industry/300': 'ìë™ì°¨',
                        'industry/100': 'ê²½ì œ',
                        'industry/200': 'ê¸ˆìœµ',
                        'industry/800': 'ë¬¸í™”',
                        'industry/1300': 'ë ˆì €',
                        'industry/1100': 'êµìœ¡',
                        'industry/1900': 'ì‚¬íšŒ',
                        'industry/1500': 'í™˜ê²½',
                        'industry/1400': 'ì •ì¹˜',
                        
                        # ì˜ë¬¸ ë‰´ìŠ¤
                        'english': 'English News',
                        
                        # ì§€ì—­ë³„
                        'region/1': 'ì¸ì²œê²½ê¸°',
                        'region/2': 'ëŒ€ì „ì¶©ì²­',
                        'region/3': 'ê´‘ì£¼ì „ë¼',
                        'region/4': 'ëŒ€êµ¬ê²½ë¶',
                        'region/5': 'ë¶€ì‚°ìš¸ì‚°ê²½ë‚¨',
                        'region/6': 'ê°•ì›',
                        'region/7': 'ê°•ì›',
                        'region/8': 'ì¶©ë¶',
                        'region/9': 'ì „ë¶',
                        'region/10': 'ì œì£¼',
                        'region/11': 'í•´ì™¸',
                        'region/123': 'ì •ì¹˜',
                    }
                    
                    for url_part, cat_name in url_category_map.items():
                        if url_part in rss_url:
                            category = cat_name
                            break
                
                # RSS ìš”ì•½ ì •ë³´ ì¶”ì¶œ
                summary = ""
                if hasattr(entry, 'description'):
                    summary = entry.description.strip()
                    # HTML íƒœê·¸ì™€ CDATA ì œê±°
                    summary = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', summary, flags=re.DOTALL)
                    summary = re.sub(r'<[^>]+>', '', summary)  # HTML íƒœê·¸ ì œê±°
                    summary = clean_newswire_content(summary)
                
                # ë‚ ì§œ í˜•ì‹ ë³€í™˜
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    date = datetime(*entry.published_parsed[:6]).strftime('%Y-%m-%d %H:%M:%S')
                else:
                    date = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                
                print(f"[{i+1}/{total_count}] {title[:50]}...")
                
                # ê¸°ì‚¬ ë³¸ë¬¸ ë° ë‹´ë‹¹ìëª… ì¶”ì¶œ
                reporter, content = extract_newswire_article_content(link, summary)
                
                # ìµœì†Œ ì¡°ê±´ í™•ì¸
                if len(content.strip()) < 30:
                    print(f"    âš  ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ ê±´ë„ˆëœ€ (ê¸¸ì´: {len(content)})\n")
                    continue
                
                # CSVì— ì“°ê¸°
                writer.writerow({
                    'ì œëª©': title,
                    'ë‚ ì§œ': date,
                    'ì¹´í…Œê³ ë¦¬': category if category else "ë¯¸ë¶„ë¥˜",
                    'ë‹´ë‹¹ìëª…': reporter if reporter else "ë¯¸ìƒ",
                    'ë³¸ë¬¸': content
                })
                
                success_count += 1
                print(f"    âœ… ì„±ê³µ! (ì¹´í…Œê³ ë¦¬: {category}, ë‹´ë‹¹ì: {reporter if reporter else 'ë¯¸ìƒ'}, ë³¸ë¬¸: {len(content)}ì)")
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if (i + 1) % 5 == 0:
                    print(f"\nğŸ“Š ì§„í–‰ë¥ : {i+1}/{total_count} ({(i+1)/total_count*100:.1f}%)")
                    print(f"ğŸ“ˆ ì„±ê³µë¥ : {success_count}/{i+1} ({success_count/(i+1)*100:.1f}%)\n")
                
                # ëœë¤ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                delay = random.uniform(1.5, 3.0)
                time.sleep(delay)
                
            except KeyboardInterrupt:
                print("\nâš  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"    âŒ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"\n{'='*70}")
        print(f"ğŸ‰ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥: {output_file}")
        print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {success_count}/{total_count}ê°œ ì„±ê³µ ({success_count/total_count*100:.1f}%)")
        print(f"{'='*70}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ë‰´ìŠ¤ì™€ì´ì–´ RSS URL ì˜µì…˜ë“¤ (ì²¨ë¶€ëœ ì´ë¯¸ì§€ ê¸°ë°˜)
    newswire_rss_options = {
        # ì „ì²´
        "ì „ì²´": "https://api.newswire.co.kr/rss/all",
        
        # ì‚°ì—…ë³„
        "ê¸°ìˆ ": "https://api.newswire.co.kr/rss/industry/600",
        "ì‚°ì—…": "https://api.newswire.co.kr/rss/industry/400",
        "í—¬ìŠ¤": "https://api.newswire.co.kr/rss/industry/1200",
        "ìƒí™œ": "https://api.newswire.co.kr/rss/industry/900",
        "ìë™ì°¨": "https://api.newswire.co.kr/rss/industry/300",
        "ê²½ì œ": "https://api.newswire.co.kr/rss/industry/100",
        "ê¸ˆìœµ": "https://api.newswire.co.kr/rss/industry/200",
        "ë¬¸í™”": "https://api.newswire.co.kr/rss/industry/800",
        "ë ˆì €": "https://api.newswire.co.kr/rss/industry/1300",
        "êµìœ¡": "https://api.newswire.co.kr/rss/industry/1100",
        "ì‚¬íšŒ": "https://api.newswire.co.kr/rss/industry/1900",
        "í™˜ê²½": "https://api.newswire.co.kr/rss/industry/1500",
        "ì •ì¹˜": "https://api.newswire.co.kr/rss/industry/1400",
        
        # ì˜ë¬¸ ë‰´ìŠ¤
        "English News": "https://api.newswire.co.kr/rss/english",
        
        # ì§€ì—­ë³„ (ì£¼ìš” ì§€ì—­ë§Œ)
        "ì¸ì²œê²½ê¸°": "https://api.newswire.co.kr/rss/region/1",
        "ëŒ€ì „ì¶©ì²­": "https://api.newswire.co.kr/rss/region/2",
        "ê´‘ì£¼ì „ë¼": "https://api.newswire.co.kr/rss/region/3",
        "ëŒ€êµ¬ê²½ë¶": "https://api.newswire.co.kr/rss/region/4",
        "ë¶€ì‚°ìš¸ì‚°ê²½ë‚¨": "https://api.newswire.co.kr/rss/region/5",
        "ê°•ì›": "https://api.newswire.co.kr/rss/region/7",
        "ê°•ì›": "https://api.newswire.co.kr/rss/region/6",
        "ì¶©ë¶": "https://api.newswire.co.kr/rss/region/8",
        "ì „ë¶": "https://api.newswire.co.kr/rss/region/9",
        "ì œì£¼": "https://api.newswire.co.kr/rss/region/10",
        "í•´ì™¸": "https://api.newswire.co.kr/rss/region/11"
    }
    
    # ì›í•˜ëŠ” ì¹´í…Œê³ ë¦¬ ì„ íƒ
    print("ë‰´ìŠ¤ì™€ì´ì–´ RSS ìˆ˜ì§‘ê¸° (ë³´ë„ìë£Œ ì „ë¬¸)")
    print("="*60)
    print("ì£¼ìš” ì‚°ì—…ë³„:")
    main_industries = ["ì „ì²´", "ê¸°ìˆ ", "ê²½ì œ", "ì‚°ì—…", "í—¬ìŠ¤", "ìƒí™œ"]
    for key in main_industries:
        if key in newswire_rss_options:
            print(f"- {key}")
    
    print("\nì „ì²´ ì¹´í…Œê³ ë¦¬:")
    for key in newswire_rss_options.keys():
        print(f"- {key}")
    
    # ì¹´í…Œê³ ë¦¬ ì…ë ¥ ë°›ê¸°
    selected_category = input("\nìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: ì „ì²´): ").strip()
    if not selected_category or selected_category not in newswire_rss_options:
        selected_category = "ì „ì²´"
    
    # ê¸°ì‚¬ ìˆ˜ ì…ë ¥ ë°›ê¸°
    try:
        max_articles = int(input("ìˆ˜ì§‘í•  ë³´ë„ìë£Œ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 20): ").strip() or "20")
    except:
        max_articles = 20
    
    selected_rss = newswire_rss_options[selected_category]
    timestamp = datetime.now().strftime('%Y%m%d_%H%M')
    output_file = f"results/newswire_{selected_category}_{timestamp}.csv"
    
    print(f"\nğŸš€ {selected_category} ì¹´í…Œê³ ë¦¬ì—ì„œ {max_articles}ê°œ ë³´ë„ìë£Œ ìˆ˜ì§‘ ì‹œì‘!")
    print(f"ğŸ“ ì €ì¥ íŒŒì¼: {output_file}\n")
    
    # ì‹¤í–‰
    fetch_newswire_rss_to_csv(selected_rss, output_file, max_articles)
