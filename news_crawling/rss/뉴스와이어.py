import feedparser
import csv
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import time
import random
import os
from lxml import html


def get_random_user_agent():
    """ëœë¤ User-Agent ë°˜í™˜"""
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    ]
    return random.choice(user_agents)


def extract_newswire_article_content(url, rss_summary=""):
    """ë‰´ìŠ¤ì™€ì´ì–´ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ê³¼ ì‘ì„±ìëª…ì„ ì¶”ì¶œ"""
    try:
        session = requests.Session()

        headers = {
            "User-Agent": get_random_user_agent(),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            # br ì œê±°: requestsëŠ” ê¸°ë³¸ì ìœ¼ë¡œ brotlië¥¼ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Referer": "https://api.newswire.co.kr/",
            "Cache-Control": "no-cache",
        }

        print(f"    ì ‘ì† ì‹œë„: {url[:80]}...")

        # ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ í›„ ì‹¤ì œ ê¸°ì‚¬ ì ‘ê·¼
        try:
            session.get("https://api.newswire.co.kr/", headers=headers, timeout=5)
            time.sleep(0.5)

            response = session.get(url, headers=headers, timeout=15)
            response.raise_for_status()

            # ì¸ì½”ë”© ë³´ì •
            try:
                if not response.encoding:
                    response.encoding = response.apparent_encoding
            except Exception:
                pass

            if len(response.content) < 2000:  # 2KB ë¯¸ë§Œì´ë©´ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ
                print(f"    âš  ì‘ë‹µ í¬ê¸°ê°€ ì‘ìŒ (í¬ê¸°: {len(response.content)} bytes)")

        except Exception as e:
            print(f"    âš  ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return "", rss_summary if rss_summary else "ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨"

        # BeautifulSoup ì¤€ë¹„ ë° ì „ì²´ í…ìŠ¤íŠ¸ í™•ë³´
        soup = BeautifulSoup(response.content, "html.parser")
        full_text = soup.get_text()

        # ì‘ì„±ìëª… ì¶”ì¶œ - ë³´ë„ìë£Œ íŠ¹ì„± ê³ ë ¤
        reporter = ""
        reporter_patterns = [
            r"([ê°€-í£]{2,4})\s*ê¸°ì\s*([a-zA-Z0-9_.+-]+@[a-zA-Z0-9.-]+)",  # ê¸°ìëª… ê¸°ì ì´ë©”ì¼
            r"([ê°€-í£]{2,4})\s*ê¸°ì",  # ê¸°ìëª… ê¸°ì
            r"([ê°€-í£]{2,4})\s*ë‹´ë‹¹ì",  # ë‹´ë‹¹ìëª… ë‹´ë‹¹ì
            r"([ê°€-í£]{2,4})\s*ëŒ€í‘œ",  # ëŒ€í‘œëª… ëŒ€í‘œ
            r"([ê°€-í£]{2,4})\s*ë§¤ë‹ˆì €",  # ë§¤ë‹ˆì €ëª… ë§¤ë‹ˆì €
            r"([ê°€-í£]{2,4})\s*íŒ€ì¥",  # íŒ€ì¥ëª… íŒ€ì¥
            r"([ê°€-í£]{2,4})\s*ì‹¤ì¥",  # ì‹¤ì¥ëª… ì‹¤ì¥
            r"([ê°€-í£]{2,4})\s*ë¶€ì¥",  # ë¶€ì¥ëª… ë¶€ì¥
            r"([ê°€-í£]{2,4})\s*ê³¼ì¥",  # ê³¼ì¥ëª… ê³¼ì¥
            r"([ê°€-í£]{2,4})\s*ì°¨ì¥",  # ì°¨ì¥ëª… ì°¨ì¥
            r"ë¬¸ì˜\s*:\s*([ê°€-í£]{2,4})",  # ë¬¸ì˜: ë‹´ë‹¹ìëª…
            r"ì—°ë½ì²˜\s*:\s*([ê°€-í£]{2,4})",  # ì—°ë½ì²˜: ë‹´ë‹¹ìëª…
            r"Contact\s*:\s*([ê°€-í£]{2,4})",  # Contact: ë‹´ë‹¹ìëª…
        ]
        article_end = full_text[-1500:]
        for pattern in reporter_patterns:
            match = re.search(pattern, article_end)
            if match:
                reporter = match.group(1)
                reporter = re.sub(
                    r"ê¸°ì|ë‹´ë‹¹ì|ëŒ€í‘œ|ë§¤ë‹ˆì €|íŒ€ì¥|ì‹¤ì¥|ë¶€ì¥|ê³¼ì¥|ì°¨ì¥|ë¬¸ì˜|ì—°ë½ì²˜|Contact", "", reporter
                ).strip()
                if 2 <= len(reporter) <= 4:
                    break

        # ë³¸ë¬¸ ì¶”ì¶œ: 1) ì§€ì •ëœ XPath
        content = ""
        try:
            tree = html.fromstring(response.text)
            xpath_expr = "/html/body/div[1]/main/div/div/div/div[2]/section"
            text_nodes = tree.xpath(xpath_expr + "//text()")
            if text_nodes:
                content = " ".join(t.strip() for t in text_nodes if t and t.strip())
        except Exception:
            pass

        # 2) XPath ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ CSS ê¸°ë°˜ ë³´ì™„
        if len(content) < 100:
            content_selectors = [
                "div.article_content",
                'div[class*="article"]',
                'div[class*="content"]',
                'div[class*="news"]',
                'div[class*="text"]',
                "div.news_content",
                "div.view_content",
                "article",
                "main",
                'div[id*="article"]',
                "div.bodycontent",
                "div.story",
                "div.article_body",
                "div.articleView",
                "div.press_content",
                "div.release_content",
            ]
            for selector in content_selectors:
                elements = soup.select(selector)
                for element in elements:
                    text = element.get_text().strip()
                    if len(text) > len(content):
                        content = text

        # 3) ì—¬ì „íˆ ì§§ìœ¼ë©´ p íƒœê·¸ ìˆœíšŒ
        if len(content) < 200:
            paragraphs = soup.find_all("p")
            content_parts = []
            for p in paragraphs:
                text = p.get_text().strip()
                if (
                    len(text) > 20
                    and not re.search(r"ì…ë ¥\s*\d{4}|ìˆ˜ì •\s*\d{4}|Copyright|ì €ì‘ê¶Œ|ë‰´ìŠ¤ì™€ì´ì–´|newswire", text)
                    and not text.startswith(("â–¶", "â˜", "â€»", "â– ", "â–²", "[", "â€»", "â—†", "â—‹", "â–³"))
                    and "ë¬´ë‹¨ ì „ì¬" not in text
                    and "ì¬ë°°í¬ ê¸ˆì§€" not in text
                    and "ê¸°ì‚¬ì œë³´" not in text
                    and "ë¬¸ì˜ì‚¬í•­" not in text
                ):
                    content_parts.append(text)
            if content_parts:
                content = " ".join(content_parts)

        # ë³¸ë¬¸ ì •ì œ
        content = clean_newswire_content(content)

        # RSS ìš”ì•½ì´ ë” ì¢‹ìœ¼ë©´ RSS ìš”ì•½ ì‚¬ìš©
        if rss_summary and (len(content) < 100 or len(rss_summary) > len(content)):
            content = rss_summary
            print(f"    RSS ìš”ì•½ ì±„íƒ (ê¸¸ì´: {len(rss_summary)})")

        print(f"    ìµœì¢… ë³¸ë¬¸ ê¸¸ì´: {len(content)}")
        return reporter, content

    except Exception as e:
        print(f"    âŒ ì—ëŸ¬: {e}")
        return "", rss_summary if rss_summary else f"ì˜¤ë¥˜: {str(e)}"


def clean_newswire_content(content):
    """ë‰´ìŠ¤ì™€ì´ì–´ ê¸°ì‚¬ ë³¸ë¬¸ ì •ì œ"""
    if not content:
        return ""

    # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ë“¤ ì œê±° - ë‰´ìŠ¤ì™€ì´ì–´ íŠ¹ì„±ì— ë§ê²Œ ìˆ˜ì •
    remove_patterns = [
        r"ì…ë ¥\s*\d{4}[-./]\d{2}[-./]\d{2}.*?\d{2}:\d{2}",
        r"ìˆ˜ì •\s*\d{4}[-./]\d{2}[-./]\d{2}.*?\d{2}:\d{2}",
        r"ì—…ë°ì´íŠ¸\s*\d{4}[-./]\d{2}[-./]\d{2}.*?\d{2}:\d{2}",
        r"ë‰´ìŠ¤ì™€ì´ì–´.*ë¬´ë‹¨.*ì „ì¬.*ê¸ˆì§€",
        r"ë¬´ë‹¨.*ì „ì¬.*ì¬ë°°í¬.*ê¸ˆì§€",
        r"ì €ì‘ê¶Œ.*ë‰´ìŠ¤ì™€ì´ì–´",
        r"ê´€ë ¨ê¸°ì‚¬.*ë”ë³´ê¸°",
        r"í˜ì´ìŠ¤ë¶.*íŠ¸ìœ„í„°.*ì¹´ì¹´ì˜¤",
        r"êµ¬ë….*ì‹ ì²­",
        r"ê´‘ê³ ",
        r"ë³´ë„ìë£Œ.*ë¬¸ì˜",
        r"ê¸°ì‚¬.*ë¬¸ì˜",
        r"newswire\.co\.kr",
        r"â“’.*ë‰´ìŠ¤ì™€ì´ì–´",
        r"Newswire",
        r"NEWS.*WIRE",
        r"ë¬¸ì˜ì‚¬í•­.*ì—°ë½ì²˜",
        r"í™ˆí˜ì´ì§€.*ë°”ë¡œê°€ê¸°",
        r"Press.*Release",
        r"ë³´ë„ìë£Œ.*ë",
        r"ì´ìƒ.*ë",
        r"\*\*\*.*ë.*\*\*\*",
        r"---.*ë.*---",
        r"Copyright.*\d{4}.*ë‰´ìŠ¤ì™€ì´ì–´",
    ]

    for pattern in remove_patterns:
        content = re.sub(pattern, "", content, flags=re.IGNORECASE)

    # ê³µë°± ì •ë¦¬
    content = re.sub(r"\s+", " ", content).strip()

    # ê¸¸ì´ ì œí•œ
    if len(content) > 2000:
        content = content[:2000] + "..."

    return content


def fetch_newswire_rss_to_csv(rss_url, output_file, max_articles=30):
    """ë‰´ìŠ¤ì™€ì´ì–´ RSSë¥¼ íŒŒì‹±í•˜ì—¬ CSVë¡œ ì €ì¥"""

    print(f"ë‰´ìŠ¤ì™€ì´ì–´ RSS í”¼ë“œ íŒŒì‹± ì¤‘: {rss_url}")

    # RSS íŒŒì‹±
    try:
        headers = {"User-Agent": get_random_user_agent()}
        response = requests.get(rss_url, headers=headers, timeout=10)
        # ë‰´ìŠ¤ì™€ì´ì–´ RSSëŠ” UTF-8 ì¸ì½”ë”© ì‚¬ìš©
        response.encoding = "utf-8"
        feed = feedparser.parse(response.content)
    except:
        feed = feedparser.parse(rss_url)

    if not feed.entries:
        print("âŒ RSS í”¼ë“œì—ì„œ ë³´ë„ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"âœ… RSSì—ì„œ {len(feed.entries)}ê°œ ë³´ë„ìë£Œ ë°œê²¬")

    success_count = 0
    total_count = min(len(feed.entries), max_articles)

    # CSV íŒŒì¼ ìƒì„±
    with open(output_file, "w", newline="", encoding="utf-8-sig") as csvfile:
        fieldnames = ["ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ë‹´ë‹¹ìëª…", "ë³¸ë¬¸"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        print(f"ì´ {total_count}ê°œ ë³´ë„ìë£Œ ì²˜ë¦¬ ì‹œì‘...\n")

        for i, entry in enumerate(feed.entries[:max_articles]):
            try:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = entry.title.strip()
                title = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", title)

                link = entry.link

                # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (ë‰´ìŠ¤ì™€ì´ì–´ RSS êµ¬ì¡°ì— ë§ê²Œ)
                category = ""
                if hasattr(entry, "category"):
                    category = entry.category.strip()
                elif hasattr(entry, "tags") and entry.tags:
                    category = entry.tags[0].term if entry.tags else ""

                # URLì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì‹œë„ (ë‰´ìŠ¤ì™€ì´ì–´ URL êµ¬ì¡° ê¸°ë°˜)
                if not category:
                    url_category_map = {
                        # ì „ì²´ ë° ì¸ê¸°
                        "rss/all": "ì „ì²´",
                        # ì‚°ì—…ë³„
                        "industry/600": "ê¸°ìˆ ",
                        "industry/400": "ì‚°ì—…",
                        "industry/1200": "í—¬ìŠ¤",
                        "industry/900": "ìƒí™œ",
                        "industry/300": "ìë™ì°¨",
                        "industry/100": "ê²½ì œ",
                        "industry/200": "ê¸ˆìœµ",
                        "industry/800": "ë¬¸í™”",
                        "industry/1300": "ë ˆì €",
                        "industry/1100": "êµìœ¡",
                        "industry/1900": "ì‚¬íšŒ",
                        "industry/1500": "í™˜ê²½",
                        "industry/1400": "ì •ì¹˜",
                        # ì˜ë¬¸ ë‰´ìŠ¤
                        "english": "English News",
                        # ì§€ì—­ë³„
                        "region/1": "ì¸ì²œê²½ê¸°",
                        "region/2": "ëŒ€ì „ì¶©ì²­",
                        "region/3": "ê´‘ì£¼ì „ë¼",
                        "region/4": "ëŒ€êµ¬ê²½ë¶",
                        "region/5": "ë¶€ì‚°ìš¸ì‚°ê²½ë‚¨",
                        "region/6": "ê°•ì›",
                        "region/7": "ê°•ì›",
                        "region/8": "ì¶©ë¶",
                        "region/9": "ì „ë¶",
                        "region/10": "ì œì£¼",
                        "region/11": "í•´ì™¸",
                        "region/123": "ì •ì¹˜",
                    }

                    for url_part, cat_name in url_category_map.items():
                        if url_part in rss_url:
                            category = cat_name
                            break

                # RSS ìš”ì•½ ì •ë³´ ì¶”ì¶œ
                summary = ""
                if hasattr(entry, "description"):
                    summary = entry.description.strip()
                    # HTML íƒœê·¸ì™€ CDATA ì œê±°
                    summary = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", summary, flags=re.DOTALL)
                    summary = re.sub(r"<[^>]+>", "", summary)  # HTML íƒœê·¸ ì œê±°
                    summary = clean_newswire_content(summary)

                # ë‚ ì§œ í˜•ì‹ ë³€í™˜
                if hasattr(entry, "published_parsed") and entry.published_parsed:
                    date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S")
                else:
                    date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                print(f"[{i+1}/{total_count}] {title[:50]}...")

                # ê¸°ì‚¬ ë³¸ë¬¸ ë° ë‹´ë‹¹ìëª… ì¶”ì¶œ
                reporter, content = extract_newswire_article_content(link, summary)

                # ìµœì†Œ ì¡°ê±´ í™•ì¸
                if len(content.strip()) < 30:
                    print(f"    âš  ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ ê±´ë„ˆëœ€ (ê¸¸ì´: {len(content)})\n")
                    continue

                # CSVì— ì“°ê¸°
                writer.writerow(
                    {
                        "ì œëª©": title,
                        "ë‚ ì§œ": date,
                        "ì¹´í…Œê³ ë¦¬": category if category else "ë¯¸ë¶„ë¥˜",
                        "ë‹´ë‹¹ìëª…": reporter if reporter else "ë¯¸ìƒ",
                        "ë³¸ë¬¸": content,
                    }
                )

                success_count += 1
                print(
                    f"    âœ… ì„±ê³µ! (ì¹´í…Œê³ ë¦¬: {category}, ë‹´ë‹¹ì: {reporter if reporter else 'ë¯¸ìƒ'}, ë³¸ë¬¸: {len(content)}ì)"
                )

                # ì§„í–‰ë¥  í‘œì‹œ
                if (i + 1) % 5 == 0:
                    print(f"\nğŸ“Š ì§„í–‰ë¥ : {i+1}/{total_count} ({(i+1)/total_count*100:.1f}%)")
                    print(f"ğŸ“ˆ ì„±ê³µë¥ : {success_count}/{i+1} ({success_count/(i+1)*100:.1f}%)\n")

                # ëœë¤ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                delay = random.uniform(1.5, 3.0)
                time.sleep(delay)

            except KeyboardInterrupt:
                print("\nâš  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"    âŒ ì˜¤ë¥˜: {e}")
                continue

        print(f"\n{'='*70}")
        print(f"ğŸ‰ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥: {output_file}")
        print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {success_count}/{total_count}ê°œ ì„±ê³µ ({success_count/total_count*100:.1f}%)")
        print(f"{'='*70}")


# ì‚¬ìš© ì˜ˆì‹œ
def collect_newswire_rss(rss_url, max_articles=20):
    """ë‰´ìŠ¤ì™€ì´ì–´ RSSë¥¼ íŒŒì‹±í•˜ì—¬ í–‰ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    rows = []
    print(f"ë‰´ìŠ¤ì™€ì´ì–´ RSS íŒŒì‹±: {rss_url}")
    try:
        headers = {"User-Agent": get_random_user_agent()}
        response = requests.get(rss_url, headers=headers, timeout=10)
        response.encoding = "utf-8"
        feed = feedparser.parse(response.content)
    except Exception:
        feed = feedparser.parse(rss_url)

    if not feed.entries:
        print("âŒ RSS í”¼ë“œì—ì„œ í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return rows

    total_count = min(len(feed.entries), max_articles)
    print(f"âœ… {total_count}ê°œ í•­ëª© ì²˜ë¦¬ ì˜ˆì •")

    for i, entry in enumerate(feed.entries[:max_articles]):
        try:
            title = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", entry.title.strip())
            link = entry.link

            # ì¹´í…Œê³ ë¦¬
            category = ""
            if hasattr(entry, "category"):
                category = entry.category.strip()
            elif hasattr(entry, "tags") and entry.tags:
                category = entry.tags[0].term if entry.tags else ""

            # URL ë§¤í•‘ ë³´ì¡°
            if not category:
                url_category_map = {
                    "rss/all": "ì „ì²´",
                    "industry/600": "ê¸°ìˆ ",
                    "industry/400": "ì‚°ì—…",
                    "industry/1200": "í—¬ìŠ¤",
                    "industry/900": "ìƒí™œ",
                    "industry/300": "ìë™ì°¨",
                    "industry/100": "ê²½ì œ",
                    "industry/200": "ê¸ˆìœµ",
                    "industry/800": "ë¬¸í™”",
                    "industry/1300": "ë ˆì €",
                    "industry/1100": "êµìœ¡",
                    "industry/1900": "ì‚¬íšŒ",
                    "industry/1500": "í™˜ê²½",
                    "industry/1400": "ì •ì¹˜",
                    "english": "English News",
                    "region/1": "ì¸ì²œê²½ê¸°",
                    "region/2": "ëŒ€ì „ì¶©ì²­",
                    "region/3": "ê´‘ì£¼ì „ë¼",
                    "region/4": "ëŒ€êµ¬ê²½ë¶",
                    "region/5": "ë¶€ì‚°ìš¸ì‚°ê²½ë‚¨",
                    "region/6": "ê°•ì›",
                    "region/7": "ê°•ì›",
                    "region/8": "ì¶©ë¶",
                    "region/9": "ì „ë¶",
                    "region/10": "ì œì£¼",
                    "region/11": "í•´ì™¸",
                    "region/123": "ì •ì¹˜",
                }
                for url_part, cat_name in url_category_map.items():
                    if url_part in rss_url:
                        category = cat_name
                        break

            # ìš”ì•½
            summary = ""
            if hasattr(entry, "description"):
                summary = entry.description.strip()
                summary = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", summary, flags=re.DOTALL)
                summary = re.sub(r"<[^>]+>", "", summary)
                summary = clean_newswire_content(summary)

            # ë‚ ì§œ
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S")
            else:
                date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            print(f"[{i+1}/{total_count}] {title[:50]}...")
            reporter, content = extract_newswire_article_content(link, summary)
            if len(content.strip()) < 30:
                print(f"    âš  ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ ê±´ë„ˆëœ€ (ê¸¸ì´: {len(content)})")
                continue

            rows.append(
                {
                    "ì–¸ë¡ ì‚¬": "ë‰´ìŠ¤ì™€ì´ì–´",
                    "ì œëª©": title,
                    "ë‚ ì§œ": date,
                    "ì¹´í…Œê³ ë¦¬": category if category else "ë¯¸ë¶„ë¥˜",
                    "ê¸°ìëª…": reporter if reporter else "ë¯¸ìƒ",
                    "ë³¸ë¬¸": content,
                }
            )

            time.sleep(random.uniform(1.5, 3.0))
        except Exception as e:
            print(f"    âŒ ì˜¤ë¥˜: {e}")
            continue

    return rows


if __name__ == "__main__":
    # ë‰´ìŠ¤ì™€ì´ì–´ RSS URL ì˜µì…˜ë“¤ (ì²¨ë¶€ëœ ì´ë¯¸ì§€ ê¸°ë°˜)
    newswire_rss_options = {
        # ì „ì²´
        "ì „ì²´": "https://api.newswire.co.kr/rss/all",
        # ì‚°ì—…ë³„
        "ê¸°ìˆ ": "https://api.newswire.co.kr/rss/industry/600",
        "ì‚°ì—…": "https://api.newswire.co.kr/rss/industry/400",
        "í—¬ìŠ¤": "https://api.newswire.co.kr/rss/industry/1200",
        "ìƒí™œ": "https://api.newswire.co.kr/rss/industry/900",
        "ìë™ì°¨": "https://api.newswire.co.kr/rss/industry/300",
        "ê²½ì œ": "https://api.newswire.co.kr/rss/industry/100",
        "ê¸ˆìœµ": "https://api.newswire.co.kr/rss/industry/200",
        "ë¬¸í™”": "https://api.newswire.co.kr/rss/industry/800",
        "ë ˆì €": "https://api.newswire.co.kr/rss/industry/1300",
        "êµìœ¡": "https://api.newswire.co.kr/rss/industry/1100",
        "ì‚¬íšŒ": "https://api.newswire.co.kr/rss/industry/1900",
        "í™˜ê²½": "https://api.newswire.co.kr/rss/industry/1500",
        "ì •ì¹˜": "https://api.newswire.co.kr/rss/industry/1400",
        # ì˜ë¬¸ ë‰´ìŠ¤
        "English News": "https://api.newswire.co.kr/rss/english",
        # ì§€ì—­ë³„ (ì£¼ìš” ì§€ì—­ë§Œ)
        "ì„œìš¸": "https://api.newswire.co.kr/rss/region/1",
        "ì¸ì²œê²½ê¸°": "https://api.newswire.co.kr/rss/region/2",
        "ëŒ€ì „ì¶©ë‚¨": "https://api.newswire.co.kr/rss/region/3",
        "ê´‘ì£¼ì „ë‚¨": "https://api.newswire.co.kr/rss/region/4",
        "ë¶€ì‚°ìš¸ì‚°ê²½ë‚¨": "https://api.newswire.co.kr/rss/region/5",
        "ëŒ€êµ¬ê²½ë¶": "https://api.newswire.co.kr/rss/region/6",
        "ê°•ì›": "https://api.newswire.co.kr/rss/region/7",
        "ì¶©ë¶": "https://api.newswire.co.kr/rss/region/8",
        "ì „ë¶": "https://api.newswire.co.kr/rss/region/9",
        "ì œì£¼": "https://api.newswire.co.kr/rss/region/10",
        "í•´ì™¸": "https://api.newswire.co.kr/rss/region/11",
    }
    # ìë™ ìˆ˜ì§‘ ëŒ€ìƒ: ì§€ì •ëœ 10ê°œ ë¶„ë¥˜ + ëª¨ë“  ì§€ì—­ ë¶„ë¥˜
    target_categories = ["ì „ì²´", "ê¸°ìˆ ", "ì‚°ì—…", "ìƒí™œ", "ê²½ì œ", "ê¸ˆìœµ", "êµìœ¡", "ì‚¬íšŒ", "í™˜ê²½", "ì •ì¹˜"]
    # ì§€ì—­ ë¶„ë¥˜: URLì— '/region/'ì´ í¬í•¨ëœ í•­ëª© ì „ë¶€
    region_categories = [k for k, v in newswire_rss_options.items() if "/region/" in v]

    # ê²°ê³¼ ë””ë ‰í„°ë¦¬ ë³´ì¥
    os.makedirs("results", exist_ok=True)

    # ê³µí†µ íƒ€ì„ìŠ¤íƒ¬í”„ (í•œ ë²ˆì˜ ì‹¤í–‰ì—ì„œ ì¼ê´€ì„± ìˆê²Œ)
    run_ts = datetime.now().strftime("%Y%m%d_%H%M")

    all_rows = []
    # ì§€ì • ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ (ê° 20ê°œ)
    for cat in target_categories:
        if cat not in newswire_rss_options:
            print(f"âš  ê²½ê³ : '{cat}' RSS URLì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ê±´ë„ˆëœ€.")
            continue
        rss_url = newswire_rss_options[cat]
        print(f"\nğŸš€ '{cat}' ì¹´í…Œê³ ë¦¬ì—ì„œ 20ê°œ ë³´ë„ìë£Œ ìˆ˜ì§‘ ì‹œì‘!")
        rows = collect_newswire_rss(rss_url, max_articles=1)
        all_rows.extend(rows)
        time.sleep(random.uniform(1.0, 2.0))

    # ì§€ì—­ ì „ì²´ ìˆ˜ì§‘
    print("\nğŸ“ ì§€ì—­ë³„ ì „ì²´ ìˆ˜ì§‘ ì‹œì‘...")
    for region in region_categories:
        rss_url = newswire_rss_options[region]
        print(f"\nğŸš€ ì§€ì—­ '{region}'ì—ì„œ 20ê°œ ë³´ë„ìë£Œ ìˆ˜ì§‘ ì‹œì‘!")
        rows = collect_newswire_rss(rss_url, max_articles=1)
        all_rows.extend(rows)
        time.sleep(random.uniform(1.0, 2.0))
    # í•˜ë‚˜ì˜ CSVë¡œ ì €ì¥
    combined_path = f"results/ë‰´ìŠ¤ì™€ì´ì–´_ì „ì²´_{run_ts}.csv"
    with open(combined_path, "w", newline="", encoding="utf-8-sig") as f:
        fieldnames = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for row in all_rows:
            writer.writerow(row)
    print(f"\nâœ… ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë° ì§€ì—­ ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ {len(all_rows)}ê±´")
    print(f"ğŸ“ ì €ì¥ íŒŒì¼: {combined_path}")
