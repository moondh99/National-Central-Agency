import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import pandas as pd
import time
import random
import re
from datetime import datetime
import csv
from urllib.parse import urljoin, urlparse
import logging
import os


class KoreaDepartmentRSSCrawler:
    def __init__(self):
        """ì •ì±…ë¸Œë¦¬í•‘ ë¶€ì²˜ë³„ RSS í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.base_url = "https://www.korea.kr"

        # 22ê°œ ë¶€ì²˜ë³„ RSS í”¼ë“œ
        self.department_feeds = {
            "êµ­ë¬´ì¡°ì •ì‹¤": "https://www.korea.kr/rss/dept_opm.xml",
            "ê¸°íšì¬ì •ë¶€": "https://www.korea.kr/rss/dept_moef.xml",
            "êµìœ¡ë¶€": "https://www.korea.kr/rss/dept_moe.xml",
            "ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€": "https://www.korea.kr/rss/dept_msit.xml",
            "ì™¸êµë¶€": "https://www.korea.kr/rss/dept_mofa.xml",
            "í†µì¼ë¶€": "https://www.korea.kr/rss/dept_unikorea.xml",
            "ë²•ë¬´ë¶€": "https://www.korea.kr/rss/dept_moj.xml",
            "êµ­ë°©ë¶€": "https://www.korea.kr/rss/dept_mnd.xml",
            "í–‰ì •ì•ˆì „ë¶€": "https://www.korea.kr/rss/dept_mois.xml",
            "êµ­ê°€ë³´í›ˆë¶€": "https://www.korea.kr/rss/dept_mpva.xml",
            "ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€": "https://www.korea.kr/rss/dept_mcst.xml",
            "ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€": "https://www.korea.kr/rss/dept_mafra.xml",
            "ì‚°ì—…í†µìƒìì›ë¶€": "https://www.korea.kr/rss/dept_motie.xml",
            "ë³´ê±´ë³µì§€ë¶€": "https://www.korea.kr/rss/dept_mw.xml",
            "í™˜ê²½ë¶€": "https://www.korea.kr/rss/dept_me.xml",
            "ê³ ìš©ë…¸ë™ë¶€": "https://www.korea.kr/rss/dept_moel.xml",
            "ì—¬ì„±ê°€ì¡±ë¶€": "https://www.korea.kr/rss/dept_mogef.xml",
            "êµ­í† êµí†µë¶€": "https://www.korea.kr/rss/dept_molit.xml",
            "í•´ì–‘ìˆ˜ì‚°ë¶€": "https://www.korea.kr/rss/dept_mof.xml",
            "ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€": "https://www.korea.kr/rss/dept_mss.xml",
            "ì¸ì‚¬í˜ì‹ ì²˜": "https://www.korea.kr/rss/dept_mpm.xml",
            "ë²•ì œì²˜": "https://www.korea.kr/rss/dept_moleg.xml",
            "ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜": "https://www.korea.kr/rss/dept_mfds.xml",
        }

        # ë¶€ì²˜ë³„ ì£¼ìš” ì •ì±… ë¶„ì•¼ (ë¶„ì„ìš©)
        self.department_areas = {
            "êµ­ë¬´ì¡°ì •ì‹¤": "ì •ë¶€ ì •ì±… ì¡°ì •",
            "ê¸°íšì¬ì •ë¶€": "ê²½ì œì •ì±…, ì˜ˆì‚°, ì„¸ì œ",
            "êµìœ¡ë¶€": "êµìœ¡ì •ì±…, ëŒ€í•™, í‰ìƒêµìœ¡",
            "ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€": "ICT, ê³¼í•™ê¸°ìˆ , ë°©ì†¡í†µì‹ ",
            "ì™¸êµë¶€": "ì™¸êµ, êµ­ì œê´€ê³„, í•´ì™¸ë™í¬",
            "í†µì¼ë¶€": "í†µì¼ì •ì±…, ë¶í•œ, ë‚¨ë¶ê´€ê³„",
            "ë²•ë¬´ë¶€": "ë²•ë¬´í–‰ì •, ì¶œì…êµ­, ì¸ê¶Œ",
            "êµ­ë°©ë¶€": "êµ­ë°©ì •ì±…, ë³‘ë¬´, êµ­ë°©ì‚°ì—…",
            "í–‰ì •ì•ˆì „ë¶€": "í–‰ì •í˜ì‹ , ì§€ë°©ìì¹˜, ì•ˆì „ê´€ë¦¬",
            "êµ­ê°€ë³´í›ˆë¶€": "êµ­ê°€ìœ ê³µì, ë³´í›ˆë³µì§€",
            "ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€": "ë¬¸í™”ì˜ˆìˆ , ì²´ìœ¡, ê´€ê´‘, ì¢…êµ",
            "ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€": "ë†ì—…, ì¶•ì‚°, ì‹í’ˆì•ˆì „",
            "ì‚°ì—…í†µìƒìì›ë¶€": "ì‚°ì—…ì •ì±…, ì—ë„ˆì§€, í†µìƒ",
            "ë³´ê±´ë³µì§€ë¶€": "ë³´ê±´ì˜ë£Œ, ë³µì§€, ì¸êµ¬ì •ì±…",
            "í™˜ê²½ë¶€": "í™˜ê²½ë³´ì „, ê¸°í›„ë³€í™”, ìƒí•˜ìˆ˜ë„",
            "ê³ ìš©ë…¸ë™ë¶€": "ê³ ìš©ì •ì±…, ë…¸ë™, ì‚°ì—…ì•ˆì „",
            "ì—¬ì„±ê°€ì¡±ë¶€": "ì—¬ì„±ì •ì±…, ê°€ì¡±, ì²­ì†Œë…„",
            "êµ­í† êµí†µë¶€": "êµ­í† ê°œë°œ, êµí†µ, ì£¼íƒ, ê±´ì„¤",
            "í•´ì–‘ìˆ˜ì‚°ë¶€": "í•´ì–‘ì •ì±…, ìˆ˜ì‚°ì—…, í•´ìš´í•­ë§Œ",
            "ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€": "ì¤‘ì†Œê¸°ì—…, ë²¤ì²˜, ì†Œìƒê³µì¸",
            "ì¸ì‚¬í˜ì‹ ì²˜": "ê³µë¬´ì› ì¸ì‚¬, ì¡°ì§ê´€ë¦¬",
            "ë²•ì œì²˜": "ë²•ë ¹ì •ë¹„, ë²•ì œì—…ë¬´",
            "ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜": "ì‹í’ˆì•ˆì „, ì˜ì•½í’ˆ, í™”ì¥í’ˆ",
        }

        # User-Agent ë¦¬ìŠ¤íŠ¸
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
        ]

        self.articles = []
        self.session = requests.Session()

        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
        self.logger = logging.getLogger(__name__)

    def get_random_headers(self):
        """ëœë¤ í—¤ë” ìƒì„±"""
        return {
            "User-Agent": random.choice(self.user_agents),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
        }

    def random_delay(self, min_delay=1, max_delay=3):
        """ëœë¤ ë”œë ˆì´"""
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)

    def fetch_rss_feed(self, rss_url, max_retries=3):
        """RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°"""
        for attempt in range(max_retries):
            try:
                headers = self.get_random_headers()
                response = self.session.get(rss_url, headers=headers, timeout=30)
                response.raise_for_status()
                response.encoding = "utf-8"
                return response.text
            except Exception as e:
                self.logger.warning(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    self.random_delay(2, 5)
                else:
                    self.logger.error(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ìµœì¢… ì‹¤íŒ¨: {rss_url}")
                    return None

    def parse_rss_feed(self, rss_content):
        """RSS í”¼ë“œ íŒŒì‹±"""
        try:
            root = ET.fromstring(rss_content)
            items = []

            for item in root.findall(".//item"):
                article_info = {}

                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title_elem = item.find("title")
                article_info["title"] = title_elem.text.strip() if title_elem is not None else ""

                link_elem = item.find("link")
                article_info["link"] = link_elem.text.strip() if link_elem is not None else ""

                pubdate_elem = item.find("pubDate")
                article_info["pub_date"] = pubdate_elem.text.strip() if pubdate_elem is not None else ""

                guid_elem = item.find("guid")
                article_info["guid"] = guid_elem.text.strip() if guid_elem is not None else ""

                # dc:creator ì¶”ì¶œ (ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê³ ë ¤)
                creator_elem = item.find(".//{http://purl.org/dc/elements/1.1/}creator")
                article_info["creator"] = creator_elem.text.strip() if creator_elem is not None else ""

                # descriptionì—ì„œ ì „ì²´ ë‚´ìš© ì¶”ì¶œ
                desc_elem = item.find("description")
                if desc_elem is not None:
                    desc_text = desc_elem.text or ""
                    # CDATA ì²˜ë¦¬
                    if desc_text.startswith("<![CDATA[") and desc_text.endswith("]]>"):
                        desc_text = desc_text[9:-3]
                    # HTML íƒœê·¸ ì œê±°í•˜ì—¬ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                    soup = BeautifulSoup(desc_text, "html.parser")
                    article_info["description"] = soup.get_text().strip()
                else:
                    article_info["description"] = ""

                items.append(article_info)

            return items
        except Exception as e:
            self.logger.error(f"RSS íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []

    def extract_article_content(self, article_url, max_retries=3):
        """ê°œë³„ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ - ë¶€ì²˜ í˜ì´ì§€ ìµœì í™”"""
        for attempt in range(max_retries):
            try:
                headers = self.get_random_headers()
                response = self.session.get(article_url, headers=headers, timeout=30)
                response.raise_for_status()
                response.encoding = "utf-8"

                soup = BeautifulSoup(response.text, "html.parser")

                # ë¶€ì²˜ë³„ í˜ì´ì§€ êµ¬ì¡°ì— ìµœì í™”ëœ ë³¸ë¬¸ ì¶”ì¶œ ì…€ë ‰í„°
                content_selectors = [
                    ".dept_cont",  # ë¶€ì²˜ ì½˜í…ì¸ 
                    ".press_cont",  # ë³´ë„ìë£Œ ì½˜í…ì¸ 
                    ".article_body",  # ì¼ë°˜ ê¸°ì‚¬
                    ".rbody",  # ë¸Œë¦¬í•‘ í˜ì´ì§€
                    ".view_cont",  # ë·° í˜ì´ì§€
                    ".cont_body",  # ì½˜í…ì¸  ë³¸ë¬¸
                    ".policy_body",  # ì •ì±… ë³¸ë¬¸
                    ".briefing_cont",  # ë¸Œë¦¬í•‘ ë‚´ìš©
                    ".news_cont",  # ë‰´ìŠ¤ ë‚´ìš©
                    ".ministry_cont",  # ë¶€ì²˜ ê³µì§€ì‚¬í•­
                ]

                content = ""
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text().strip()
                        break

                # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„
                if not content:
                    # í—¤ë”, í‘¸í„°, ì‚¬ì´ë“œë°” ë“± ì œê±°
                    for elem in soup.find_all(["header", "footer", "nav", "aside", "script", "style"]):
                        elem.decompose()

                    main_content = soup.find("main") or soup.find("div", class_="content") or soup.find("body")
                    if main_content:
                        content = main_content.get_text().strip()

                # ë¶€ì²˜ëª…/ë‹´ë‹¹ì/ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ
                contact_info = self.extract_contact_info(content)
                policy_keywords = self.extract_policy_keywords(content)

                # í…ìŠ¤íŠ¸ ì •ë¦¬
                content = re.sub(r"\s+", " ", content).strip()

                return {
                    "content": content[:3000] + "..." if len(content) > 3000 else content,
                    "contact_info": contact_info,
                    "policy_keywords": policy_keywords,
                }

            except Exception as e:
                self.logger.warning(f"ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    self.random_delay(1, 3)
                else:
                    return {"content": "ì¶”ì¶œ ì‹¤íŒ¨", "contact_info": "", "policy_keywords": ""}

    def extract_contact_info(self, content):
        """ì—°ë½ì²˜/ë‹´ë‹¹ì ì •ë³´ ì¶”ì¶œ - ë¶€ì²˜ë³„ íŠ¹í™”"""
        # ë¶€ì²˜ë³„ ì—°ë½ì²˜ ì •ë³´ ì¶”ì¶œ íŒ¨í„´
        patterns = [
            r"ë¬¸ì˜\s*:\s*([^(]+?)(?:\(([^)]+)\))?",
            r"ë‹´ë‹¹\s*:\s*([^(]+?)(?:\(([^)]+)\))?",
            r"ì—°ë½ì²˜\s*:\s*([^(]+?)(?:\(([^)]+)\))?",
            r"ë¬¸ì˜ì²˜\s*:\s*([^(]+?)(?:\(([^)]+)\))?",
            r"ë‹´ë‹¹ë¶€ì„œ\s*:\s*([^(]+?)(?:\(([^)]+)\))?",
            r"ë‹´ë‹¹ì\s*:\s*([^(]+?)(?:\(([^)]+)\))?",
            r"([ê°€-í£]+ë¶€|[ê°€-í£]+ì²­|[ê°€-í£]+ì›|[ê°€-í£]+ì‹¤|[ê°€-í£]+ìœ„ì›íšŒ|[ê°€-í£]+ì²˜)\s+([ê°€-í£]+ê³¼|[ê°€-í£]+íŒ€|[ê°€-í£]+êµ­)\s*(?:\(([^)]+)\))?",
        ]

        contact_info = {}

        for pattern in patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if isinstance(match, tuple):
                    dept = match[0].strip() if match[0] else ""
                    phone = match[1].strip() if len(match) > 1 and match[1] else ""

                    if dept and len(dept) > 1 and len(dept) < 100:
                        contact_info["department"] = dept
                    if phone and ("02-" in phone or "044-" in phone or "070-" in phone):
                        contact_info["phone"] = phone
                else:
                    dept = match.strip()
                    if dept and len(dept) > 1 and len(dept) < 100:
                        contact_info["department"] = dept

        return "; ".join([f"{k}: {v}" for k, v in contact_info.items()])

    def extract_policy_keywords(self, content):
        """ì •ì±… í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ì£¼ìš” ì •ì±… í‚¤ì›Œë“œ íŒ¨í„´
        policy_patterns = [
            r"(ì •ì±…|ì œë„|ë°©ì•ˆ|ê³„íš|ì‚¬ì—…|í”„ë¡œê·¸ë¨|ì§€ì›|ê°œì„ |ê°•í™”|í™•ëŒ€|ë„ì…|ì‹œí–‰|ì¶”ì§„)",
            r"(ì˜ˆì‚°|íˆ¬ì|ì§€ì›ê¸ˆ|ë³´ì¡°ê¸ˆ|ìœµì|ì„¸ì œ|í˜œíƒ)",
            r"(ë²•ë ¹|ê·œì •|ê¸°ì¤€|ê°€ì´ë“œë¼ì¸|ë§¤ë‰´ì–¼)",
            r"(ê°œí˜|í˜ì‹ |ë””ì§€í„¸|ìŠ¤ë§ˆíŠ¸|ê·¸ë¦°|ì¹œí™˜ê²½)",
            r"(ì•ˆì „|ë³´ì•ˆ|ì˜ˆë°©|ëŒ€ì‘|ê´€ë¦¬)",
            r"(ì¼ìë¦¬|ê³ ìš©|ì°½ì—…|ì‚°ì—…|ê²½ì œ)",
            r"(ë³µì§€|ê±´ê°•|êµìœ¡|ë¬¸í™”|í™˜ê²½)",
        ]

        keywords = set()
        for pattern in policy_patterns:
            matches = re.findall(pattern, content)
            keywords.update(matches)

        return ", ".join(list(keywords)[:10])  # ìµœëŒ€ 10ê°œ í‚¤ì›Œë“œ

    def crawl_department_feed(self, department, rss_url, max_items=30):
        """ê°œë³„ ë¶€ì²˜ RSS í”¼ë“œ í¬ë¡¤ë§"""
        self.logger.info(f"ë¶€ì²˜ í¬ë¡¤ë§ ì‹œì‘: {department}")

        # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
        rss_content = self.fetch_rss_feed(rss_url)
        if not rss_content:
            return

        # RSS íŒŒì‹±
        rss_items = self.parse_rss_feed(rss_content)
        if not rss_items:
            self.logger.warning(f"RSS ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤: {department}")
            return

        # ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ë§Œ ì²˜ë¦¬
        items_to_process = rss_items[:max_items]

        for i, item in enumerate(items_to_process, 1):
            try:
                self.logger.info(f"{department} ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘: {i}/{len(items_to_process)} - {item['title'][:50]}...")

                # ê¸°ì‚¬ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
                if item["link"]:
                    article_detail = self.extract_article_content(item["link"])

                    article_data = {
                        "department": department,
                        "policy_area": self.department_areas.get(department, ""),
                        "title": item["title"],
                        "link": item["link"],
                        "pub_date": item["pub_date"],
                        "creator": item["creator"],
                        "description": item["description"],
                        "content": article_detail["content"],
                        "contact_info": article_detail["contact_info"],
                        "policy_keywords": article_detail["policy_keywords"],
                        "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    }

                    self.articles.append(article_data)

                # ë”œë ˆì´
                self.random_delay(1, 3)

            except Exception as e:
                self.logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue

        self.logger.info(f"{department} í¬ë¡¤ë§ ì™„ë£Œ: {len(items_to_process)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬")

    def crawl_all_departments(self, max_items_per_department=30):
        """ëª¨ë“  ë¶€ì²˜ RSS í”¼ë“œ í¬ë¡¤ë§"""
        total_departments = len(self.department_feeds)
        self.logger.info(f"ì „ì²´ {total_departments}ê°œ ë¶€ì²˜ RSS í”¼ë“œ í¬ë¡¤ë§ ì‹œì‘")

        for i, (department, rss_url) in enumerate(self.department_feeds.items(), 1):
            try:
                self.logger.info(f"[{i}/{total_departments}] {department} í”¼ë“œ í¬ë¡¤ë§ ì¤‘...")
                self.crawl_department_feed(department, rss_url, max_items_per_department)

                # ë¶€ì²˜ ê°„ ë”œë ˆì´
                if i < total_departments:
                    self.random_delay(3, 6)

            except Exception as e:
                self.logger.error(f"{department} ë¶€ì²˜ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                continue

        self.logger.info(f"ì „ì²´ ë¶€ì²˜ í¬ë¡¤ë§ ì™„ë£Œ: {len(self.articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        self.print_statistics()

    def crawl_specific_departments(self, department_names, max_items_per_department=30):
        """íŠ¹ì • ë¶€ì²˜ë“¤ë§Œ í¬ë¡¤ë§"""
        for dept_name in department_names:
            if dept_name in self.department_feeds:
                self.crawl_department_feed(dept_name, self.department_feeds[dept_name], max_items_per_department)
            else:
                self.logger.warning(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë¶€ì²˜: {dept_name}")
                available_depts = list(self.department_feeds.keys())
                self.logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë¶€ì²˜: {available_depts}")

    def crawl_by_policy_area(self, policy_areas, max_items_per_department=20):
        """ì •ì±… ë¶„ì•¼ë³„ ë¶€ì²˜ í¬ë¡¤ë§"""
        target_departments = []

        for dept, area in self.department_areas.items():
            for policy_area in policy_areas:
                if policy_area in area:
                    target_departments.append(dept)
                    break

        if target_departments:
            self.logger.info(f"ì •ì±… ë¶„ì•¼ '{', '.join(policy_areas)}'ì— í•´ë‹¹í•˜ëŠ” ë¶€ì²˜: {target_departments}")
            self.crawl_specific_departments(target_departments, max_items_per_department)
        else:
            self.logger.warning(f"í•´ë‹¹ ì •ì±… ë¶„ì•¼ì— ë§ëŠ” ë¶€ì²˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {policy_areas}")

    def save_to_csv(self, filename=None):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"results/ë¶€ì²˜ë³„_RSS_{timestamp}.csv"

        # ê³ ì • ì»¬ëŸ¼ ìˆœì„œë¡œ CSV ì €ì¥
        fieldnames = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        try:
            with open(filename, "w", newline="", encoding="utf-8-sig") as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                for art in self.articles:
                    writer.writerow(
                        {
                            "ì–¸ë¡ ì‚¬": "ì •ì±…í¬í„¸_ë¶€ì²˜ë³„",
                            "ì œëª©": art.get("title", ""),
                            "ë‚ ì§œ": art.get("pub_date", ""),
                            "ì¹´í…Œê³ ë¦¬": art.get("policy_area", ""),
                            "ê¸°ìëª…": "ì •ì±…í¬í„¸",
                            "ë³¸ë¬¸": art.get("description", ""),
                        }
                    )
            self.logger.info(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            self.logger.info(f"ì´ {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥")
        except Exception as e:
            self.logger.error(f"CSV ì €ì¥ ì˜¤ë¥˜: {e}")

    def save_by_department(self):
        """ë¶€ì²˜ë³„ë¡œ ê°œë³„ CSV íŒŒì¼ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        df = pd.DataFrame(self.articles)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        for department in df["department"].unique():
            dept_df = df[df["department"] == department]
            filename = f"results/ë¶€ì²˜ë³„_{department}_{timestamp}.csv"
            dept_df.to_csv(filename, index=False, encoding="utf-8-sig")
            self.logger.info(f"{department} ì €ì¥ ì™„ë£Œ: {filename} ({len(dept_df)}ê°œ ê¸°ì‚¬)")

    def print_statistics(self):
        """í¬ë¡¤ë§ í†µê³„ ì¶œë ¥"""
        if not self.articles:
            return

        df = pd.DataFrame(self.articles)

        print("\n" + "=" * 60)
        print("ë¶€ì²˜ë³„ RSS í¬ë¡¤ë§ í†µê³„")
        print("=" * 60)

        # ë¶€ì²˜ë³„ í†µê³„
        dept_stats = df["department"].value_counts()
        print(f"\nğŸ›ï¸ ë¶€ì²˜ë³„ ê¸°ì‚¬ ìˆ˜:")
        for dept, count in dept_stats.items():
            policy_area = self.department_areas.get(dept, "")
            print(f"  â€¢ {dept} ({policy_area}): {count}ê°œ")

        # ì •ì±… ë¶„ì•¼ë³„ í†µê³„
        policy_area_stats = df["policy_area"].value_counts().head(10)
        if not policy_area_stats.empty:
            print(f"\nğŸ“Š ì£¼ìš” ì •ì±… ë¶„ì•¼ë³„ ê¸°ì‚¬ ìˆ˜:")
            for area, count in policy_area_stats.items():
                print(f"  â€¢ {area}: {count}ê°œ")

        # ì—°ë½ì²˜ ì •ë³´ í†µê³„
        contact_available = len(df[df["contact_info"] != ""])
        print(f"\nğŸ“ ì—°ë½ì²˜ ì •ë³´:")
        print(f"  â€¢ ì—°ë½ì²˜ ì¶”ì¶œ ì„±ê³µ: {contact_available}ê°œ")
        print(f"  â€¢ ì—°ë½ì²˜ ì¶”ì¶œ ì‹¤íŒ¨: {len(df) - contact_available}ê°œ")

        print(f"\nğŸ“ˆ ì „ì²´ ìš”ì•½:")
        print(f"  â€¢ ì´ ê¸°ì‚¬ ìˆ˜: {len(self.articles)}ê°œ")
        print(f"  â€¢ í¬ë¡¤ë§ ë¶€ì²˜ ìˆ˜: {len(dept_stats)}ê°œ")
        print(f"  â€¢ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {len(df[df['content'] != 'ì¶”ì¶œ ì‹¤íŒ¨'])}ê°œ")
        print(f"  â€¢ ì •ì±… í‚¤ì›Œë“œ ì¶”ì¶œ: {len(df[df['policy_keywords'] != ''])}ê°œ")
        print("=" * 60)

    def get_available_departments(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë¶€ì²˜ ëª©ë¡ ë°˜í™˜"""
        return list(self.department_feeds.keys())

    def get_policy_areas(self):
        """ì •ì±… ë¶„ì•¼ ëª©ë¡ ë°˜í™˜"""
        return list(set(self.department_areas.values()))


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ì •ì±…ë¸Œë¦¬í•‘ ë¶€ì²˜ë³„ RSS í¬ë¡¤ëŸ¬")
    print("=" * 50)

    crawler = KoreaDepartmentRSSCrawler()

    # ì‚¬ìš© ì˜ˆì‹œ 1: ì „ì²´ ë¶€ì²˜ í¬ë¡¤ë§ (ê° ë¶€ì²˜ë‹¹ 20ê°œì”©)
    print("ì „ì²´ ë¶€ì²˜ RSS í”¼ë“œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ê° ë¶€ì²˜ë‹¹ 20ê±´)")
    crawler.crawl_all_departments(max_items_per_department=20)

    # CSV ì €ì¥
    crawler.save_to_csv()

    print("\në¶€ì²˜ë³„ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
