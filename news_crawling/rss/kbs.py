from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd
from bs4 import BeautifulSoup
import time
import re
from datetime import datetime
import os
import requests


# KBS ë‰´ìŠ¤ ì„¹ì…˜ ì„¤ì •
KBS_SECTIONS = {
    "politics": {"code": "0003", "name": "ì •ì¹˜"},
    "economy": {"code": "0004", "name": "ê²½ì œ"},
    "society": {"code": "0005", "name": "ì‚¬íšŒ"},
    "international": {"code": "0008", "name": "êµ­ì œ"},
    "it_science": {"code": "0007", "name": "ITÂ·ê³¼í•™"},
    "culture": {"code": "0006", "name": "ë¬¸í™”"},
}


def setup_chrome_driver(headless=True):
    """
    Chrome WebDriver ì„¤ì •
    headless: ë¸Œë¼ìš°ì € ì°½ì„ ìˆ¨ê¸¸ì§€ ì—¬ë¶€
    """
    options = Options()
    if headless:
        options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    )

    try:
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        return driver
    except Exception as e:
        print(f"Chrome WebDriver ì„¤ì • ì˜¤ë¥˜: {e}")
        return None


def build_kbs_url(section_code, date_str, page_num=1):
    """
    KBS ë‰´ìŠ¤ URL ìƒì„±
    section_code: ì„¹ì…˜ ì½”ë“œ (ì˜ˆ: '0003')
    date_str: ë‚ ì§œ ë¬¸ìì—´ (ì˜ˆ: '20250815')
    page_num: í˜ì´ì§€ ë²ˆí˜¸
    """
    base_url = "https://news.kbs.co.kr/news/pc/category/category.do"
    url = f"{base_url}?ctcd={section_code}&ref=pSiteMap#{date_str}&{page_num}"
    return url


def get_kbs_articles_from_page(driver, section_code, section_name, date_str, page_num=1):
    """
    KBS ë‰´ìŠ¤ íŠ¹ì • í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘
    """
    url = build_kbs_url(section_code, date_str, page_num)

    try:
        print(f"  [{section_name}] í˜ì´ì§€ {page_num} ë¡œë”© ì¤‘... ({url})")
        # í˜ì´ì§€ ë¡œë“œ ë˜ëŠ” í˜ì´ì§€ ë²„íŠ¼ í´ë¦­
        if page_num == 1:
            driver.get(url)
        else:
            try:
                # í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜ ë²„íŠ¼ í´ë¦­
                btn = driver.find_element(By.CSS_SELECTOR, f"ul.number-buttons button[data-page='{page_num}']")
                driver.execute_script("arguments[0].click();", btn)
            except Exception as e:
                print(f"  [{section_name}] í˜ì´ì§€ {page_num} ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨ ({e}), URLë¡œ ì´ë™")
                driver.get(url)
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        # í˜„ì¬ URL í™•ì¸ (ë””ë²„ê¹…ìš©)
        print(f"  [{section_name}] í˜„ì¬ URL: {driver.current_url}")
        time.sleep(3)  # JavaScript ë¡œë”© ëŒ€ê¸°

        # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # box-contents íŒ¨í„´ì„ ëª¨ë“  í˜ì´ì§€ì—ì„œ ì¶”ì¶œ
        container = soup.find("div", class_="box-contents has-wrap")
        if container:
            link_elems = container.find_all("a", class_="box-content flex-style")
            articles = []
            for elem in link_elems:
                # ì œëª©
                title_tag = elem.find("p", class_="title")
                title = title_tag.get_text().strip() if title_tag else ""
                # ë§í¬
                href = elem.get("href", "")
                if href.startswith("/"):
                    link = "https://news.kbs.co.kr" + href
                else:
                    link = href
                if title and link:
                    articles.append({"title": title, "link": link, "section": section_name})
            print(f"  [{section_name}] box-contentsì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ì¶”ì¶œ ì™„ë£Œ")
            return articles

        # KBS ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸° (ì—¬ëŸ¬ íŒ¨í„´ ì‹œë„)
        articles = []

        article_patterns = [
            soup.find_all("div", class_=lambda x: x and "news" in str(x).lower()),
            soup.find_all("li", class_=lambda x: x and "news" in str(x).lower()),
            soup.find_all("div", class_=lambda x: x and "list" in str(x).lower()),
            soup.find_all("article"),
            soup.find_all("a", href=lambda x: x and "/news/" in str(x)),
        ]

        found_articles = []
        for pattern in article_patterns:
            if pattern and len(pattern) > 0:
                found_articles = pattern
                break

        print(f"  [{section_name}] í˜ì´ì§€ {page_num}ì—ì„œ {len(found_articles)}ê°œ ìš”ì†Œ ë°œê²¬")

        for article_elem in found_articles:
            try:
                # ì œëª© ì°¾ê¸°
                title = ""
                title_patterns = [
                    article_elem.find("h3"),
                    article_elem.find("h4"),
                    article_elem.find("strong"),
                    article_elem.find("a", title=True),
                    article_elem.find(string=True),
                ]

                for title_elem in title_patterns:
                    if title_elem:
                        if hasattr(title_elem, "get_text"):
                            title = title_elem.get_text().strip()
                        elif hasattr(title_elem, "get"):
                            title = title_elem.get("title", "").strip()
                        else:
                            title = str(title_elem).strip()

                        if title and len(title) > 10:  # ì˜ë¯¸ìˆëŠ” ì œëª©ë§Œ
                            break

                # ë§í¬ ì°¾ê¸°
                link = ""
                link_elem = article_elem.find("a", href=True)
                if link_elem:
                    href = link_elem.get("href")
                    if href:
                        if href.startswith("/"):
                            link = "https://news.kbs.co.kr" + href
                        elif not href.startswith("http"):
                            link = "https://news.kbs.co.kr/" + href
                        else:
                            link = href

                if title and link:
                    articles.append({"title": title, "link": link, "section": section_name})

            except Exception as e:
                continue

        print(f"  [{section_name}] í˜ì´ì§€ {page_num}ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ì¶”ì¶œ")
        return articles

    except TimeoutException:
        print(f"  [{section_name}] í˜ì´ì§€ {page_num} ë¡œë”© íƒ€ì„ì•„ì›ƒ")
        return []
    except Exception as e:
        print(f"  [{section_name}] í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return []


def extract_kbs_article_detail(article_url, section_name):
    """
    KBS ë‰´ìŠ¤ ê°œë³„ ê¸°ì‚¬ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }

    try:
        response = requests.get(article_url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, "html.parser")

        # ì–¸ë¡ ì‚¬ëª…
        media_name = "KBS"

        # ì œëª© ì¶”ì¶œ: headline-title ìš°ì„ 
        title = ""
        headline_elem = soup.find("h4", class_="headline-title")
        if headline_elem:
            title = headline_elem.get_text().strip()
        else:
            # ê¸°ì¡´ ì œëª© íŒ¨í„´
            title_patterns = [
                soup.find("h1"),
                soup.find("h2", class_=lambda x: x and "title" in str(x).lower()),
                soup.find("div", class_=lambda x: x and "title" in str(x).lower()),
            ]
            for title_elem in title_patterns:
                if title_elem:
                    title = title_elem.get_text().strip()
                    if title:
                        break

        # ë‚ ì§œ ì¶”ì¶œ
        date = ""
        date_patterns = [
            soup.find("meta", {"property": "article:published_time"}),
            soup.find("meta", {"name": "article:published_time"}),
            soup.find("time"),
            soup.find("span", class_=lambda x: x and "date" in str(x).lower()),
            soup.find("div", class_=lambda x: x and "date" in str(x).lower()),
        ]

        for date_elem in date_patterns:
            if date_elem:
                date_text = ""
                if date_elem.name == "meta":
                    date_text = date_elem.get("content", "")
                else:
                    date_text = date_elem.get_text().strip()

                if date_text:
                    date = date_text
                    # ISO í˜•ì‹ ë³€í™˜ ì‹œë„
                    try:
                        if "T" in date and ("+" in date or "Z" in date):
                            dt = datetime.fromisoformat(date.replace("Z", "+00:00"))
                            date = dt.strftime("%Y-%m-%d %H:%M:%S")
                    except:
                        pass
                    break

        # ê¸°ìëª… ì¶”ì¶œ
        author = ""
        author_patterns = [
            soup.find("meta", {"name": "author"}),
            soup.find("span", string=re.compile(r"ê¸°ì|íŠ¹íŒŒì›|ì•µì»¤")),
            soup.find("div", string=re.compile(r"ê¸°ì|íŠ¹íŒŒì›|ì•µì»¤")),
            soup.find("p", string=re.compile(r"ê¸°ì|íŠ¹íŒŒì›|ì•µì»¤")),
        ]

        for author_elem in author_patterns:
            if author_elem:
                if author_elem.name == "meta":
                    author = author_elem.get("content", "")
                else:
                    author_text = author_elem.get_text().strip()
                    # ê¸°ìëª… ì¶”ì¶œ
                    author_match = re.search(r"([ê°€-í£]{2,4})\s*(ê¸°ì|íŠ¹íŒŒì›|ì•µì»¤)", author_text)
                    if author_match:
                        author = author_match.group(1)
                    else:
                        author = author_text

                if author:
                    break

        # ë³¸ë¬¸ ì¶”ì¶œ
        content = ""
        content_patterns = [
            soup.find("div", {"id": "cont_newstext"}),
            soup.find("div", class_=lambda x: x and "content" in str(x).lower()),
            soup.find("div", class_=lambda x: x and "article" in str(x).lower()),
            soup.find("div", class_=lambda x: x and "text" in str(x).lower()),
        ]

        for content_elem in content_patterns:
            if content_elem:
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                for script in content_elem(["script", "style", "aside", "nav"]):
                    script.decompose()

                content = content_elem.get_text().strip()
                if content:
                    content = re.sub(r"\s+", " ", content)
                    break

        return {
            "ì–¸ë¡ ì‚¬ëª…": media_name,
            "ì œëª©": title,
            "ë‚ ì§œ": date,
            "ì¹´í…Œê³ ë¦¬": section_name,
            "ê¸°ìëª…": author,
            "ë³¸ë¬¸": content,
        }

    except Exception as e:
        print(f"    ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜ ({article_url}): {e}")
        return {"ì–¸ë¡ ì‚¬ëª…": "KBS", "ì œëª©": "", "ë‚ ì§œ": "", "ì¹´í…Œê³ ë¦¬": section_name, "ê¸°ìëª…": "", "ë³¸ë¬¸": ""}


def crawl_kbs_section(driver, section_key, date_str, max_pages=20):
    """
    KBS íŠ¹ì • ì„¹ì…˜ì˜ ëª¨ë“  í˜ì´ì§€ í¬ë¡¤ë§
    """
    section_info = KBS_SECTIONS[section_key]
    section_code = section_info["code"]
    section_name = section_info["name"]

    print(f"\n{'='*20} [KBS {section_name}] ì„¹ì…˜ í¬ë¡¤ë§ ì‹œì‘ {'='*20}")
    print(f"ë‚ ì§œ: {date_str} (í˜ì´ì§€ ì œí•œ ì—†ìŒ)")

    all_articles = []
    collected_urls = set()

    # í˜ì´ì§€ ë²ˆí˜¸ ë¬´ì œí•œ ë°˜ë³µ
    page = 1
    while True:
        print(f"\n--- [KBS {section_name}] í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ---")

        # í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘
        page_articles = get_kbs_articles_from_page(driver, section_code, section_name, date_str, page)

        if not page_articles:
            print(f"[KBS {section_name}] í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
            break

        # ìƒˆë¡œìš´ ê¸°ì‚¬ë§Œ ì²˜ë¦¬
        new_articles = [art for art in page_articles if art["link"] not in collected_urls]

        if not new_articles:
            print(f"[KBS {section_name}] í˜ì´ì§€ {page}ì— ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
            break

        print(f"[KBS {section_name}] í˜ì´ì§€ {page}ì—ì„œ {len(new_articles)}ê°œ ìƒˆ ê¸°ì‚¬ ë°œê²¬")

        # ê° ê¸°ì‚¬ì˜ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        for i, article in enumerate(new_articles):
            print(f"    [{section_name}] ê¸°ì‚¬ {i+1}/{len(new_articles)} ì²˜ë¦¬ ì¤‘...")

            detail_info = extract_kbs_article_detail(article["link"], section_name)

            if detail_info["ì œëª©"]:  # ì œëª©ì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                all_articles.append(detail_info)
                collected_urls.add(article["link"])

            # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
            time.sleep(0.5)

        print(f"[KBS {section_name}] í˜ì´ì§€ {page} ì™„ë£Œ: {len(new_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        # í˜ì´ì§€ ê°„ ê°„ê²©
        time.sleep(2)
        page += 1
    # ëª¨ë“  í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ
    print(f"\n[KBS {section_name}] ì„¹ì…˜ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
    return all_articles


def crawl_all_kbs_sections(date_str, sections_to_crawl=None, max_pages=20, headless=True):
    """
    KBS ëª¨ë“  ì„¹ì…˜ í¬ë¡¤ë§
    date_str: ë‚ ì§œ ë¬¸ìì—´ (YYYYMMDD í˜•ì‹, ì˜ˆ: '20250815')
    sections_to_crawl: í¬ë¡¤ë§í•  ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ì„¹ì…˜)
    max_pages: ê° ì„¹ì…˜ì—ì„œ í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
    headless: ë¸Œë¼ìš°ì €ë¥¼ ìˆ¨ê¸¸ì§€ ì—¬ë¶€
    """
    if sections_to_crawl is None:
        sections_to_crawl = list(KBS_SECTIONS.keys())

    print("=" * 60)
    print("KBS ë‰´ìŠ¤ ë‹¤ì¤‘ ì„¹ì…˜ í¬ë¡¤ë§ í”„ë¡œê·¸ë¨")
    print(f"í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ: {date_str}")
    print(f"í¬ë¡¤ë§ ëŒ€ìƒ ì„¹ì…˜: {', '.join([KBS_SECTIONS[s]['name'] for s in sections_to_crawl])}")
    print("=" * 60)

    # WebDriver ì„¤ì •
    driver = setup_chrome_driver(headless=headless)
    if not driver:
        print("âŒ WebDriver ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return [], {}

    all_articles = []
    section_results = {}

    try:
        for section_key in sections_to_crawl:
            if section_key not in KBS_SECTIONS:
                print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì„¹ì…˜: {section_key}")
                continue

            try:
                articles = crawl_kbs_section(driver, section_key, date_str, max_pages)
                all_articles.extend(articles)
                section_results[KBS_SECTIONS[section_key]["name"]] = len(articles)

                # ì„¹ì…˜ ê°„ ê°„ê²©
                time.sleep(3)

            except Exception as e:
                print(f"âŒ [KBS {KBS_SECTIONS[section_key]['name']}] ì„¹ì…˜ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                section_results[KBS_SECTIONS[section_key]["name"]] = 0

    finally:
        driver.quit()
        print("\në¸Œë¼ìš°ì € ì¢…ë£Œ")

    return all_articles, section_results


def save_kbs_to_csv(articles_data, date_str, filename=None, split_by_section=False):
    """
    KBS ë‰´ìŠ¤ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
    """
    # ê°•ì œ ë‹¨ì¼ íŒŒì¼ ì €ì¥ (split_by_section ë¬´ì‹œ)
    split_by_section = False

    if not articles_data:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # DataFrame ìƒì„±
    df = pd.DataFrame(articles_data)

    # ì—´ ìˆœì„œ
    column_order = ["ì–¸ë¡ ì‚¬ëª…", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
    df = df[column_order]

    # results ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("results", exist_ok=True)

    saved_files = []

    if split_by_section:
        # ì„¹ì…˜ë³„ë¡œ íŒŒì¼ ì €ì¥
        for category in df["ì¹´í…Œê³ ë¦¬"].unique():
            section_df = df[df["ì¹´í…Œê³ ë¦¬"] == category]
            section_filename = f"results/KBS_{category}_{date_str}.csv"

            section_df.to_csv(section_filename, index=False, encoding="utf-8-sig")

            print(f"âœ“ [KBS {category}] CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {section_filename}")
            print(f"  - {len(section_df)}ê°œ ê¸°ì‚¬, {os.path.getsize(section_filename):,} bytes")
            saved_files.append(section_filename)
    else:
        # í†µí•© íŒŒì¼ë¡œ ì €ì¥
        if filename is None:
            filename = f"results/KBS_ì „ì²´_{date_str}.csv"

        df.to_csv(filename, index=False, encoding="utf-8-sig")

        print(f"âœ“ KBS í†µí•© CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"  - ì´ {len(df)}ê°œ ê¸°ì‚¬, {os.path.getsize(filename):,} bytes")
        saved_files.append(filename)

    return saved_files


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("KBS ë‰´ìŠ¤ í¬ë¡¤ë§ í”„ë¡œê·¸ë¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("\nì‚¬ìš© ê°€ëŠ¥í•œ ì„¹ì…˜:")
    for key, info in KBS_SECTIONS.items():
        print(f"  - {key}: {info['name']} (ì½”ë“œ: {info['code']})")

    # ì‚¬ìš©ì ì„¤ì •: ì˜¤ëŠ˜ ë‚ ì§œ ìë™ ì‚¬ìš©
    date_str = datetime.now().strftime("%Y%m%d")  # í¬ë¡¤ë§í•  ë‚ ì§œ (YYYYMMDD)
    sections_to_crawl = None  # ëª¨ë“  ì„¹ì…˜ (íŠ¹ì • ì„¹ì…˜: ['politics', 'economy'])
    max_pages = 20  # ê° ì„¹ì…˜ë‹¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜
    split_by_section = True  # True: ì„¹ì…˜ë³„ íŒŒì¼, False: í†µí•© íŒŒì¼
    headless = True  # True: ë¸Œë¼ìš°ì € ìˆ¨ê¹€, False: ë¸Œë¼ìš°ì € í‘œì‹œ

    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        articles, section_results = crawl_all_kbs_sections(date_str, sections_to_crawl, max_pages, headless)

        if articles:
            # CSV íŒŒì¼ë¡œ ì €ì¥
            saved_files = save_kbs_to_csv(articles, date_str, split_by_section=split_by_section)

            # ê²°ê³¼ ìš”ì•½
            print("\n" + "=" * 60)
            print("KBS ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ ê²°ê³¼")
            print("=" * 60)
            print(f"âœ“ í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ: {date_str}")
            print(f"âœ“ ì´ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜: {len(articles):,}ê°œ")

            print(f"\nğŸ“Š ì„¹ì…˜ë³„ ìˆ˜ì§‘ ê²°ê³¼:")
            for section, count in section_results.items():
                print(f"  - {section}: {count:,}ê°œ")

            print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
            for file in saved_files:
                print(f"  - {file}")

            # ìƒ˜í”Œ ë°ì´í„° í‘œì‹œ
            df = pd.DataFrame(articles)
            print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
            category_counts = df["ì¹´í…Œê³ ë¦¬"].value_counts()
            for category, count in category_counts.items():
                print(f"  - {category}: {count}ê°œ")

            print(f"\nğŸ“° ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìƒ˜í”Œ (ìµœëŒ€ 3ê°œ):")
            for i in range(min(3, len(df))):
                row = df.iloc[i]
                print(f"{i+1}. [KBS {row['ì¹´í…Œê³ ë¦¬']}] {row['ì œëª©']}")
                print(f"   ê¸°ì: {row['ê¸°ìëª…']}, ë‚ ì§œ: {row['ë‚ ì§œ']}")
                print()

            return saved_files
        else:
            print("âŒ í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

    except Exception as e:
        print(f"âŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


def crawl_kbs_specific_date_sections(date_str, section_list, max_pages=5):

    articles, section_results = crawl_all_kbs_sections(date_str, section_list, max_pages)

    if articles:
        saved_files = save_kbs_to_csv(articles, date_str, split_by_section=True)
        print(f"\nâœ… KBS ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        return saved_files
    else:
        print("âŒ KBS ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨")
        return None


if __name__ == "__main__":
    # ë©”ì¸ í”„ë¡œê·¸ë¨ ì‹¤í–‰
    result_files = main()
