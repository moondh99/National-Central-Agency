import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import csv
import time
import random
import logging
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse


class YonhapNewsTVCrawler:
    def __init__(self):
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[logging.FileHandler("yonhap_newstv_rss_crawler.log", encoding="utf-8"), logging.StreamHandler()],
        )
        self.logger = logging.getLogger(__name__)

        # ì—°í•©ë‰´ìŠ¤TV RSS í”¼ë“œ ëª©ë¡
        self.rss_feeds = {
            "ìµœì‹ ": "http://www.yonhapnewstv.co.kr/browse/feed/",
            "ì •ì¹˜": "http://www.yonhapnewstv.co.kr/category/news/politics/feed/",
            "ê²½ì œ": "http://www.yonhapnewstv.co.kr/category/news/economy/feed/",
            "ì‚¬íšŒ": "http://www.yonhapnewstv.co.kr/category/news/society/feed/",
            "ì§€ì—­": "http://www.yonhapnewstv.co.kr/category/news/local/feed/",
            "ì„¸ê³„": "http://www.yonhapnewstv.co.kr/category/news/international/feed/",
        }

        # User-Agent ëª©ë¡
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        ]

        self.articles = []

    def get_random_headers(self):
        """ëœë¤ User-Agent í—¤ë” ë°˜í™˜"""
        return {
            "User-Agent": random.choice(self.user_agents),
            "Accept": "application/rss+xml,application/xml,text/xml,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Cache-Control": "max-age=0",
        }

    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜"""
        if not text:
            return ""

        # CDATA íƒœê·¸ ì œê±°
        text = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", text, flags=re.DOTALL)

        # HTML íƒœê·¸ ì œê±°
        soup = BeautifulSoup(text, "html.parser")
        text = soup.get_text()

        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
        unwanted_texts = [
            "ê¸°ì‚¬ ì½ì–´ì£¼ê¸° ì„œë¹„ìŠ¤ëŠ” í¬ë¡¬ê¸°ë°˜ì˜ ë¸Œë¼ìš°ì €ì—ì„œë§Œ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "â“’ì—°í•©ë‰´ìŠ¤TV, ë¬´ë‹¨ ì „ì¬-ì¬ë°°í¬, AI í•™ìŠµ ë° í™œìš© ê¸ˆì§€",
            "ì—°í•©ë‰´ìŠ¤TV ì œê³µ",
            "ë¬´ë‹¨ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€",
            "Copyright by Yonhap News TV",
            "ì €ì‘ê¶Œì",
            "â–¶ ì—°í•©ë‰´ìŠ¤TV",
            "í™ˆí˜ì´ì§€ì—ì„œ ì‹œì²­í•˜ì„¸ìš”",
            "ë‰´ìŠ¤ë£¸ ì œë³´",
            "AI í•™ìŠµ ë° í™œìš© ê¸ˆì§€",
            "ë¬´ë‹¨ ì „ì¬-ì¬ë°°í¬",
            "â“’ì—°í•©ë‰´ìŠ¤TV",
        ]

        for unwanted in unwanted_texts:
            text = text.replace(unwanted, "")

        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€í™˜
        text = re.sub(r"\s+", " ", text)

        return text.strip()

    def extract_reporter_name(self, title, description=""):
        """RSS ì œëª©ê³¼ descriptionì—ì„œ ê¸°ìëª… ì¶”ì¶œ"""
        # ì œëª©ê³¼ descriptionì„ í•©ì³ì„œ ê²€ìƒ‰
        combined_text = f"{title} {description}"

        # ì—°í•©ë‰´ìŠ¤TV ê¸°ìëª… íŒ¨í„´ë“¤
        patterns = [
            r"([ê°€-í£]{2,4})\s*ê¸°ì",
            r"ê¸°ì\s*([ê°€-í£]{2,4})",
            r"([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›",
            r"([ê°€-í£]{2,4})\s*ì•„ë‚˜ìš´ì„œ",
            r"ì—°í•©ë‰´ìŠ¤TV\s*([ê°€-í£]{2,4})",
            r"ì—°í•©ë‰´ìŠ¤\s*([ê°€-í£]{2,4})",
            r"ë¦¬í¬í„°\s*([ê°€-í£]{2,4})",
            r"([ê°€-í£]{2,4})\s*ë¦¬í¬í„°",
            r"ì•µì»¤\s*([ê°€-í£]{2,4})",
            r"([ê°€-í£]{2,4})\s*ì•µì»¤",
            r"PD\s*([ê°€-í£]{2,4})",
            r"([ê°€-í£]{2,4})\s*PD",
        ]

        for pattern in patterns:
            match = re.search(pattern, combined_text)
            if match:
                return match.group(1) + " ê¸°ì"

        return "ê¸°ìëª… ì—†ìŒ"

    def format_date(self, date_str):
        """ë‚ ì§œ í˜•ì‹ ë³€í™˜"""
        try:
            # RFC 2822 í˜•ì‹ íŒŒì‹± ì‹œë„
            dt = datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S %z")
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        except:
            try:
                # GMT ì—†ëŠ” í˜•ì‹ ì‹œë„
                dt = datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S GMT")
                return dt.strftime("%Y-%m-%d %H:%M:%S")
            except:
                try:
                    # KST í˜•ì‹ ì‹œë„
                    dt = datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S KST")
                    return dt.strftime("%Y-%m-%d %H:%M:%S")
                except:
                    try:
                        # +0900 í˜•ì‹ ì‹œë„
                        dt = datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S +0900")
                        return dt.strftime("%Y-%m-%d %H:%M:%S")
                    except:
                        try:
                            # ë‹¤ë¥¸ í˜•ì‹ ì‹œë„
                            dt = datetime.strptime(date_str[:19], "%Y-%m-%d %H:%M:%S")
                            return dt.strftime("%Y-%m-%d %H:%M:%S")
                        except:
                            return date_str

    def get_content_from_description(self, description):
        """RSS descriptionì—ì„œ ë‚´ìš© ì¶”ì¶œ (ì›¹í˜ì´ì§€ ì ‘ê·¼ ëŒ€ì‹ )"""
        if not description:
            return "RSS description ì—†ìŒ"

        # description ì •ì œ
        content = self.clean_text(description)

        # ë„ˆë¬´ ì§§ì€ ê²½ìš°
        if len(content) < 20:
            return "RSS descriptionì´ ë„ˆë¬´ ì§§ìŒ"

        return content

    def parse_rss_feed(self, category, url):
        """RSS í”¼ë“œ íŒŒì‹± - description í™œìš©"""
        try:
            self.logger.info(f"{category} RSS í”¼ë“œ í¬ë¡¤ë§ ì‹œì‘: {url}")

            headers = self.get_random_headers()
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()

            # ì‘ë‹µì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if not response.content.strip():
                self.logger.error(f"{category} RSS í”¼ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {url}")
                return []

            # XML íŒŒì‹±
            try:
                root = ET.fromstring(response.content)
            except ET.ParseError as e:
                self.logger.error(f"{category} RSS XML íŒŒì‹± ì˜¤ë¥˜: {e}")
                return []

            # RSS ì•„ì´í…œ ì¶”ì¶œ
            items = root.findall(".//item")

            if not items:
                self.logger.warning(f"{category} RSS í”¼ë“œì—ì„œ ì•„ì´í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []

            articles = []

            # ìµœëŒ€ 20ê°œ ê¸°ì‚¬ë§Œ ì²˜ë¦¬
            for i, item in enumerate(items[:20]):
                try:
                    title_elem = item.find("title")
                    link_elem = item.find("link")
                    pubdate_elem = item.find("pubDate")
                    description_elem = item.find("description")

                    if title_elem is None or link_elem is None:
                        continue

                    title = self.clean_text(title_elem.text)
                    link = link_elem.text.strip()
                    pub_date = self.format_date(pubdate_elem.text) if pubdate_elem is not None else ""
                    description = self.clean_text(description_elem.text) if description_elem is not None else ""
                    # RSS content:encodedì—ì„œ ë³¸ë¬¸ ìš°ì„  ì¶”ì¶œ
                    content_enc = item.find("content:encoded") or item.find(
                        "{http://purl.org/rss/1.0/modules/content/}encoded"
                    )
                    if content_enc is not None and content_enc.text:
                        raw_html = content_enc.text
                        content = self.clean_text(raw_html)
                    else:
                        self.logger.info(f"RSS description í™œìš©: {title[:30]}...")
                        content = self.get_content_from_description(description)
                    # RSS dc:creator íƒœê·¸ì—ì„œ ê¸°ìëª… ì¶”ì¶œ ìš°ì„ 
                    creator_elem = item.find("dc:creator") or item.find("{http://purl.org/dc/elements/1.1/}creator")
                    if creator_elem is not None and creator_elem.text:
                        reporter = creator_elem.text.strip()
                    else:
                        reporter = self.extract_reporter_name(title, description)

                    article = {
                        "title": title,
                        "category": category,
                        "date": pub_date,
                        "reporter": reporter,
                        "content": content,
                        "url": link,
                    }

                    articles.append(article)

                    # RSSë§Œ ì²˜ë¦¬í•˜ë¯€ë¡œ ë”œë ˆì´ ë‹¨ì¶•
                    time.sleep(random.uniform(1, 2))

                except Exception as e:
                    self.logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue

            self.logger.info(f"{category} ì¹´í…Œê³ ë¦¬ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
            return articles

        except requests.RequestException as e:
            self.logger.error(f"{category} RSS í”¼ë“œ ìš”ì²­ ì˜¤ë¥˜: {e}")
            return []
        except Exception as e:
            self.logger.error(f"{category} RSS í”¼ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    def crawl_all_feeds(self):
        """ëª¨ë“  RSS í”¼ë“œ í¬ë¡¤ë§"""
        self.logger.info("ì—°í•©ë‰´ìŠ¤TV RSS í¬ë¡¤ë§ ì‹œì‘ (RSS Description í™œìš©)")

        for category, url in self.rss_feeds.items():
            self.logger.info(f"\n=== {category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ===")

            articles = self.parse_rss_feed(category, url)
            self.articles.extend(articles)

            # ì¹´í…Œê³ ë¦¬ ê°„ ë”œë ˆì´ (ë‹¨ì¶•)
            time.sleep(random.uniform(2, 4))

        self.logger.info(f"\nì´ {len(self.articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")

    def save_to_csv(self, filename=None):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        # í•˜ë‚˜ì˜ CSV íŒŒì¼ ì €ì¥: ì–¸ë¡ ì‚¬_ì „ì²´_{timestamp}.csv
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"results/ì—°í•©ë‰´ìŠ¤TV_ì „ì²´_{timestamp}.csv"
        try:
            # rows êµ¬ì„±: ì–¸ë¡ ì‚¬, ì œëª©, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, ê¸°ìëª…, ë³¸ë¬¸
            rows = []
            for article in self.articles:
                rows.append(
                    {
                        "ì–¸ë¡ ì‚¬": "ì—°í•©ë‰´ìŠ¤TV",
                        "ì œëª©": article.get("title", ""),
                        "ë‚ ì§œ": article.get("date", ""),
                        "ì¹´í…Œê³ ë¦¬": article.get("category", ""),
                        "ê¸°ìëª…": article.get("reporter", ""),
                        "ë³¸ë¬¸": article.get("content", ""),
                    }
                )
            with open(filename, "w", newline="", encoding="utf-8-sig") as csvfile:
                fieldnames = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(rows)

            self.logger.info(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {filename}")
            print(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(self.articles)}ê°œ")

        except Exception as e:
            self.logger.error(f"CSV íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = YonhapNewsTVCrawler()

    print("ğŸš€ ì—°í•©ë‰´ìŠ¤TV RSS í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ğŸ“° RSS Descriptionì„ í™œìš©í•œ ë¹ ë¥¸ í¬ë¡¤ë§")
    print("ğŸ“° ìˆ˜ì§‘ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬: ìµœì‹ , ì •ì¹˜, ê²½ì œ, ì‚¬íšŒ, ì§€ì—­, ì„¸ê³„, ë¬¸í™”ì—°ì˜ˆ, ìŠ¤í¬ì¸ , ë‚ ì”¨")
    print("â³ RSSë§Œ ì²˜ë¦¬í•˜ë¯€ë¡œ ë¹ ë¥´ê²Œ ì™„ë£Œë©ë‹ˆë‹¤.\n")

    try:
        # RSS í”¼ë“œ í¬ë¡¤ë§
        crawler.crawl_all_feeds()

        # CSV íŒŒì¼ë¡œ ì €ì¥
        crawler.save_to_csv()

        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        if crawler.articles:
            print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ í˜„í™©:")
            category_count = {}
            for article in crawler.articles:
                category = article["category"]
                category_count[category] = category_count.get(category, 0) + 1

            for category, count in category_count.items():
                print(f"   â€¢ {category}: {count}ê°œ")

    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.error(f"ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    main()
