import requests
from bs4 import BeautifulSoup
import csv
import re
from datetime import datetime
import time
from urllib.parse import urljoin


def extract_mbc_article_content(url):
    """MBC ë‰´ìŠ¤ ê¸°ì‚¬ URLì—ì„œ ì „ì²´ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        response.encoding = "utf-8"

        soup = BeautifulSoup(response.text, "html.parser")

        # MBC ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„)
        content_selectors = [
            "div.news-content",  # ë‰´ìŠ¤ ì»¨í…ì¸ 
            "div.article-content",  # ê¸°ì‚¬ ì»¨í…ì¸ 
            "div.content",  # ì»¨í…ì¸ 
            "div.article-body",  # ê¸°ì‚¬ ë³¸ë¬¸
            "div.news-text",  # ë‰´ìŠ¤ í…ìŠ¤íŠ¸
            ".news_txt",  # ë‰´ìŠ¤ í…ìŠ¤íŠ¸ í´ë˜ìŠ¤
            "div.view-content",  # ë·° ì»¨í…ì¸ 
            "#content",  # ID ê¸°ë°˜ ì»¨í…ì¸ 
            "div.text_area",  # í…ìŠ¤íŠ¸ ì˜ì—­
            "section.article-content",  # ì„¹ì…˜ ê¸°ì‚¬ ì»¨í…ì¸ 
            "div.detail-content",  # ìƒì„¸ ì»¨í…ì¸ 
        ]

        full_content = ""

        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                for element in elements:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ ì œê±°
                    for unwanted in element.find_all(
                        [
                            "script",
                            "style",
                            "iframe",
                            "ins",
                            "div.ad",
                            ".advertisement",
                            ".related-articles",
                            ".tags",
                            ".share",
                            ".comment",
                            ".footer",
                            "div.reporter",
                            ".reporter_info",
                            ".social",
                            ".video",
                            ".photo",
                            ".image",
                            ".btn",
                        ]
                    ):
                        unwanted.decompose()

                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text = element.get_text(separator="\n", strip=True)
                    if text and len(text) > len(full_content):
                        full_content = text
                        break

                if full_content:
                    break

        # ë³¸ë¬¸ì´ ì—¬ì „íˆ ì§§ë‹¤ë©´ ì „ì²´ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if len(full_content) < 100:
            # MBC ê¸°ì‚¬ì˜ ê²½ìš° HTML êµ¬ì¡°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
            page_text = soup.get_text(separator="\n", strip=True)

            # ê¸°ì‚¬ ì‹œì‘ê³¼ ëì„ ì°¾ê¸°
            start_markers = ["â—€ ì•µì»¤ â–¶", "â—€ ë¦¬í¬íŠ¸ â–¶", "[ì•µì»¤]", "[ê¸°ì]"]
            end_markers = ["MBC ë‰´ìŠ¤ëŠ” 24ì‹œê°„", "â–· ì „í™”", "â–· ì´ë©”ì¼", "â–· ì¹´ì¹´ì˜¤í†¡"]

            lines = page_text.split("\n")
            content_lines = []
            in_content = False

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                # ê¸°ì‚¬ ì‹œì‘ ë§ˆì»¤ ì°¾ê¸°
                if any(marker in line for marker in start_markers):
                    in_content = True
                    content_lines.append(line)
                    continue

                # ê¸°ì‚¬ ë ë§ˆì»¤ ì°¾ê¸°
                if any(marker in line for marker in end_markers):
                    break

                if in_content:
                    # ë¶ˆí•„ìš”í•œ ë¼ì¸ ì œì™¸
                    if (
                        not line.startswith("â–·")
                        and not line.startswith("â€»")
                        and "MBC" not in line
                        and "ì œë³´" not in line
                        and len(line) > 5
                    ):
                        content_lines.append(line)

            if content_lines:
                full_content = "\n".join(content_lines)

        # í…ìŠ¤íŠ¸ ì •ë¦¬
        if full_content:
            # MBC ê´€ë ¨ ì •ë³´ ë° ì œë³´ ì •ë³´ ì œê±°
            full_content = re.sub(r"MBC ë‰´ìŠ¤ëŠ” 24ì‹œê°„.*?@mbcì œë³´", "", full_content, flags=re.DOTALL)
            full_content = re.sub(r"â–· ì „í™”.*?ì¹´ì¹´ì˜¤í†¡.*?@mbcì œë³´", "", full_content, flags=re.DOTALL)
            full_content = re.sub(r"ì˜ìƒì·¨ì¬:.*?ì˜ìƒí¸ì§‘:.*?$", "", full_content, flags=re.MULTILINE)
            # ê¸°ìëª… ë¼ì¸ ì œê±° (ë§ˆì§€ë§‰ì— ìˆëŠ” ê²½ìš°)
            full_content = re.sub(r"\n[ê°€-í£]{2,4}\s*(ê¸°ì|íŠ¹íŒŒì›).*?$", "", full_content, flags=re.MULTILINE)
            # ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
            full_content = re.sub(r"\n+", "\n", full_content)
            full_content = re.sub(r"\s+", " ", full_content)
            full_content = full_content.strip()

        return full_content

    except Exception as e:
        print(f"ë³¸ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""


def extract_mbc_reporter_name(soup, article_text):
    """MBC ë‰´ìŠ¤ ê¸°ìëª…ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        # MBC ë‰´ìŠ¤ì˜ ê¸°ìëª… ì¶”ì¶œ íŒ¨í„´
        reporter_patterns = [
            # HTMLì—ì„œ ê¸°ìëª… ì¶”ì¶œ
            r"<span[^>]*class[^>]*reporter[^>]*>([^<]+)</span>",
            r"<div[^>]*class[^>]*reporter[^>]*>([^<]+)</div>",
            r"<p[^>]*class[^>]*reporter[^>]*>([^<]+)</p>",
            # í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìëª… ì¶”ì¶œ (MBC íŠ¹ì„±ì— ë§ê²Œ)
            r"MBCë‰´ìŠ¤\s*([ê°€-í£]{2,4})ì…ë‹ˆë‹¤",  # MBCë‰´ìŠ¤ ê¹€ì¬ìš©ì…ë‹ˆë‹¤
            r"([ê°€-í£]{2,4})\s*(ê¸°ì|íŠ¹íŒŒì›)ì…ë‹ˆë‹¤",
            r"ì—ì„œ\s*MBCë‰´ìŠ¤\s*([ê°€-í£]{2,4})ì…ë‹ˆë‹¤",  # ì›Œì‹±í„´ì—ì„œ MBCë‰´ìŠ¤ ê¹€ì¬ìš©ì…ë‹ˆë‹¤
            r"([ê°€-í£]{2,4})\s*(ê¸°ì|íŠ¹íŒŒì›)(?:\s*=|\s*âˆ™|\s*Â·|\s*ì…ë ¥|\s*ìˆ˜ì •|\s*ì‘ì„±)",
            r"ê¸°ì\s*([ê°€-í£]{2,4})(?:\s*=|\s*âˆ™|\s*Â·)",
            r"([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›",
            r"([ê°€-í£]{2,4})\s*ì•µì»¤",
            r"([ê°€-í£]{2,4})\s*ë…¼ì„¤ìœ„ì›",
            r"([ê°€-í£]{2,4})\s*í¸ì§‘ìœ„ì›",
            r"/\s*([ê°€-í£]{2,4})\s*ê¸°ì",
            r"=\s*([ê°€-í£]{2,4})\s*ê¸°ì",
            r"âˆ™\s*([ê°€-í£]{2,4})\s*ê¸°ì",
            r"Â·\s*([ê°€-í£]{2,4})\s*ê¸°ì",
            r"ê¸°ì\s*:\s*([ê°€-í£]{2,4})",
            r"\[([ê°€-í£]{2,4})\s*ê¸°ì\]",
            r"^([ê°€-í£]{2,4})\s*ê¸°ì",  # ì¤„ ì‹œì‘ì—ì„œ ê¸°ìëª…
            r"ì·¨ì¬\s*:\s*([ê°€-í£]{2,4})",  # ì·¨ì¬: ê¸°ìëª…
            r"ì˜ìƒì·¨ì¬\s*:\s*([ê°€-í£]{2,4})",  # ì˜ìƒì·¨ì¬: ê¸°ìëª…
        ]

        # BeautifulSoup ê°ì²´ì—ì„œ ê¸°ìëª… ì°¾ê¸°
        if soup:
            # ê¸°ìëª…ì´ í¬í•¨ë  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ìš”ì†Œë“¤ ì°¾ê¸°
            reporter_elements = soup.find_all(["span", "div", "p"], string=re.compile(r"ê¸°ì|íŠ¹íŒŒì›|ì•µì»¤"))
            for element in reporter_elements:
                text = element.get_text(strip=True)
                if ("ê¸°ì" in text or "íŠ¹íŒŒì›" in text or "ì•µì»¤" in text) and "MBC" in text:
                    match = re.search(r"([ê°€-í£]{2,4})", text)
                    if match:
                        name = match.group(1)
                        if "ê¸°ì" in text:
                            return name + " ê¸°ì"
                        elif "íŠ¹íŒŒì›" in text:
                            return name + " íŠ¹íŒŒì›"
                        elif "ì•µì»¤" in text:
                            return name + " ì•µì»¤"

        # ê¸°ì‚¬ í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìëª… ì°¾ê¸°
        full_text = str(soup) + "\n" + article_text if soup else article_text

        for pattern in reporter_patterns:
            matches = re.findall(pattern, full_text)
            if matches:
                if isinstance(matches[0], tuple):
                    reporter = matches[0][0].strip()
                    role = matches[0][1] if len(matches[0]) > 1 else "ê¸°ì"
                else:
                    reporter = matches[0].strip()
                    role = "ê¸°ì"

                if reporter and len(reporter) >= 2:
                    return reporter + f" {role}" if role not in reporter else reporter

        return "ê¸°ìëª… ì—†ìŒ"

    except Exception as e:
        print(f"ê¸°ìëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return "ê¸°ìëª… ì—†ìŒ"


def get_mbc_news_list(base_url="https://imnews.imbc.com", categories=None, max_pages=3):
    """MBC ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜. categories ë¦¬ìŠ¤íŠ¸ ì§€ì • ì‹œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ë§Œ ìˆ˜ì§‘"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        news_items = []

        # ì¹´í…Œê³ ë¦¬ë³„ URL íŒ¨í„´ (categories ì§€ì • ì‹œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ë§Œ ì‚¬ìš©)
        suffix_map = {
            "politics": "politics",
            "society": "society",
            "economy": "econo",
            "international": "world",
            "culture": "culture",
            "sports": "sports",
        }
        if categories:
            category_urls = [f"{base_url}/news/2025/{suffix_map.get(cat, cat)}/" for cat in categories]
        else:
            # ê¸°ë³¸: ëª¨ë“  ì¹´í…Œê³ ë¦¬ì™€ ë©”ì¸ í˜ì´ì§€ í¬í•¨
            all_cats = list(suffix_map.values())
            category_urls = [f"{base_url}/news/2025/{cat}/" for cat in all_cats]
            category_urls.insert(0, f"{base_url}/")

        for category_url in category_urls:
            try:
                print(f"ğŸ“„ {category_url} í˜ì´ì§€ ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")

                response = requests.get(category_url, headers=headers, timeout=15)
                response.encoding = "utf-8"
                soup = BeautifulSoup(response.text, "html.parser")

                # ë‰´ìŠ¤ ë§í¬ë“¤ ì°¾ê¸° (ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„)
                link_selectors = [
                    'a[href*="/article/"]',  # ê¸°ì‚¬ ë§í¬
                    'a[href*="_"]',  # MBC ê¸°ì‚¬ ë§í¬ íŒ¨í„´
                    "h3 a, h2 a, .title a",  # ì œëª© ë§í¬
                    ".news-list a",  # ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ë§í¬
                    ".headline a",  # í—¤ë“œë¼ì¸ ë§í¬
                    ".article-list a",  # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ë§í¬
                ]

                page_links = []
                for selector in link_selectors:
                    links = soup.select(selector)
                    if links:
                        page_links.extend(links)

                # ì¤‘ë³µ ì œê±° ë° ìœ íš¨í•œ ë§í¬ë§Œ ì„ ë³„
                seen_urls = set()
                for link in page_links:
                    href = link.get("href")
                    if href and ("article" in href or "_" in href.split("/")[-1]):
                        full_url = urljoin(base_url, href)
                        if full_url not in seen_urls and "imnews.imbc.com" in full_url:
                            seen_urls.add(full_url)

                            # ì œëª© ì¶”ì¶œ
                            title = link.get_text(strip=True)
                            if not title:
                                # ë¶€ëª¨ ìš”ì†Œì—ì„œ ì œëª© ì°¾ê¸°
                                title_elem = link.find_parent().find(["h1", "h2", "h3", "h4"])
                                if title_elem:
                                    title = title_elem.get_text(strip=True)

                            if title and len(title) > 5:  # ë„ˆë¬´ ì§§ì€ ì œëª© ì œì™¸
                                news_items.append({"url": full_url, "title": title[:100]})  # ì œëª© ê¸¸ì´ ì œí•œ

                print(f"  â¤ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ë°œê²¬")
                time.sleep(1)  # í˜ì´ì§€ ìš”ì²­ ê°„ ë”œë ˆì´

            except Exception as e:
                print(f"  â¤ ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        # ì¤‘ë³µ ì œê±°
        unique_news = []
        seen_urls = set()
        for item in news_items:
            if item["url"] not in seen_urls:
                seen_urls.add(item["url"])
                unique_news.append(item)

        print(f"ğŸ“Š ì´ {len(unique_news)}ê°œì˜ ê³ ìœ í•œ ë‰´ìŠ¤ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        return unique_news

    except Exception as e:
        print(f"ë‰´ìŠ¤ ëª©ë¡ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return []


def scrape_mbc_news(max_articles=50, categories=None):
    """MBC ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ì—¬ CSVë¡œ ì €ì¥í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""

    print("ğŸ—ï¸  MBC ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 60)

    try:
        # ë‰´ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        news_list = get_mbc_news_list(categories=categories)

        if not news_list:
            print("âŒ ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        news_data = []
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        print(f"\nğŸ“° {len(news_list)}ê°œ ê¸°ì‚¬ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")

        for i, news_item in enumerate(news_list):
            try:
                url = news_item["url"]
                base_title = news_item["title"]

                print(f"[{i+1}/{len(news_list)}] ì²˜ë¦¬ ì¤‘: {base_title[:50]}...")

                # ê°œë³„ ê¸°ì‚¬ í˜ì´ì§€ í¬ë¡¤ë§
                response = requests.get(url, headers=headers, timeout=20)
                response.encoding = "utf-8"
                soup = BeautifulSoup(response.text, "html.parser")

                # ì œëª© ì¶”ì¶œ (ë” ì •í™•í•œ ì œëª©)
                title = base_title
                title_selectors = ["h1.title", "h1", ".news_title", ".article_title", "title"]
                for selector in title_selectors:
                    title_elem = soup.select_one(selector)
                    if title_elem:
                        extracted_title = title_elem.get_text(strip=True)
                        if extracted_title and len(extracted_title) > len(title):
                            title = extracted_title
                        break

                # ë‚ ì§œ ì¶”ì¶œ (URLì´ë‚˜ í˜ì´ì§€ì—ì„œ)
                date_text = "ë‚ ì§œ ì—†ìŒ"

                # URLì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
                url_date_match = re.search(r"/(\d{4})/|(\d{4})[-/](\d{1,2})[-/](\d{1,2})", url)
                if url_date_match:
                    if url_date_match.group(1):  # /2025/ í˜•íƒœ
                        date_text = f"{url_date_match.group(1)}-{datetime.now().month:02d}-{datetime.now().day:02d}"
                    else:  # 2025-06-28 í˜•íƒœ
                        year, month, day = url_date_match.groups()[1:]
                        date_text = f"{year}-{month.zfill(2)}-{day.zfill(2)}"

                # í˜ì´ì§€ì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
                page_text = soup.get_text()
                date_patterns = [
                    r"(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼",
                    r"(\d{4})-(\d{1,2})-(\d{1,2})\s*(\d{1,2}):(\d{2})",
                    r"(\d{4})\.(\d{1,2})\.(\d{1,2})",
                    r"(\d{4})/(\d{1,2})/(\d{1,2})",
                ]

                for pattern in date_patterns:
                    match = re.search(pattern, page_text)
                    if match:
                        groups = match.groups()
                        if len(groups) >= 3:
                            year, month, day = groups[0], groups[1], groups[2]
                            if len(groups) >= 5:
                                hour, minute = groups[3], groups[4]
                                date_text = f"{year}-{month.zfill(2)}-{day.zfill(2)} {hour.zfill(2)}:{minute}"
                            else:
                                date_text = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                        break

                if date_text == "ë‚ ì§œ ì—†ìŒ":
                    date_text = datetime.now().strftime("%Y-%m-%d %H:%M")

                # ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ
                full_content = extract_mbc_article_content(url)

                # ê¸°ìëª… ì¶”ì¶œ
                reporter_name = extract_mbc_reporter_name(soup, full_content)

                # ë°ì´í„° ì €ì¥
                if full_content.strip():  # ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš°ë§Œ ì €ì¥
                    news_data.append(
                        {"ì œëª©": title.strip(), "ë‚ ì§œ": date_text, "ê¸°ìëª…": reporter_name, "ë³¸ë¬¸": full_content}
                    )
                else:
                    print(f"  â¤ ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")

                # ì„œë²„ ë¶€í•˜ ë°©ì§€
                time.sleep(1.5)

            except Exception as e:
                print(f"  â¤ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        # CSV íŒŒì¼ë¡œ ì €ì¥ì€ ë©”ì¸ì—ì„œ ì²˜ë¦¬
        # í†µê³„ ì •ë³´ ì¶œë ¥
        total_chars = sum(len(item["ë³¸ë¬¸"]) for item in news_data)
        avg_chars = total_chars // len(news_data) if news_data else 0
        print(f"ğŸ“Š í‰ê·  ë³¸ë¬¸ ê¸¸ì´: {avg_chars:,}ì")
        print(f"ğŸ“Š ì´ ë³¸ë¬¸ ê¸¸ì´: {total_chars:,}ì")
        # ë‰´ìŠ¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return news_data
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


def scrape_mbc_by_category(category=None):
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ MBC ë‰´ìŠ¤ í¬ë¡¤ë§"""

    category_mapping = {
        "politics": "ì •ì¹˜",
        "society": "ì‚¬íšŒ",
        "economy": "ê²½ì œ",
        "international": "êµ­ì œ",
        "culture": "ë¬¸í™”",
        "sports": "ìŠ¤í¬ì¸ ",
    }

    if category and category not in category_mapping:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤.")
        print(f"âœ… ì§€ì› ì¹´í…Œê³ ë¦¬: {', '.join(category_mapping.keys())}")
        return None

    if category:
        print(f"ğŸ“° MBC {category_mapping[category]} ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    else:
        print("ğŸ“° MBC ì „ì²´ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")

    # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ë§Œ ìˆ˜ì§‘ (ì œí•œ ì—†ì´ ì „ì²´ ìˆ˜ì§‘)
    return scrape_mbc_news(categories=[category] if category else None)


if __name__ == "__main__":
    # ìë™ìœ¼ë¡œ ê° ì¹´í…Œê³ ë¦¬ë³„ 20ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ í›„ í•˜ë‚˜ì˜ CSVë¡œ ì €ì¥
    categories = [
        ("politics", "ì •ì¹˜"),
        ("society", "ì‚¬íšŒ"),
        ("economy", "ê²½ì œ"),
        ("international", "êµ­ì œ"),
        ("culture", "ë¬¸í™”"),
    ]
    all_articles = []
    for key, kor in categories:
        print(f"\n=== MBC {kor} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ ===")
        data = scrape_mbc_by_category(key)
        for item in data:
            item["ì¹´í…Œê³ ë¦¬"] = kor
        all_articles.extend(data)
    # CSV ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"results/MBC_ì „ì²´_{timestamp}.csv"
    with open(filename, "w", newline="", encoding="utf-8-sig") as csvfile:
        writer = csv.writer(csvfile)
        # ì–¸ë¡ ì‚¬, ì œëª©, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, ê¸°ìëª…, ë³¸ë¬¸
        writer.writerow(["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"])
        for art in all_articles:
            writer.writerow(
                [
                    "MBC",  # ì–¸ë¡ ì‚¬
                    art.get("ì œëª©", ""),
                    art.get("ë‚ ì§œ", ""),
                    art.get("ì¹´í…Œê³ ë¦¬", ""),
                    "mbc",  # ê¸°ìëª… í†µì¼
                    art.get("ë³¸ë¬¸", ""),
                ]
            )
    print(f"\nâœ… MBC ë‰´ìŠ¤ {len(all_articles)}ê°œ ìë™ ì €ì¥ ì™„ë£Œ: {filename}")
