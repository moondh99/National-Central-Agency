import requests
from bs4 import BeautifulSoup
import csv
import re
from datetime import datetime
import time
from urllib.parse import urljoin

def extract_mbc_article_content(url):
    """MBC ë‰´ìŠ¤ ê¸°ì‚¬ URLì—ì„œ ì „ì²´ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # MBC ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„)
        content_selectors = [
            'div.news-content',          # ë‰´ìŠ¤ ì»¨í…ì¸ 
            'div.article-content',       # ê¸°ì‚¬ ì»¨í…ì¸ 
            'div.content',               # ì»¨í…ì¸ 
            'div.article-body',          # ê¸°ì‚¬ ë³¸ë¬¸
            'div.news-text',             # ë‰´ìŠ¤ í…ìŠ¤íŠ¸
            '.news_txt',                 # ë‰´ìŠ¤ í…ìŠ¤íŠ¸ í´ë˜ìŠ¤
            'div.view-content',          # ë·° ì»¨í…ì¸ 
            '#content',                  # ID ê¸°ë°˜ ì»¨í…ì¸ 
            'div.text_area',             # í…ìŠ¤íŠ¸ ì˜ì—­
            'section.article-content',   # ì„¹ì…˜ ê¸°ì‚¬ ì»¨í…ì¸ 
            'div.detail-content'         # ìƒì„¸ ì»¨í…ì¸ 
        ]
        
        full_content = ""
        
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                for element in elements:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ ì œê±°
                    for unwanted in element.find_all([
                        'script', 'style', 'iframe', 'ins', 
                        'div.ad', '.advertisement', '.related-articles',
                        '.tags', '.share', '.comment', '.footer',
                        'div.reporter', '.reporter_info', '.social',
                        '.video', '.photo', '.image', '.btn'
                    ]):
                        unwanted.decompose()
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text = element.get_text(separator='\n', strip=True)
                    if text and len(text) > len(full_content):
                        full_content = text
                        break
                
                if full_content:
                    break
        
        # ë³¸ë¬¸ì´ ì—¬ì „íˆ ì§§ë‹¤ë©´ ì „ì²´ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if len(full_content) < 100:
            # MBC ê¸°ì‚¬ì˜ ê²½ìš° HTML êµ¬ì¡°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
            page_text = soup.get_text(separator='\n', strip=True)
            
            # ê¸°ì‚¬ ì‹œì‘ê³¼ ëì„ ì°¾ê¸°
            start_markers = ['â—€ ì•µì»¤ â–¶', 'â—€ ë¦¬í¬íŠ¸ â–¶', '[ì•µì»¤]', '[ê¸°ì]']
            end_markers = ['MBC ë‰´ìŠ¤ëŠ” 24ì‹œê°„', 'â–· ì „í™”', 'â–· ì´ë©”ì¼', 'â–· ì¹´ì¹´ì˜¤í†¡']
            
            lines = page_text.split('\n')
            content_lines = []
            in_content = False
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # ê¸°ì‚¬ ì‹œì‘ ë§ˆì»¤ ì°¾ê¸°
                if any(marker in line for marker in start_markers):
                    in_content = True
                    content_lines.append(line)
                    continue
                
                # ê¸°ì‚¬ ë ë§ˆì»¤ ì°¾ê¸°
                if any(marker in line for marker in end_markers):
                    break
                
                if in_content:
                    # ë¶ˆí•„ìš”í•œ ë¼ì¸ ì œì™¸
                    if (not line.startswith('â–·') and 
                        not line.startswith('â€»') and
                        'MBC' not in line and
                        'ì œë³´' not in line and
                        len(line) > 5):
                        content_lines.append(line)
            
            if content_lines:
                full_content = '\n'.join(content_lines)
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        if full_content:
            # MBC ê´€ë ¨ ì •ë³´ ë° ì œë³´ ì •ë³´ ì œê±°
            full_content = re.sub(r'MBC ë‰´ìŠ¤ëŠ” 24ì‹œê°„.*?@mbcì œë³´', '', full_content, flags=re.DOTALL)
            full_content = re.sub(r'â–· ì „í™”.*?ì¹´ì¹´ì˜¤í†¡.*?@mbcì œë³´', '', full_content, flags=re.DOTALL)
            full_content = re.sub(r'ì˜ìƒì·¨ì¬:.*?ì˜ìƒí¸ì§‘:.*?$', '', full_content, flags=re.MULTILINE)
            # ê¸°ìëª… ë¼ì¸ ì œê±° (ë§ˆì§€ë§‰ì— ìˆëŠ” ê²½ìš°)
            full_content = re.sub(r'\n[ê°€-í£]{2,4}\s*(ê¸°ì|íŠ¹íŒŒì›).*?$', '', full_content, flags=re.MULTILINE)
            # ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
            full_content = re.sub(r'\n+', '\n', full_content)
            full_content = re.sub(r'\s+', ' ', full_content)
            full_content = full_content.strip()
        
        return full_content
        
    except Exception as e:
        print(f"ë³¸ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""

def extract_mbc_reporter_name(soup, article_text):
    """MBC ë‰´ìŠ¤ ê¸°ìëª…ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        # MBC ë‰´ìŠ¤ì˜ ê¸°ìëª… ì¶”ì¶œ íŒ¨í„´
        reporter_patterns = [
            # HTMLì—ì„œ ê¸°ìëª… ì¶”ì¶œ
            r'<span[^>]*class[^>]*reporter[^>]*>([^<]+)</span>',
            r'<div[^>]*class[^>]*reporter[^>]*>([^<]+)</div>',
            r'<p[^>]*class[^>]*reporter[^>]*>([^<]+)</p>',
            
            # í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìëª… ì¶”ì¶œ (MBC íŠ¹ì„±ì— ë§ê²Œ)
            r'MBCë‰´ìŠ¤\s*([ê°€-í£]{2,4})ì…ë‹ˆë‹¤',  # MBCë‰´ìŠ¤ ê¹€ì¬ìš©ì…ë‹ˆë‹¤
            r'([ê°€-í£]{2,4})\s*(ê¸°ì|íŠ¹íŒŒì›)ì…ë‹ˆë‹¤',
            r'ì—ì„œ\s*MBCë‰´ìŠ¤\s*([ê°€-í£]{2,4})ì…ë‹ˆë‹¤',  # ì›Œì‹±í„´ì—ì„œ MBCë‰´ìŠ¤ ê¹€ì¬ìš©ì…ë‹ˆë‹¤
            r'([ê°€-í£]{2,4})\s*(ê¸°ì|íŠ¹íŒŒì›)(?:\s*=|\s*âˆ™|\s*Â·|\s*ì…ë ¥|\s*ìˆ˜ì •|\s*ì‘ì„±)',
            r'ê¸°ì\s*([ê°€-í£]{2,4})(?:\s*=|\s*âˆ™|\s*Â·)',
            r'([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›',
            r'([ê°€-í£]{2,4})\s*ì•µì»¤',
            r'([ê°€-í£]{2,4})\s*ë…¼ì„¤ìœ„ì›',
            r'([ê°€-í£]{2,4})\s*í¸ì§‘ìœ„ì›',
            r'/\s*([ê°€-í£]{2,4})\s*ê¸°ì',
            r'=\s*([ê°€-í£]{2,4})\s*ê¸°ì',
            r'âˆ™\s*([ê°€-í£]{2,4})\s*ê¸°ì',
            r'Â·\s*([ê°€-í£]{2,4})\s*ê¸°ì',
            r'ê¸°ì\s*:\s*([ê°€-í£]{2,4})',
            r'\[([ê°€-í£]{2,4})\s*ê¸°ì\]',
            r'^([ê°€-í£]{2,4})\s*ê¸°ì',  # ì¤„ ì‹œì‘ì—ì„œ ê¸°ìëª…
            r'ì·¨ì¬\s*:\s*([ê°€-í£]{2,4})',  # ì·¨ì¬: ê¸°ìëª…
            r'ì˜ìƒì·¨ì¬\s*:\s*([ê°€-í£]{2,4})'   # ì˜ìƒì·¨ì¬: ê¸°ìëª…
        ]
        
        # BeautifulSoup ê°ì²´ì—ì„œ ê¸°ìëª… ì°¾ê¸°
        if soup:
            # ê¸°ìëª…ì´ í¬í•¨ë  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ìš”ì†Œë“¤ ì°¾ê¸°
            reporter_elements = soup.find_all(['span', 'div', 'p'], string=re.compile(r'ê¸°ì|íŠ¹íŒŒì›|ì•µì»¤'))
            for element in reporter_elements:
                text = element.get_text(strip=True)
                if ('ê¸°ì' in text or 'íŠ¹íŒŒì›' in text or 'ì•µì»¤' in text) and 'MBC' in text:
                    match = re.search(r'([ê°€-í£]{2,4})', text)
                    if match:
                        name = match.group(1)
                        if 'ê¸°ì' in text:
                            return name + ' ê¸°ì'
                        elif 'íŠ¹íŒŒì›' in text:
                            return name + ' íŠ¹íŒŒì›'
                        elif 'ì•µì»¤' in text:
                            return name + ' ì•µì»¤'
        
        # ê¸°ì‚¬ í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìëª… ì°¾ê¸°
        full_text = str(soup) + '\n' + article_text if soup else article_text
        
        for pattern in reporter_patterns:
            matches = re.findall(pattern, full_text)
            if matches:
                if isinstance(matches[0], tuple):
                    reporter = matches[0][0].strip()
                    role = matches[0][1] if len(matches[0]) > 1 else 'ê¸°ì'
                else:
                    reporter = matches[0].strip()
                    role = 'ê¸°ì'
                
                if reporter and len(reporter) >= 2:
                    return reporter + f' {role}' if role not in reporter else reporter
        
        return "ê¸°ìëª… ì—†ìŒ"
        
    except Exception as e:
        print(f"ê¸°ìëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return "ê¸°ìëª… ì—†ìŒ"

def get_mbc_news_list(base_url="https://imnews.imbc.com", max_pages=3):
    """MBC ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        news_items = []
        
        # ì¹´í…Œê³ ë¦¬ë³„ URL íŒ¨í„´
        category_urls = [
            f"{base_url}/news/2025/politics/",      # ì •ì¹˜
            f"{base_url}/news/2025/society/",       # ì‚¬íšŒ
            f"{base_url}/news/2025/economy/",       # ê²½ì œ
            f"{base_url}/news/2025/international/", # êµ­ì œ
            f"{base_url}/news/2025/culture/",       # ë¬¸í™”
            f"{base_url}/news/2025/sports/"         # ìŠ¤í¬ì¸ 
        ]
        
        # ë©”ì¸ ë‰´ìŠ¤ í˜ì´ì§€ë„ í¬í•¨
        category_urls.insert(0, f"{base_url}/")
        
        for category_url in category_urls:
            try:
                print(f"ğŸ“„ {category_url} í˜ì´ì§€ ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
                
                response = requests.get(category_url, headers=headers, timeout=15)
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ë‰´ìŠ¤ ë§í¬ë“¤ ì°¾ê¸° (ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„)
                link_selectors = [
                    'a[href*="/article/"]',        # ê¸°ì‚¬ ë§í¬
                    'a[href*="_"]',                # MBC ê¸°ì‚¬ ë§í¬ íŒ¨í„´
                    'h3 a, h2 a, .title a',       # ì œëª© ë§í¬
                    '.news-list a',                # ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ë§í¬
                    '.headline a',                 # í—¤ë“œë¼ì¸ ë§í¬
                    '.article-list a'              # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ë§í¬
                ]
                
                page_links = []
                for selector in link_selectors:
                    links = soup.select(selector)
                    if links:
                        page_links.extend(links)
                
                # ì¤‘ë³µ ì œê±° ë° ìœ íš¨í•œ ë§í¬ë§Œ ì„ ë³„
                seen_urls = set()
                for link in page_links:
                    href = link.get('href')
                    if href and ('article' in href or '_' in href.split('/')[-1]):
                        full_url = urljoin(base_url, href)
                        if full_url not in seen_urls and 'imnews.imbc.com' in full_url:
                            seen_urls.add(full_url)
                            
                            # ì œëª© ì¶”ì¶œ
                            title = link.get_text(strip=True)
                            if not title:
                                # ë¶€ëª¨ ìš”ì†Œì—ì„œ ì œëª© ì°¾ê¸°
                                title_elem = link.find_parent().find(['h1', 'h2', 'h3', 'h4'])
                                if title_elem:
                                    title = title_elem.get_text(strip=True)
                            
                            if title and len(title) > 5:  # ë„ˆë¬´ ì§§ì€ ì œëª© ì œì™¸
                                news_items.append({
                                    'url': full_url,
                                    'title': title[:100]  # ì œëª© ê¸¸ì´ ì œí•œ
                                })
                
                print(f"  â¤ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ë°œê²¬")
                time.sleep(1)  # í˜ì´ì§€ ìš”ì²­ ê°„ ë”œë ˆì´
                
            except Exception as e:
                print(f"  â¤ ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        unique_news = []
        seen_urls = set()
        for item in news_items:
            if item['url'] not in seen_urls:
                seen_urls.add(item['url'])
                unique_news.append(item)
        
        print(f"ğŸ“Š ì´ {len(unique_news)}ê°œì˜ ê³ ìœ í•œ ë‰´ìŠ¤ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        return unique_news
        
    except Exception as e:
        print(f"ë‰´ìŠ¤ ëª©ë¡ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

def scrape_mbc_news(max_articles=50):
    """MBC ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ì—¬ CSVë¡œ ì €ì¥í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    
    print("ğŸ—ï¸  MBC ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 60)
    
    try:
        # ë‰´ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        news_list = get_mbc_news_list()
        
        if not news_list:
            print("âŒ ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ ì œí•œ
        if len(news_list) > max_articles:
            news_list = news_list[:max_articles]
            print(f"âš ï¸  ìµœëŒ€ {max_articles}ê°œ ê¸°ì‚¬ë¡œ ì œí•œí•©ë‹ˆë‹¤.")
        
        news_data = []
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        print(f"\nğŸ“° {len(news_list)}ê°œ ê¸°ì‚¬ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
        
        for i, news_item in enumerate(news_list):
            try:
                url = news_item['url']
                base_title = news_item['title']
                
                print(f"[{i+1}/{len(news_list)}] ì²˜ë¦¬ ì¤‘: {base_title[:50]}...")
                
                # ê°œë³„ ê¸°ì‚¬ í˜ì´ì§€ í¬ë¡¤ë§
                response = requests.get(url, headers=headers, timeout=20)
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ì œëª© ì¶”ì¶œ (ë” ì •í™•í•œ ì œëª©)
                title = base_title
                title_selectors = ['h1.title', 'h1', '.news_title', '.article_title', 'title']
                for selector in title_selectors:
                    title_elem = soup.select_one(selector)
                    if title_elem:
                        extracted_title = title_elem.get_text(strip=True)
                        if extracted_title and len(extracted_title) > len(title):
                            title = extracted_title
                        break
                
                # ë‚ ì§œ ì¶”ì¶œ (URLì´ë‚˜ í˜ì´ì§€ì—ì„œ)
                date_text = "ë‚ ì§œ ì—†ìŒ"
                
                # URLì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
                url_date_match = re.search(r'/(\d{4})/|(\d{4})[-/](\d{1,2})[-/](\d{1,2})', url)
                if url_date_match:
                    if url_date_match.group(1):  # /2025/ í˜•íƒœ
                        date_text = f"{url_date_match.group(1)}-{datetime.now().month:02d}-{datetime.now().day:02d}"
                    else:  # 2025-06-28 í˜•íƒœ
                        year, month, day = url_date_match.groups()[1:]
                        date_text = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                
                # í˜ì´ì§€ì—ì„œ ë‚ ì§œ ì¶”ì¶œ ì‹œë„
                page_text = soup.get_text()
                date_patterns = [
                    r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
                    r'(\d{4})-(\d{1,2})-(\d{1,2})\s*(\d{1,2}):(\d{2})',
                    r'(\d{4})\.(\d{1,2})\.(\d{1,2})',
                    r'(\d{4})/(\d{1,2})/(\d{1,2})'
                ]
                
                for pattern in date_patterns:
                    match = re.search(pattern, page_text)
                    if match:
                        groups = match.groups()
                        if len(groups) >= 3:
                            year, month, day = groups[0], groups[1], groups[2]
                            if len(groups) >= 5:
                                hour, minute = groups[3], groups[4]
                                date_text = f"{year}-{month.zfill(2)}-{day.zfill(2)} {hour.zfill(2)}:{minute}"
                            else:
                                date_text = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                        break
                
                if date_text == "ë‚ ì§œ ì—†ìŒ":
                    date_text = datetime.now().strftime('%Y-%m-%d %H:%M')
                
                # ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ
                full_content = extract_mbc_article_content(url)
                
                # ê¸°ìëª… ì¶”ì¶œ
                reporter_name = extract_mbc_reporter_name(soup, full_content)
                
                # ë°ì´í„° ì €ì¥
                if full_content.strip():  # ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš°ë§Œ ì €ì¥
                    news_data.append({
                        'ì œëª©': title.strip(),
                        'ë‚ ì§œ': date_text,
                        'ê¸°ìëª…': reporter_name,
                        'ë³¸ë¬¸': full_content
                    })
                else:
                    print(f"  â¤ ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€
                time.sleep(1.5)
                
            except Exception as e:
                print(f"  â¤ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # CSV íŒŒì¼ë¡œ ì €ì¥
        if news_data:
            filename = f"results/MBCë‰´ìŠ¤_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            
            with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['ì œëª©', 'ë‚ ì§œ', 'ê¸°ìëª…', 'ë³¸ë¬¸']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                
                writer.writeheader()
                writer.writerows(news_data)
            
            print(f"\nâœ… ì„±ê³µì ìœ¼ë¡œ {len(news_data)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“ íŒŒì¼ëª…: {filename}")
            
            # í†µê³„ ì •ë³´ ì¶œë ¥
            total_chars = sum(len(item['ë³¸ë¬¸']) for item in news_data)
            avg_chars = total_chars // len(news_data) if news_data else 0
            print(f"ğŸ“Š í‰ê·  ë³¸ë¬¸ ê¸¸ì´: {avg_chars:,}ì")
            print(f"ğŸ“Š ì´ ë³¸ë¬¸ ê¸¸ì´: {total_chars:,}ì")
            
            return filename
        else:
            print("âŒ ì¶”ì¶œëœ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def scrape_mbc_by_category(category=None, max_articles=30):
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ MBC ë‰´ìŠ¤ í¬ë¡¤ë§"""
    
    category_mapping = {
        "politics": "ì •ì¹˜",
        "society": "ì‚¬íšŒ", 
        "economy": "ê²½ì œ",
        "international": "êµ­ì œ",
        "culture": "ë¬¸í™”",
        "sports": "ìŠ¤í¬ì¸ "
    }
    
    if category and category not in category_mapping:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤.")
        print(f"âœ… ì§€ì› ì¹´í…Œê³ ë¦¬: {', '.join(category_mapping.keys())}")
        return None
    
    if category:
        print(f"ğŸ“° MBC {category_mapping[category]} ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    else:
        print("ğŸ“° MBC ì „ì²´ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    
    return scrape_mbc_news(max_articles=max_articles)

if __name__ == "__main__":
    print("ğŸ—ï¸  MBC ë‰´ìŠ¤ í¬ë¡¤ë§")
    print("=" * 60)
    print("âš ï¸  ì£¼ì˜: MBCëŠ” í˜„ì¬ ê³µì‹ RSSë¥¼ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì›¹ì‚¬ì´íŠ¸ë¥¼ ì§ì ‘ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
    
    try:
        print("\nğŸ“‹ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ì „ì²´ ìµœì‹  ë‰´ìŠ¤ (50ê°œ)")
        print("2. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10ê°œ)")
        print("3. ëŒ€ëŸ‰ ìˆ˜ì§‘ (100ê°œ)")
        print("4. ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘")
        
        choice = input("\nì„ íƒ (1-4): ").strip()
        
        if choice == "1":
            print("\nğŸš€ ì „ì²´ ìµœì‹  ë‰´ìŠ¤ 50ê°œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
            result = scrape_mbc_news(max_articles=50)
            
        elif choice == "2":
            print("\nğŸš€ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (10ê°œ ê¸°ì‚¬)ë¡œ ì‹œì‘í•©ë‹ˆë‹¤...")
            result = scrape_mbc_news(max_articles=10)
            
        elif choice == "3":
            print("\nğŸš€ ëŒ€ëŸ‰ ìˆ˜ì§‘ ëª¨ë“œ (100ê°œ ê¸°ì‚¬)ë¡œ ì‹œì‘í•©ë‹ˆë‹¤...")
            result = scrape_mbc_news(max_articles=100)
            
        elif choice == "4":
            print("\nğŸ“‚ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
            categories = ["politics", "society", "economy", "international", "culture", "sports"]
            korean_names = ["ì •ì¹˜", "ì‚¬íšŒ", "ê²½ì œ", "êµ­ì œ", "ë¬¸í™”", "ìŠ¤í¬ì¸ "]
            
            for i, (cat, kor_name) in enumerate(zip(categories, korean_names), 1):
                print(f"{i}. {cat} ({kor_name})")
            
            cat_choice = input("\nì¹´í…Œê³ ë¦¬ ë²ˆí˜¸ ë˜ëŠ” ì´ë¦„: ").strip()
            
            if cat_choice.isdigit() and 1 <= int(cat_choice) <= len(categories):
                selected_category = categories[int(cat_choice) - 1]
            elif cat_choice in categories:
                selected_category = cat_choice
            else:
                selected_category = None
                print("âš ï¸  ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ì „ì²´ ë‰´ìŠ¤ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            
            print(f"\nğŸš€ MBC {korean_names[categories.index(selected_category)] if selected_category else 'ì „ì²´'} ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            result = scrape_mbc_by_category(selected_category, max_articles=30)
            
        else:
            print("âš ï¸  ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            result = scrape_mbc_news(max_articles=30)
        
        if result:
            print(f"\nğŸ‰ ì™„ë£Œ! íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {result}")
        else:
            print("\nâŒ ì‘ì—…ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
