import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import csv
import time
import random
import logging
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse


class DailyNewsCrawler:
    def __init__(self):
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[logging.FileHandler("daily_news_rss_crawler.log", encoding="utf-8"), logging.StreamHandler()],
        )
        self.logger = logging.getLogger(__name__)

        # ë°ì¼ë¦¬ë‰´ìŠ¤ RSS í”¼ë“œ ëª©ë¡
        self.rss_feeds = {"ì „ì²´ê¸°ì‚¬": "https://www.idailynews.co.kr/rss/allArticle.xml"}

        # User-Agent ëª©ë¡
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        ]

        self.articles = []

    def get_random_headers(self):
        """ëœë¤ User-Agent í—¤ë” ë°˜í™˜"""
        return {
            "User-Agent": random.choice(self.user_agents),
            "Accept": "application/rss+xml,application/xml,text/xml,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Cache-Control": "max-age=0",
            "DNT": "1",
        }

    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ í•¨ìˆ˜"""
        if not text:
            return ""

        # CDATA íƒœê·¸ ì œê±°
        text = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", text, flags=re.DOTALL)

        # HTML íƒœê·¸ ì œê±°
        soup = BeautifulSoup(text, "html.parser")
        text = soup.get_text()

        # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
        unwanted_texts = [
            "ë°ì¼ë¦¬ë‰´ìŠ¤ ì œê³µ",
            "ë¬´ë‹¨ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€",
            "Copyright by Daily News",
            "ì €ì‘ê¶Œì",
            "â–¶ ë°ì¼ë¦¬ë‰´ìŠ¤",
            "í™ˆí˜ì´ì§€ì—ì„œ í™•ì¸í•˜ì„¸ìš”",
            "ë‰´ìŠ¤ë£¸ ì œë³´",
            "ë¬´ë‹¨ ì „ì¬-ì¬ë°°í¬",
            "â“’ë°ì¼ë¦¬ë‰´ìŠ¤",
        ]

        for unwanted in unwanted_texts:
            text = text.replace(unwanted, "")

        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€í™˜
        text = re.sub(r"\s+", " ", text)

        return text.strip()

    def extract_reporter_name(self, title, description=""):
        """RSS ì œëª©ê³¼ descriptionì—ì„œ ê¸°ìëª… ì¶”ì¶œ"""
        # ì œëª©ê³¼ descriptionì„ í•©ì³ì„œ ê²€ìƒ‰
        combined_text = f"{title} {description}"

        # ë°ì¼ë¦¬ë‰´ìŠ¤ ê¸°ìëª… íŒ¨í„´ë“¤
        patterns = [
            r"([ê°€-í£]{2,4})\s*ê¸°ì",
            r"ê¸°ì\s*([ê°€-í£]{2,4})",
            r"([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›",
            r"([ê°€-í£]{2,4})\s*ë…¼ì„¤ìœ„ì›",
            r"([ê°€-í£]{2,4})\s*í¸ì§‘ìœ„ì›",
            r"ë°ì¼ë¦¬ë‰´ìŠ¤\s*([ê°€-í£]{2,4})",
            r"ë¦¬í¬í„°\s*([ê°€-í£]{2,4})",
            r"([ê°€-í£]{2,4})\s*ë¦¬í¬í„°",
        ]

        for pattern in patterns:
            match = re.search(pattern, combined_text)
            if match:
                return match.group(1) + " ê¸°ì"

        return "ê¸°ìëª… ì—†ìŒ"

    def format_date(self, date_str):
        """ë‚ ì§œ í˜•ì‹ ë³€í™˜"""
        try:
            # RFC 2822 í˜•ì‹ íŒŒì‹± ì‹œë„
            dt = datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S %z")
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        except:
            try:
                # GMT ì—†ëŠ” í˜•ì‹ ì‹œë„
                dt = datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S GMT")
                return dt.strftime("%Y-%m-%d %H:%M:%S")
            except:
                try:
                    # KST í˜•ì‹ ì‹œë„
                    dt = datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S KST")
                    return dt.strftime("%Y-%m-%d %H:%M:%S")
                except:
                    try:
                        # +0900 í˜•ì‹ ì‹œë„
                        dt = datetime.strptime(date_str, "%a, %d %b %Y %H:%M:%S +0900")
                        return dt.strftime("%Y-%m-%d %H:%M:%S")
                    except:
                        try:
                            # ë‹¤ë¥¸ í˜•ì‹ ì‹œë„
                            dt = datetime.strptime(date_str[:19], "%Y-%m-%d %H:%M:%S")
                            return dt.strftime("%Y-%m-%d %H:%M:%S")
                        except:
                            return date_str

    def get_content_from_description(self, description):
        """RSS descriptionì—ì„œ ë‚´ìš© ì¶”ì¶œ"""
        if not description:
            return "RSS description ì—†ìŒ"

        # description ì •ì œ
        content = self.clean_text(description)

        # ë„ˆë¬´ ì§§ì€ ê²½ìš°
        if len(content) < 20:
            return "RSS descriptionì´ ë„ˆë¬´ ì§§ìŒ"

        return content

    def get_article_content_fallback(self, url):
        """ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„ (fallback)"""
        try:
            headers = self.get_random_headers()
            session = requests.Session()
            session.headers.update(headers)

            response = session.get(url, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            # ìƒˆ ëª…ì‹œì  ì…€ë ‰í„° ì‚¬ìš©í•˜ì—¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
            specific_selector = "html > body > div:nth-of-type(1) > div > div:nth-of-type(1) > div > div:nth-of-type(1) > section > div:nth-of-type(5) > article > section > article > article:nth-of-type(1) > section > article:nth-of-type(1)"
            content_elem = soup.select_one(specific_selector)
            if content_elem:
                content = self.clean_text(content_elem.get_text())
                if content and len(content.strip()) > 30:
                    return content

            # ê¸°ì¡´ ë°ì¼ë¦¬ë‰´ìŠ¤ ë³¸ë¬¸ ì…€ë ‰í„°ë“¤
            content_selectors = [
                "div.article_content p",
                ".article_content p",
                "div.news_body p",
                ".news_body p",
                "div.article_body p",
                ".article_body p",
                "div.entry-content p",
                ".entry-content p",
                # ì „ì²´ ì˜ì—­
                "div.article_content",
                ".article_content",
                "div.news_body",
                ".news_body",
                "div.article_body",
                ".article_body",
            ]

            content = ""

            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    if "p" in selector:
                        paragraphs = []
                        for elem in elements:
                            text = elem.get_text().strip()
                            if len(text) > 20 and not any(
                                skip in text
                                for skip in [
                                    "ì €ì‘ê¶Œì",
                                    "Copyright",
                                    "ë¬´ë‹¨ì „ì¬",
                                    "ì¬ë°°í¬ ê¸ˆì§€",
                                    "ë°ì¼ë¦¬ë‰´ìŠ¤ ì œê³µ",
                                    "ë‰´ìŠ¤ë£¸ ì œë³´",
                                ]
                            ):
                                paragraphs.append(text)

                        if paragraphs:
                            content = " ".join(paragraphs[:3])
                            break
                    else:
                        for elem in elements:
                            text = elem.get_text()
                            if len(text.strip()) > 50:
                                content = text
                                break

                if content and len(content.strip()) > 50:
                    break

            # ë‚´ìš© ì •ì œ
            content = self.clean_text(content)

            if content and len(content.strip()) > 30:
                return content

            return "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"

        except Exception as e:
            self.logger.warning(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
            return "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"

    def parse_rss_feed(self, category, url):
        """RSS í”¼ë“œ íŒŒì‹±"""
        try:
            self.logger.info(f"{category} RSS í”¼ë“œ í¬ë¡¤ë§ ì‹œì‘: {url}")

            headers = self.get_random_headers()
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()

            # ì‘ë‹µì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if not response.content.strip():
                self.logger.error(f"{category} RSS í”¼ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {url}")
                return []

            # XML íŒŒì‹±
            try:
                root = ET.fromstring(response.content)
            except ET.ParseError as e:
                self.logger.error(f"{category} RSS XML íŒŒì‹± ì˜¤ë¥˜: {e}")
                return []

            # RSS ì•„ì´í…œ ì¶”ì¶œ
            items = root.findall(".//item")

            if not items:
                self.logger.warning(f"{category} RSS í”¼ë“œì—ì„œ ì•„ì´í…œì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return []

            articles = []

            # ìµœëŒ€ 20ê°œ ê¸°ì‚¬ ì²˜ë¦¬
            for i, item in enumerate(items[:20]):
                try:
                    title_elem = item.find("title")
                    link_elem = item.find("link")
                    pubdate_elem = item.find("pubDate")
                    description_elem = item.find("description")

                    if title_elem is None or link_elem is None:
                        continue

                    title = self.clean_text(title_elem.text)
                    link = link_elem.text.strip()
                    pub_date = self.format_date(pubdate_elem.text) if pubdate_elem is not None else ""
                    description = self.clean_text(description_elem.text) if description_elem is not None else ""

                    # ê¸°ìëª… ì¶”ì¶œ: rss author íƒœê·¸ ìš°ì„  ì‚¬ìš©
                    author_elem = item.find("author")
                    if author_elem is not None and author_elem.text:
                        reporter = self.clean_text(author_elem.text)
                    else:
                        reporter = self.extract_reporter_name(title, description)

                    # ì›ë¬¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì§ì ‘ ì¶”ì¶œ
                    self.logger.info(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘: {title[:30]}... ì›ë¬¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì§ì ‘ ì¶”ì¶œ ì‹œë„")
                    content = self.get_article_content_fallback(link)
                    time.sleep(random.uniform(2, 4))  # ë³¸ë¬¸ ì¶”ì¶œ ì‹œ ë”œë ˆì´

                    article = {
                        "title": title,
                        "category": category,
                        "date": pub_date,
                        "reporter": reporter,
                        "content": content,
                        "url": link,
                    }

                    articles.append(article)

                    # ê¸°ë³¸ ë”œë ˆì´
                    time.sleep(random.uniform(1, 2))

                except Exception as e:
                    self.logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue

            self.logger.info(f"{category} ì¹´í…Œê³ ë¦¬ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
            return articles

        except requests.RequestException as e:
            self.logger.error(f"{category} RSS í”¼ë“œ ìš”ì²­ ì˜¤ë¥˜: {e}")
            return []
        except Exception as e:
            self.logger.error(f"{category} RSS í”¼ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

    def crawl_all_feeds(self):
        """ëª¨ë“  RSS í”¼ë“œ í¬ë¡¤ë§"""
        self.logger.info("ë°ì¼ë¦¬ë‰´ìŠ¤ RSS í¬ë¡¤ë§ ì‹œì‘")

        for category, url in self.rss_feeds.items():
            self.logger.info(f"\n=== {category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ===")

            articles = self.parse_rss_feed(category, url)
            self.articles.extend(articles)

            # ì¹´í…Œê³ ë¦¬ ê°„ ë”œë ˆì´
            time.sleep(random.uniform(3, 5))

        self.logger.info(f"\nì´ {len(self.articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")

    def save_to_csv(self, filename=None):
        """CSV íŒŒì¼ë¡œ ì €ì¥ (ë³€ê²½ëœ í˜•ì‹)"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"results/ë°ì¼ë¦¬ë‰´ìŠ¤_ì „ì²´_{timestamp}.csv"

        try:
            with open(filename, "w", newline="", encoding="utf-8-sig") as csvfile:
                # CSV ì—´ ìˆœì„œ: ì–¸ë¡ ì‚¬, ì œëª©, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, ê¸°ìëª…, ë³¸ë¬¸
                fieldnames = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

                writer.writeheader()
                for article in self.articles:
                    row = {
                        "ì–¸ë¡ ì‚¬": "ë°ì¼ë¦¬ë‰´ìŠ¤",
                        "ì œëª©": article["title"],
                        "ë‚ ì§œ": article["date"],
                        "ì¹´í…Œê³ ë¦¬": article["category"],
                        "ê¸°ìëª…": article["reporter"],
                        "ë³¸ë¬¸": article["content"],
                    }
                    writer.writerow(row)

            self.logger.info(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {filename}")
            print(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(self.articles)}ê°œ")

        except Exception as e:
            self.logger.error(f"CSV íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = DailyNewsCrawler()

    print("ğŸš€ ë°ì¼ë¦¬ë‰´ìŠ¤ RSS í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ğŸ“° ìˆ˜ì§‘ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬: ì „ì²´ê¸°ì‚¬, ì¸ê¸°ê¸°ì‚¬")
    print("ğŸ“° RSS Description ìš°ì„  í™œìš©, í•„ìš”ì‹œ ë³¸ë¬¸ ì¶”ì¶œ")
    print("â³ ì•ˆì •ì ì¸ í¬ë¡¤ë§ì„ ìœ„í•´ ì ì ˆí•œ ë”œë ˆì´ê°€ ì ìš©ë©ë‹ˆë‹¤.\n")

    try:
        # RSS í”¼ë“œ í¬ë¡¤ë§
        crawler.crawl_all_feeds()

        # CSV íŒŒì¼ë¡œ ì €ì¥
        crawler.save_to_csv()

        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        if crawler.articles:
            print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ í˜„í™©:")
            category_count = {}
            for article in crawler.articles:
                category = article["category"]
                category_count[category] = category_count.get(category, 0) + 1

            for category, count in category_count.items():
                print(f"   â€¢ {category}: {count}ê°œ")

    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logging.error(f"ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    main()
