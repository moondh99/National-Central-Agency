import requests
import feedparser
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
import os
import time
from urllib.parse import urljoin

# RSS URL ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ (9ê°œ ì¹´í…Œê³ ë¦¬)
rss_urls = {
    "ì „ì²´ê¸°ì‚¬": "https://www.kihoilbo.co.kr/rss/allArticle.xml",
    "ì •ì¹˜": "https://www.kihoilbo.co.kr/rss/clickTop.xml",
    "ê²½ì œ": "https://www.kihoilbo.co.kr/rss/S1N2.xml",
    "ì‚¬íšŒ": "https://www.kihoilbo.co.kr/rss/S1N4.xml",
    "ë¬¸í™”": "https://www.kihoilbo.co.kr/rss/S1N5.xml",
    "êµìœ¡": "https://www.kihoilbo.co.kr/rss/S1N6.xml",
    "ì§€ì—­": "https://www.kihoilbo.co.kr/rss/S1N7.xml",
    "ì¢…í•©": "https://www.kihoilbo.co.kr/rss/S1N8.xml",
    "ì˜¤í”¼ë‹ˆì–¸": "https://www.kihoilbo.co.kr/rss/S1N11.xml",
}


def parse_rss_feed(url, category, max_articles=20):
    """
    RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ì •ë³´ë¥¼ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜

    Args:
        url (str): RSS í”¼ë“œ URL
        category (str): ì¹´í…Œê³ ë¦¬ëª…
        max_articles (int): ìµœëŒ€ ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜

    Returns:
        list: ê¸°ì‚¬ ì •ë³´ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    articles = []

    try:
        # RSS í”¼ë“œ íŒŒì‹±
        feed = feedparser.parse(url)

        if not feed.entries:
            print(f"[ê²½ê³ ] {category} ì¹´í…Œê³ ë¦¬ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return articles

        for entry in feed.entries[:max_articles]:
            article_data = {
                "ì–¸ë¡ ì‚¬": "í‚¤í˜¸ì¼ë³´",
                "ì œëª©": entry.get("title", "ì œëª© ì—†ìŒ"),
                "ë‚ ì§œ": entry.get("published", "ë‚ ì§œ ì—†ìŒ"),
                "ì¹´í…Œê³ ë¦¬": category,
                "ê¸°ìëª…": entry.get("author", "ê¸°ìëª… ì—†ìŒ"),
                "ë§í¬": entry.get("link", ""),
            }
            articles.append(article_data)

        print(f"[ì™„ë£Œ] {category} ì¹´í…Œê³ ë¦¬: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

    except Exception as e:
        print(f"[ì˜¤ë¥˜] {category} RSS íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    return articles


def extract_article_content(url, max_retries=3):
    """
    ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜

    Args:
        url (str): ê¸°ì‚¬ URL
        max_retries (int): ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

    Returns:
        str: ì¶”ì¶œëœ ë³¸ë¬¸ ë‚´ìš©
    """
    if not url:
        return "URL ì—†ìŒ"

    for attempt in range(max_retries):
        try:
            # ì›¹í˜ì´ì§€ ìš”ì²­
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }

            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = "utf-8"

            if response.status_code == 200:
                soup = BeautifulSoup(response.content, "html.parser")

                # ì§€ì •ëœ XPathì— í•´ë‹¹í•˜ëŠ” CSS ì„ íƒìë¡œ ë³¸ë¬¸ ì¶”ì¶œ
                # XPath: /html/body/div[1]/div/div[1]/div/div[1]/section/div[4]/div/section/article/div[2]/div/article[1]
                # CSS ì„ íƒìë¡œ ë³€í™˜
                content_selectors = [
                    "body > div:nth-child(1) > div > div:nth-child(1) > div > div:nth-child(1) > section > div:nth-child(4) > div > section > article > div:nth-child(2) > div > article:nth-child(1)",
                    "article div.article-content",
                    "div.article-content",
                    ".article_view",
                    "#articleText",
                    ".news_text",
                ]

                content = ""
                for selector in content_selectors:
                    element = soup.select_one(selector)
                    if element:
                        content = element.get_text(strip=True)
                        break

                if not content:
                    # ëŒ€ì•ˆ: p íƒœê·¸ë“¤ì„ ëª¨ì•„ì„œ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
                    paragraphs = soup.find_all("p")
                    content = " ".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])

                return content[:1000] + "..." if len(content) > 1000 else content if content else "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"

            else:
                print(f"[ê²½ê³ ] HTTP {response.status_code} ì˜¤ë¥˜: {url}")

        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(1)  # 1ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                continue
            else:
                print(f"[ì˜¤ë¥˜] ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {str(e)}")
                return "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"

    return "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"


def collect_all_news():
    """
    ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ ë‰´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜

    Returns:
        list: ëª¨ë“  ê¸°ì‚¬ ì •ë³´ê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸
    """
    all_articles = []

    print("í‚¤í˜¸ì¼ë³´ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print(f"ì´ {len(rss_urls)}ê°œ ì¹´í…Œê³ ë¦¬ì—ì„œ ê°ê° 20ê°œì”© ìˆ˜ì§‘ ì˜ˆì •\n")

    for i, (category, url) in enumerate(rss_urls.items(), 1):
        print(f"[{i}/{len(rss_urls)}] {category} ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì¤‘...")

        # RSSì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘
        articles = parse_rss_feed(url, category, 20)

        if articles:
            # ê° ê¸°ì‚¬ì˜ ë³¸ë¬¸ ì¶”ì¶œ
            for j, article in enumerate(articles, 1):
                print(f"  - {j}/{len(articles)} ë³¸ë¬¸ ì¶”ì¶œ ì¤‘: {article['ì œëª©'][:30]}...")

                # ë³¸ë¬¸ ì¶”ì¶œ
                content = extract_article_content(article["ë§í¬"])
                article["ë³¸ë¬¸"] = content

                # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                if j % 5 == 0:
                    time.sleep(0.5)

            all_articles.extend(articles)
            print(f"  â†’ {category} ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘\n")
        else:
            print(f"  â†’ {category}: ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì—†ìŒ\n")

        # ì¹´í…Œê³ ë¦¬ ê°„ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        if i < len(rss_urls):
            time.sleep(1)

    print(f"ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ë¨")
    return all_articles


def save_to_csv(articles):
    """
    ìˆ˜ì§‘ëœ ê¸°ì‚¬ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜

    Args:
        articles (list): ê¸°ì‚¬ ì •ë³´ê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸
    """
    # results ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = "results"
    os.makedirs(results_dir, exist_ok=True)

    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"ê¸°í˜¸ì¼ë³´_ì „ì²´_{timestamp}.csv"
    filepath = os.path.join(results_dir, filename)

    # DataFrame ìƒì„± (ì»¬ëŸ¼ ìˆœì„œ: ì–¸ë¡ ì‚¬, ì œëª©, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, ê¸°ìëª…, ë³¸ë¬¸)
    df = pd.DataFrame(articles)

    # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬
    column_order = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
    df = df[column_order]

    # CSV íŒŒì¼ë¡œ ì €ì¥ (UTF-8 ì¸ì½”ë”©)
    df.to_csv(filepath, index=False, encoding="utf-8-sig")

    print(f"âœ… CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ!")
    print(f"íŒŒì¼ ê²½ë¡œ: {filepath}")
    print(f"íŒŒì¼ í¬ê¸°: {os.path.getsize(filepath):,} bytes")

    # ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½:")
    print(f"âœ… ì´ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜: {len(df)}ê°œ")
    print(f"âœ… ìˆ˜ì§‘ ì¹´í…Œê³ ë¦¬: {len(df['ì¹´í…Œê³ ë¦¬'].unique())}ê°œ")

    print(f"\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ í˜„í™©:")
    category_counts = df["ì¹´í…Œê³ ë¦¬"].value_counts().sort_index()
    for category, count in category_counts.items():
        status = "âœ…" if count >= 15 else "âš ï¸" if count >= 10 else "âŒ"
        print(f"  {status} {category}: {count}ê°œ")

    return filepath


# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("=" * 60)
    print("ğŸ‰ í‚¤í˜¸ì¼ë³´ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì‹œì‘!")
    print("=" * 60)

    # ë‰´ìŠ¤ ìˆ˜ì§‘
    collected_articles = collect_all_news()

    if collected_articles:
        # CSV íŒŒì¼ë¡œ ì €ì¥
        filepath = save_to_csv(collected_articles)

        print(f"\nğŸ“… ìˆ˜ì§‘ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)

        return filepath
    else:
        print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None


# í”„ë¡œê·¸ë¨ ì‹¤í–‰
if __name__ == "__main__":
    # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ì²˜ìŒ ì‹¤í–‰ ì‹œ)
    # pip install feedparser requests beautifulsoup4 pandas lxml

    main()
