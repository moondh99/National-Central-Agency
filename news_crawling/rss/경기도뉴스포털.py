import feedparser
import csv
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import time
import random


def get_random_user_agent():
    """ëœë¤ User-Agent ë°˜í™˜"""
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    ]
    return random.choice(user_agents)


def extract_reporter_from_description(description):
    """RSS descriptionì—ì„œ ê¸°ìëª… ì¶”ì¶œ (ì˜ˆ: 'ì´ë¯¸ì˜ | 2025-08-12')"""
    reporter = ""

    if description and "|" in description:
        parts = description.split("|")
        if len(parts) >= 2:
            potential_reporter = parts[0].strip()
            # í•œê¸€ ì´ë¦„ íŒ¨í„´ í™•ì¸ (2-4ê¸€ì)
            if len(potential_reporter) >= 2 and len(potential_reporter) <= 10:
                # í•œê¸€ë§Œ í¬í•¨ëœ ì´ë¦„ì¸ì§€ í™•ì¸
                if re.match(r"^[ê°€-í£\s]+$", potential_reporter):
                    reporter = potential_reporter

    return reporter


def extract_gnews_article_content(url, rss_description=""):
    """ê²½ê¸°ë„ ë‰´ìŠ¤í¬í„¸ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ê³¼ ê¸°ìëª…ì„ ì¶”ì¶œ"""
    try:
        session = requests.Session()

        headers = {
            "User-Agent": get_random_user_agent(),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Referer": "https://gnews.gg.go.kr/",
            "Cache-Control": "no-cache",
        }

        print(f"    ì ‘ì† ì‹œë„: {url[:80]}...")

        # ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ í›„ ì‹¤ì œ ê¸°ì‚¬ ì ‘ê·¼
        try:
            session.get("https://gnews.gg.go.kr/", headers=headers, timeout=5)
            time.sleep(0.5)

            response = session.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            # ê²½ê¸°ë„ ë‰´ìŠ¤í¬í„¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ì ‘ê·¼ ì œí•œì´ ì—†ìŒ
            if len(response.content) < 3000:  # 3KB ë¯¸ë§Œì´ë©´ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ
                print(f"    âš  ì‘ë‹µ í¬ê¸°ê°€ ì‘ìŒ (í¬ê¸°: {len(response.content)} bytes)")

        except Exception as e:
            print(f"    âš  ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            # RSS descriptionì—ì„œ ê¸°ìëª… ì¶”ì¶œ ì‹œë„
            reporter = extract_reporter_from_description(rss_description)
            return reporter, rss_description if rss_description else "ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨"

        soup = BeautifulSoup(response.content, "html.parser")

        # RSS descriptionì—ì„œ ê¸°ìëª… ì¶”ì¶œ
        reporter = extract_reporter_from_description(rss_description)

        # ë³¸ë¬¸ ì¶”ì¶œ - ë‹¤ì–‘í•œ ë°©ë²• ì‹œë„
        content = ""
        full_text = soup.get_text()

        # ë°©ë²• 1: íŠ¹ì • íƒœê·¸ì—ì„œ ì¶”ì¶œ
        content_tags = ["div", "article", "main", "section"]
        for tag in content_tags:
            elements = soup.find_all(tag)
            for element in elements:
                text = element.get_text().strip()
                # ê¸´ í…ìŠ¤íŠ¸ë¥¼ ë³¸ë¬¸ìœ¼ë¡œ ê°„ì£¼
                if len(text) > len(content) and len(text) > 100:
                    content = text

        # ë°©ë²• 2: P íƒœê·¸ë“¤ì„ ëª¨ë‘ í•©ì¹˜ê¸°
        if len(content) < 200:
            paragraphs = soup.find_all("p")
            content_parts = []

            for p in paragraphs:
                text = p.get_text().strip()
                if len(text) > 20:
                    # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œì™¸
                    if not any(skip_word in text for skip_word in ["â“’", "#ê²½ê¸°", "#Gyeonggi", "ë‚´ì¼ì´ ë¨¼ì €"]):
                        content_parts.append(text)

            if content_parts:
                content = " ".join(content_parts)

        # ë°©ë²• 3: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
        if len(content) < 100:
            lines = full_text.split("\n")
            content_lines = []

            for line in lines:
                line = line.strip()
                if len(line) > 30:  # ì¶©ë¶„íˆ ê¸´ ë¼ì¸ë§Œ
                    # ë¶ˆí•„ìš”í•œ ë¼ì¸ ì œì™¸
                    if not any(skip_word in line for skip_word in ["â“’", "#ê²½ê¸°", "#Gyeonggi", "ë‚´ì¼ì´ ë¨¼ì €"]):
                        content_lines.append(line)

            if content_lines:
                content = " ".join(content_lines[:10])  # ì²˜ìŒ 10ê°œ ë¼ì¸ë§Œ

        # ë³¸ë¬¸ ì •ì œ
        content = clean_gnews_content(content)

        # RSS descriptionì´ ë” ì¢‹ìœ¼ë©´ RSS description ì‚¬ìš©
        if rss_description and (len(content) < 100 or len(rss_description) > len(content)):
            content = rss_description
            print(f"    RSS description ì±„íƒ (ê¸¸ì´: {len(rss_description)})")

        print(f"    ìµœì¢… ë³¸ë¬¸ ê¸¸ì´: {len(content)}")
        return reporter, content

    except Exception as e:
        print(f"    âŒ ì—ëŸ¬: {e}")
        reporter = extract_reporter_from_description(rss_description)
        return reporter, rss_description if rss_description else f"ì˜¤ë¥˜: {str(e)}"


def clean_gnews_content(content):
    """ê²½ê¸°ë„ ë‰´ìŠ¤í¬í„¸ ê¸°ì‚¬ ë³¸ë¬¸ ì •ì œ - ì•ˆì „í•œ ë°©ë²• ì‚¬ìš©"""
    if not content:
        return ""

    # ë¬¸ìì—´ ì¹˜í™˜ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±° (ì •ê·œì‹ ì‚¬ìš© ìµœì†Œí™”)

    # ì €ì‘ê¶Œ í‘œì‹œ ì œê±°
    content = content.replace("â“’ ê²½ê¸°ë„ì²­", "")
    content = content.replace("â“’ ê²½ê¸°ë„", "")
    content = content.replace("ë‚´ì¼ì´ ë¨¼ì € ì‹œì‘ë˜ëŠ” ê²½ê¸°.", "")

    # í•´ì‹œíƒœê·¸ ì œê±° (ê°„ë‹¨í•œ ë°©ë²•)
    lines = content.split("\n")
    cleaned_lines = []
    for line in lines:
        if not line.strip().startswith("#"):
            cleaned_lines.append(line)
    content = "\n".join(cleaned_lines)

    # ê³µë°± ì •ë¦¬ (ì•ˆì „í•œ ì •ê·œì‹ë§Œ ì‚¬ìš©)
    content = re.sub(r"\s+", " ", content).strip()

    # ê¸¸ì´ ì œí•œ
    if len(content) > 1500:
        content = content[:1500] + "..."

    return content


def fetch_gnews_rss_to_csv(rss_url, category, writer, max_articles=30):
    """ê²½ê¸°ë„ ë‰´ìŠ¤í¬í„¸ RSSë¥¼ íŒŒì‹±í•˜ì—¬ CSV writerì— ì¶”ê°€"""

    print(f"ê²½ê¸°ë„ ë‰´ìŠ¤í¬í„¸ RSS í”¼ë“œ íŒŒì‹± ì¤‘: {rss_url}")

    # RSS íŒŒì‹±
    try:
        headers = {"User-Agent": get_random_user_agent()}
        response = requests.get(rss_url, headers=headers, timeout=10)
        feed = feedparser.parse(response.content)
    except:
        feed = feedparser.parse(rss_url)

    if not feed.entries:
        print("âŒ RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return 0

    print(f"âœ… RSSì—ì„œ {len(feed.entries)}ê°œ ê¸°ì‚¬ ë°œê²¬")

    success_count = 0
    total_count = min(len(feed.entries), max_articles)

    print(f"ì´ {total_count}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘...\n")

    for i, entry in enumerate(feed.entries[:max_articles]):
        try:
            # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
            title = entry.title.strip()
            # CDATA ì œê±° (ì•ˆì „í•œ ë°©ë²•)
            if "<![CDATA[" in title:
                title = title.replace("<![CDATA[", "").replace("]]>", "")

            link = entry.link

            # RSS deion ì •ë³´ ì¶”ì¶œ (ê¸°ìëª… | ë‚ ì§œ í˜•ì‹)
            reporter = ""

            # deion í•„ë“œì—ì„œ ê¸°ìëª… ì •ë³´ ì¶”ì¶œ
            deion_value = entry.get("deion")
            if deion_value:
                deion_value = deion_value.strip()
                reporter = extract_reporter_from_description(deion_value)
                print(f"    deionì—ì„œ ê¸°ìëª… ì •ë³´ ë°œê²¬: {deion_value} â†’ {reporter}")
            elif hasattr(entry, "deion"):
                deion_value = entry.deion.strip()
                reporter = extract_reporter_from_description(deion_value)
                print(f"    deion ì†ì„±ì—ì„œ ê¸°ìëª… ì •ë³´ ë°œê²¬: {deion_value} â†’ {reporter}")
            else:
                # deionì´ ì—†ìœ¼ë©´ descriptionì—ì„œ ì°¾ê¸°
                description_for_reporter = ""
                if hasattr(entry, "description"):
                    description_for_reporter = entry.description.strip()
                    description_for_reporter = re.sub(r"<[^>]+>", "", description_for_reporter)
                    reporter = extract_reporter_from_description(description_for_reporter)
                    print(f"    descriptionì—ì„œ ê¸°ìëª… ì •ë³´ ë°œê²¬: {description_for_reporter} â†’ {reporter}")
                elif hasattr(entry, "summary"):
                    description_for_reporter = entry.summary.strip()
                    description_for_reporter = re.sub(r"<[^>]+>", "", description_for_reporter)
                    reporter = extract_reporter_from_description(description_for_reporter)
                    print(f"    summaryì—ì„œ ê¸°ìëª… ì •ë³´ ë°œê²¬: {description_for_reporter} â†’ {reporter}")
                else:
                    print(f"    deion/description/summary í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")

            # ë³¸ë¬¸ ì¶”ì¶œìš© description ë³„ë„ ì²˜ë¦¬
            description = ""
            if hasattr(entry, "description"):
                description = entry.description.strip()
                description = re.sub(r"<[^>]+>", "", description)
            elif hasattr(entry, "summary"):
                description = entry.summary.strip()
                description = re.sub(r"<[^>]+>", "", description)

            # ë‚ ì§œ í˜•ì‹ ë³€í™˜
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S")
            elif hasattr(entry, "pubdate_parsed") and entry.pubdate_parsed:
                date = datetime(*entry.pubdate_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S")
            else:
                date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            print(f"[{i+1}/{total_count}] {title[:60]}...")

            # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (ê¸°ìëª…ì€ ì´ë¯¸ ì¶”ì¶œë¨)
            _, content = extract_gnews_article_content(link, description)

            # ìµœì†Œ ì¡°ê±´ í™•ì¸
            if len(content.strip()) < 20:
                print(f"    âš  ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ ê±´ë„ˆëœ€ (ê¸¸ì´: {len(content)})\n")
                continue

            # CSVì— ì“°ê¸° - ì–¸ë¡ ì‚¬, ì œëª©, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, ê¸°ìëª…, ë³¸ë¬¸ ìˆœ
            writer.writerow(
                {
                    "ì–¸ë¡ ì‚¬": "ê²½ê¸°ë„ë‰´ìŠ¤í¬í„¸",
                    "ì œëª©": title,
                    "ë‚ ì§œ": date,
                    "ì¹´í…Œê³ ë¦¬": category,
                    "ê¸°ìëª…": reporter if reporter else "ë¯¸ìƒ",
                    "ë³¸ë¬¸": content,
                }
            )

            success_count += 1
            print(f"    âœ… ì„±ê³µ! (ê¸°ì: {reporter if reporter else 'ë¯¸ìƒ'}, ë³¸ë¬¸: {len(content)}ì)")

            # ì§„í–‰ë¥  í‘œì‹œ
            if (i + 1) % 5 == 0:
                print(f"\nğŸ“Š ì§„í–‰ë¥ : {i+1}/{total_count} ({(i+1)/total_count*100:.1f}%)")
                print(f"ğŸ“ˆ ì„±ê³µë¥ : {success_count}/{i+1} ({success_count/(i+1)*100:.1f}%)\n")

            # ëœë¤ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            delay = random.uniform(1.5, 2.5)
            time.sleep(delay)

        except KeyboardInterrupt:
            print("\nâš  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"    âŒ ì˜¤ë¥˜: {e}")
            continue

    print(f"\n{'='*70}")
    print(f"ğŸ‰ {category} ì¹´í…Œê³ ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“Š ê²°ê³¼: {success_count}/{total_count}ê°œ ì„±ê³µ ({success_count/total_count*100:.1f}%)")
    print(f"{'='*70}")

    return success_count


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ê²½ê¸°ë„ ë‰´ìŠ¤í¬í„¸ RSS URL ì˜µì…˜ë“¤
    gnews_rss_options = {
        "ì •ì¹˜": "https://gnews.gg.go.kr/rss/categoryRssSearch.do?kwd=E001&policyCode=E001",
        "ë³µì§€": "https://gnews.gg.go.kr/rss/categoryRssSearch.do?kwd=E002&policyCode=E002",
        "êµìœ¡": "https://gnews.gg.go.kr/rss/categoryRssSearch.do?kwd=E003&policyCode=E003",
        "ì£¼íƒ": "https://gnews.gg.go.kr/rss/categoryRssSearch.do?kwd=E004&policyCode=E004",
        "í™˜ê²½": "https://gnews.gg.go.kr/rss/categoryRssSearch.do?kwd=E005&policyCode=E005",
        "ë¬¸í™”": "https://gnews.gg.go.kr/rss/categoryRssSearch.do?kwd=E006&policyCode=E006",
        "êµí†µ": "https://gnews.gg.go.kr/rss/categoryRssSearch.do?kwd=E007&policyCode=E007",
        "ì•ˆì „": "https://gnews.gg.go.kr/rss/categoryRssSearch.do?kwd=E008&policyCode=E008",
        "ë³´ë„ìë£Œ": "https://gnews.gg.go.kr/rss/gnewsRssBodo.do",
        "ê²½ê¸°ë‰´ìŠ¤ê´‘ì¥": "https://gnews.gg.go.kr/rss/gnewsZoneRss.do",
        "ì¼ì¼ë‰´ìŠ¤": "https://gnews.gg.go.kr/rss/gnewsDailyRss.do",
        "ë‚˜ì˜ê²½ê¸°ë„": "https://gnews.gg.go.kr/rss/gnewsMyGyeonggiRss.do",
    }

    # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ìë™ìœ¼ë¡œ 20ê°œì”© ìˆ˜ì§‘í•˜ì—¬ í•˜ë‚˜ì˜ CSV íŒŒì¼ì— ì €ì¥
    print("ê²½ê¸°ë„ ë‰´ìŠ¤í¬í„¸ RSS ìë™ ìˆ˜ì§‘ê¸°")
    print("=" * 60)
    print("ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ ê°ê° 20ê°œì”© ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ì—¬ í•˜ë‚˜ì˜ CSV íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.")
    print("=" * 60)

    max_articles = 20
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    output_file = f"results/ê²½ê¸°ë„ë‰´ìŠ¤í¬í„¸_ì „ì²´_{timestamp}.csv"

    total_categories = len(gnews_rss_options)
    current_category = 0
    total_success_count = 0

    # ë‹¨ì¼ CSV íŒŒì¼ ìƒì„±
    with open(output_file, "w", newline="", encoding="utf-8-sig") as csvfile:
        fieldnames = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        for category, rss_url in gnews_rss_options.items():
            current_category += 1

            print(f"\nğŸš€ [{current_category}/{total_categories}] {category} ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì‹œì‘!")
            print(f"ğŸ”— RSS URL: {rss_url}")
            print("-" * 60)

            try:
                # ì‹¤í–‰
                success_count = fetch_gnews_rss_to_csv(rss_url, category, writer, max_articles)
                total_success_count += success_count

                print(f"âœ… {category} ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì™„ë£Œ! ({success_count}ê°œ ê¸°ì‚¬)")

                # ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì „ì— ì ì‹œ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                if current_category < total_categories:
                    print("â³ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ì„ ìœ„í•´ 3ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(3)

            except KeyboardInterrupt:
                print(f"\nâš  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤. ({current_category}/{total_categories} ì™„ë£Œ)")
                break
            except Exception as e:
                print(f"âŒ {category} ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

    print(f"\n{'='*60}")
    print(f"ğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ!")
    print(f"ğŸ“Š ì²˜ë¦¬ëœ ì¹´í…Œê³ ë¦¬: {current_category}/{total_categories}")
    print(f"ğŸ“ˆ ì´ ìˆ˜ì§‘ ê¸°ì‚¬: {total_success_count}ê°œ")
    print(f"ğŸ“ ì €ì¥ íŒŒì¼: {output_file}")
    print(f"{'='*60}")
