import requests
import xml.etree.ElementTree as ET
import csv
import re
from datetime import datetime
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import time
import json
from email.utils import parsedate_tz
import html

def parse_vop_rss_to_csv():
    """
    ë¯¼ì¤‘ì˜ì†Œë¦¬ RSS í”¼ë“œë¥¼ íŒŒì‹±í•˜ì—¬ ì œëª©/ë‚ ì§œ/ê¸°ìëª…/ë³¸ë¬¸ ìˆœìœ¼ë¡œ CSV íŒŒì¼ì— ì €ì¥
    """
    
    # ë¯¼ì¤‘ì˜ì†Œë¦¬ RSS URL (HTTP ì‚¬ìš©)
    rss_url = "http://www.vop.co.kr/rss"
    
    # CSV íŒŒì¼ëª… (í˜„ì¬ ë‚ ì§œ ê¸°ì¤€)
    csv_filename = f"results/vop_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    
    try:
        # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
        print("ë¯¼ì¤‘ì˜ì†Œë¦¬ RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
        }
        
        response = requests.get(rss_url, headers=headers, timeout=30, verify=False)
        response.raise_for_status()
        
        # XML íŒŒì‹±
        root = ET.fromstring(response.content)
        
        # ê¸°ì‚¬ ëª©ë¡ ì¶”ì¶œ
        items = root.findall('.//item')
        print(f"ì´ {len(items)}ê°œì˜ ê¸°ì‚¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        
        if len(items) == 0:
            print("âš ï¸ RSSì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìµœì‹  ê¸°ì‚¬ë¥¼ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê² ìŠµë‹ˆë‹¤.")
            return parse_vop_website_to_csv()
        
        # ì„¸ì…˜ ìƒì„± (ì—°ê²° ì¬ì‚¬ìš©)
        session = requests.Session()
        session.headers.update(headers)
        session.verify = False
        
        # CSV íŒŒì¼ ìƒì„±
        with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.writer(csvfile)
            # í—¤ë” ì‘ì„±
            writer.writerow(['ì œëª©', 'ë‚ ì§œ', 'ê¸°ìëª…', 'ë³¸ë¬¸'])
            
            for i, item in enumerate(items, 1):
                try:
                    # RSSì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                    title = item.find('title').text if item.find('title') is not None else ""
                    link = item.find('link').text if item.find('link') is not None else ""
                    pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ""
                    
                    # CDATA íƒœê·¸ ì œê±°
                    title = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', title)
                    title = html.unescape(title)
                    
                    # ë‚ ì§œ í¬ë§· ì •ë¦¬
                    formatted_date = format_date(pub_date)
                    
                    print(f"ì²˜ë¦¬ ì¤‘: {i}/{len(items)} - {title[:50]}...")
                    
                    # ê°œë³„ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                    article_content, reporter, article_date = get_vop_article_details(session, link)
                    
                    # RSS ë‚ ì§œê°€ ì—†ìœ¼ë©´ ê¸°ì‚¬ì—ì„œ ì¶”ì¶œí•œ ë‚ ì§œ ì‚¬ìš©
                    final_date = formatted_date or article_date
                    
                    # CSVì— ë°ì´í„° ì“°ê¸°
                    writer.writerow([title, final_date, reporter, article_content])
                    
                    # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
                    time.sleep(1.5)
                    
                except Exception as e:
                    print(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    # ì˜¤ë¥˜ ë°œìƒì‹œì—ë„ ê¸°ë³¸ ì •ë³´ëŠ” ì €ì¥
                    writer.writerow([title, formatted_date, "ì •ë³´ì—†ìŒ", "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"])
                    continue
        
        print(f"\nâœ… CSV íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {csv_filename}")
        print(f"ğŸ“Š ì´ {len(items)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ")
        return csv_filename
        
    except Exception as e:
        print(f"âŒ RSS íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ğŸ”„ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ ìµœì‹  ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¤ê² ìŠµë‹ˆë‹¤...")
        return parse_vop_website_to_csv()

def parse_vop_website_to_csv():
    """
    ë¯¼ì¤‘ì˜ì†Œë¦¬ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ ìµœì‹  ê¸°ì‚¬ë¥¼ ê°€ì ¸ì™€ì„œ CSVë¡œ ì €ì¥
    """
    csv_filename = f"results/vop_news_web_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
        }
        
        session = requests.Session()
        session.headers.update(headers)
        session.verify = False
        
        # ë¯¼ì¤‘ì˜ì†Œë¦¬ ë©”ì¸í˜ì´ì§€ì—ì„œ ìµœì‹  ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
        print("ë¯¼ì¤‘ì˜ì†Œë¦¬ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        response = session.get("http://www.vop.co.kr/", timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ê¸°ì‚¬ ë§í¬ ì°¾ê¸° (ë¯¼ì¤‘ì˜ì†Œë¦¬ íŠ¹í™”)
        article_links = []
        
        # ë‹¤ì–‘í•œ ê¸°ì‚¬ ë§í¬ íŒ¨í„´ ì‹œë„
        link_patterns = [
            'a[href*="/A000"]',  # ë¯¼ì¤‘ì˜ì†Œë¦¬ ê¸°ì‚¬ ID íŒ¨í„´
            '.article-list a',
            '.news-list a',
            '.main-news a',
            'a[href*="vop.co.kr/A"]'
        ]
        
        for pattern in link_patterns:
            links = soup.select(pattern)
            for link in links:
                href = link.get('href')
                if href and '/A000' in href:
                    if href.startswith('/'):
                        href = 'http://www.vop.co.kr' + href
                    elif not href.startswith('http'):
                        href = 'http://www.vop.co.kr/' + href
                    
                    if href not in article_links:
                        article_links.append(href)
            
            if article_links:
                break
        
        # ì¶”ê°€ë¡œ í…ìŠ¤íŠ¸ì—ì„œ ê¸°ì‚¬ ë§í¬ ì°¾ê¸°
        if not article_links:
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link['href']
                if '/A000' in href:
                    if href.startswith('/'):
                        href = 'http://www.vop.co.kr' + href
                    elif not href.startswith('http'):
                        href = 'http://www.vop.co.kr/' + href
                    article_links.append(href)
        
        # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 20ê°œë§Œ
        article_links = list(set(article_links))[:20]
        
        if not article_links:
            print("âŒ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        print(f"ì´ {len(article_links)}ê°œì˜ ê¸°ì‚¬ ë§í¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        
        # CSV íŒŒì¼ ìƒì„±
        with open(csv_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.writer(csvfile)
            # í—¤ë” ì‘ì„±
            writer.writerow(['ì œëª©', 'ë‚ ì§œ', 'ê¸°ìëª…', 'ë³¸ë¬¸'])
            
            for i, link in enumerate(article_links, 1):
                try:
                    print(f"ì²˜ë¦¬ ì¤‘: {i}/{len(article_links)} - {link}")
                    
                    # ê°œë³„ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ì •ë³´ ì¶”ì¶œ
                    article_content, reporter, article_date, title = get_vop_article_details_full(session, link)
                    
                    # CSVì— ë°ì´í„° ì“°ê¸°
                    writer.writerow([title, article_date, reporter, article_content])
                    
                    # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
                    time.sleep(2)
                    
                except Exception as e:
                    print(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue
        
        print(f"\nâœ… CSV íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {csv_filename}")
        return csv_filename
        
    except Exception as e:
        print(f"âŒ ì›¹ì‚¬ì´íŠ¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def format_date(date_string):
    """
    ë‚ ì§œ ë¬¸ìì—´ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    """
    if not date_string:
        return ""
    
    try:
        # RFC 2822 í˜•ì‹ íŒŒì‹± ì‹œë„
        parsed = parsedate_tz(date_string)
        if parsed:
            timestamp = time.mktime(parsed[:9])
            return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')
    except:
        pass
    
    # ISO í˜•ì‹ ì‹œë„
    try:
        date_string = re.sub(r'([+-]\d{2}):(\d{2})$', r'\1\2', date_string)
        dt = datetime.fromisoformat(date_string.replace('Z', '+00:00'))
        return dt.strftime('%Y-%m-%d %H:%M:%S')
    except:
        pass
    
    return date_string

def get_vop_article_details(session, url):
    """
    ë¯¼ì¤‘ì˜ì†Œë¦¬ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸, ê¸°ìëª…, ë‚ ì§œë¥¼ ì¶”ì¶œ
    """
    try:
        response = session.get(url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ë³¸ë¬¸ ì¶”ì¶œ (ë¯¼ì¤‘ì˜ì†Œë¦¬ íŠ¹í™”)
        content = extract_vop_content(soup)
        
        # ê¸°ìëª… ì¶”ì¶œ (ë¯¼ì¤‘ì˜ì†Œë¦¬ íŠ¹í™”)
        reporter = extract_vop_reporter(soup, content)
        
        # ê¸°ì‚¬ ë‚ ì§œ ì¶”ì¶œ
        article_date = extract_vop_date(soup)
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        content = html.unescape(content)
        reporter = html.unescape(reporter)
            
        return content, reporter, article_date
        
    except Exception as e:
        print(f"  âš ï¸ ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨", "ì •ë³´ì—†ìŒ", ""

def get_vop_article_details_full(session, url):
    """
    ë¯¼ì¤‘ì˜ì†Œë¦¬ ê¸°ì‚¬ URLì—ì„œ ì œëª©, ë³¸ë¬¸, ê¸°ìëª…, ë‚ ì§œë¥¼ ëª¨ë‘ ì¶”ì¶œ
    """
    try:
        response = session.get(url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ
        title = extract_vop_title(soup)
        
        # ë³¸ë¬¸ ì¶”ì¶œ
        content = extract_vop_content(soup)
        
        # ê¸°ìëª… ì¶”ì¶œ
        reporter = extract_vop_reporter(soup, content)
        
        # ê¸°ì‚¬ ë‚ ì§œ ì¶”ì¶œ
        article_date = extract_vop_date(soup)
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        title = html.unescape(title)
        content = html.unescape(content)
        reporter = html.unescape(reporter)
            
        return content, reporter, article_date, title
        
    except Exception as e:
        print(f"  âš ï¸ ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨", "ì •ë³´ì—†ìŒ", "", "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"

def extract_vop_title(soup):
    """
    ë¯¼ì¤‘ì˜ì†Œë¦¬ ê¸°ì‚¬ ì œëª© ì¶”ì¶œ
    """
    # ì œëª© ì„ íƒìë“¤
    title_selectors = [
        'h1.title',
        'h1',
        '.article-title',
        '.news-title',
        'title'
    ]
    
    for selector in title_selectors:
        element = soup.select_one(selector)
        if element:
            title = element.get_text(strip=True)
            if title and len(title) > 5:
                # "ë¯¼ì¤‘ì˜ì†Œë¦¬" ë“± ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
                title = re.sub(r'\s*-\s*ë¯¼ì¤‘ì˜ì†Œë¦¬.*$', '', title)
                return title
    
    return "ì œëª© ì—†ìŒ"

def extract_vop_content(soup):
    """
    ë¯¼ì¤‘ì˜ì†Œë¦¬ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
    """
    content = ""
    
    # ë¯¼ì¤‘ì˜ì†Œë¦¬ ë³¸ë¬¸ ì„ íƒìë“¤
    content_selectors = [
        '.article-content',
        '.news-content', 
        '.content',
        '#article-content',
        '.article-body',
        'div[class*="content"]'
    ]
    
    for selector in content_selectors:
        elements = soup.select(selector)
        if elements:
            content_parts = []
            for element in elements:
                # ê´‘ê³ ë‚˜ ê´€ë ¨ê¸°ì‚¬ ë“± ì œê±°
                for unwanted in element.find_all(['script', 'style', 'aside', '.ad', '.related', '.recommend', '.banner']):
                    unwanted.decompose()
                
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = element.get_text(separator=' ', strip=True)
                if text and len(text) > 50:
                    # ë¶ˆí•„ìš”í•œ ë‚´ìš© í•„í„°ë§
                    lines = text.split('\n')
                    filtered_lines = []
                    for line in lines:
                        line = line.strip()
                        if len(line) > 15 and not is_vop_unwanted_content(line):
                            filtered_lines.append(line)
                    
                    if filtered_lines:
                        content = ' '.join(filtered_lines)
                        break
            
            if content:
                break
    
    # ì „ì²´ ë³¸ë¬¸ì—ì„œ ì¶”ì¶œ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
    if not content:
        # íŠ¹ì • íŒ¨í„´ìœ¼ë¡œ ë³¸ë¬¸ ì°¾ê¸°
        paragraphs = soup.find_all('p')
        content_parts = []
        for p in paragraphs:
            text = p.get_text(strip=True)
            if len(text) > 30 and not is_vop_unwanted_content(text):
                content_parts.append(text)
        
        if content_parts:
            content = ' '.join(content_parts[:10])  # ì²˜ìŒ 10ê°œ ë¬¸ë‹¨
    
    # ë³¸ë¬¸ ì •ë¦¬ (ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°)
    if len(content) > 2000:
        content = content[:2000] + "..."
    
    return content

def extract_vop_reporter(soup, content):
    """
    ë¯¼ì¤‘ì˜ì†Œë¦¬ ê¸°ìëª… ì¶”ì¶œ
    """
    reporter = "ê¸°ìëª… ì—†ìŒ"
    
    # 1. CSS ì„ íƒìë¡œ ê¸°ìëª… ì°¾ê¸°
    reporter_selectors = [
        '.reporter',
        '.writer', 
        '.author',
        '.byline',
        '.journalist',
        '.article-info .reporter',
        '.article-info .writer',
        'span[class*="reporter"]',
        'span[class*="writer"]'
    ]
    
    for selector in reporter_selectors:
        elements = soup.select(selector)
        for element in elements:
            text = element.get_text(strip=True)
            if text and ('ê¸°ì' in text or 'íŠ¹íŒŒì›' in text):
                return clean_vop_reporter_name(text)
    
    # 2. ë³¸ë¬¸ì—ì„œ ê¸°ìëª… íŒ¨í„´ ì°¾ê¸°
    if content:
        # ë¯¼ì¤‘ì˜ì†Œë¦¬ íŠ¹í™” íŒ¨í„´
        patterns = [
            r'([ê°€-í£]{2,4})\s*(ê¸°ì|íŠ¹íŒŒì›)\s*ì‘ì›í•˜ê¸°',  # "ê¸°ì ì‘ì›í•˜ê¸°" íŒ¨í„´
            r'([ê°€-í£]{2,4})\s*(ê¸°ì|íŠ¹íŒŒì›)$',           # ì¤„ ëì— ìˆëŠ” ê²½ìš°
            r'([ê°€-í£]{2,4})\s*(ê¸°ì|íŠ¹íŒŒì›)\s*=',        # "ê¸°ì=" í˜•íƒœ
            r'ê¸°ì\s*([ê°€-í£]{2,4})',                      # "ê¸°ì ì´ë¦„"
            r'([ê°€-í£]{2,4})\s*([ê°€-í£]{2,4})\s*(ê¸°ì|íŠ¹íŒŒì›)',  # "ì„± ì´ë¦„ ê¸°ì"
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content)
            if matches:
                match = matches[-1]  # ë§ˆì§€ë§‰ ë§¤ì¹˜ ì‚¬ìš©
                if isinstance(match, tuple):
                    if len(match) >= 2:
                        name = match[0] if len(match[0]) >= 2 else match[0] + match[1]
                        title = match[-1]
                        return f"{name} {title}"
                break
    
    # 3. í˜ì´ì§€ í•˜ë‹¨ì—ì„œ ì°¾ê¸°
    footer_text = soup.get_text()
    footer_patterns = [
        r'([ê°€-í£]{2,4})\s*(ê¸°ì|íŠ¹íŒŒì›)\s*ì‘ì›í•˜ê¸°',
        r'([ê°€-í£]{2,4})\s*(ê¸°ì|íŠ¹íŒŒì›)\s*\w*@\w*'
    ]
    
    for pattern in footer_patterns:
        matches = re.findall(pattern, footer_text)
        if matches:
            match = matches[-1]
            if isinstance(match, tuple):
                name = match[0]
                title = match[1]
                return f"{name} {title}"
    
    return reporter

def extract_vop_date(soup):
    """
    ë¯¼ì¤‘ì˜ì†Œë¦¬ ê¸°ì‚¬ ë‚ ì§œ ì¶”ì¶œ
    """
    # ë‚ ì§œ ì„ íƒìë“¤
    date_selectors = [
        '.date',
        '.article-date',
        '.news-date',
        '.pub-date',
        '.published',
        '.article-info .date',
        'time'
    ]
    
    for selector in date_selectors:
        element = soup.select_one(selector)
        if element:
            date_text = element.get_text(strip=True)
            # ë‚ ì§œ í˜•ì‹ ì •ê·œí™”
            date_match = re.search(r'(\d{4})[.-](\d{1,2})[.-](\d{1,2})', date_text)
            if date_match:
                year, month, day = date_match.groups()
                return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
    
    # ë³¸ë¬¸ì—ì„œ ë‚ ì§œ ì°¾ê¸°
    text = soup.get_text()
    date_patterns = [
        r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
        r'(\d{4})[.-](\d{1,2})[.-](\d{1,2})',
        r'(\d{4})/(\d{1,2})/(\d{1,2})'
    ]
    
    for pattern in date_patterns:
        matches = re.findall(pattern, text)
        if matches:
            year, month, day = matches[0]
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
    
    return ""

def clean_vop_reporter_name(name):
    """
    ë¯¼ì¤‘ì˜ì†Œë¦¬ ê¸°ìëª… ì •ë¦¬
    """
    if not name:
        return "ê¸°ìëª… ì—†ìŒ"
    
    # HTML íƒœê·¸ ì œê±°
    name = re.sub(r'<[^>]+>', '', name)
    
    # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
    name = re.sub(r'[^\w\sê°€-í£Â·]', ' ', name)
    name = re.sub(r'\s+', ' ', name).strip()
    
    # "ì‘ì›í•˜ê¸°" ë“± ì œê±°
    name = re.sub(r'\s*(ì‘ì›í•˜ê¸°|í›„ì›í•˜ê¸°)\s*', '', name)
    
    # ì´ë©”ì¼ ì£¼ì†Œ ì œê±°
    name = re.sub(r'\S+@\S+', '', name).strip()
    
    # "ê¸°ì", "íŠ¹íŒŒì›" ë“±ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if any(title in name for title in ['ê¸°ì', 'íŠ¹íŒŒì›', 'ë…¼ì„¤ìœ„ì›', 'í¸ì§‘ìœ„ì›']):
        return name
    
    # ì´ë¦„ë§Œ ìˆëŠ” ê²½ìš° " ê¸°ì" ì¶”ê°€
    if name and name != "ê¸°ìëª… ì—†ìŒ" and len(name) >= 2:
        return name + " ê¸°ì"
    
    return "ê¸°ìëª… ì—†ìŒ"

def is_vop_unwanted_content(text):
    """
    ë¯¼ì¤‘ì˜ì†Œë¦¬ ë¶ˆí•„ìš”í•œ ë‚´ìš© í•„í„°ë§
    """
    unwanted_patterns = [
        'êµ¬ë…í•˜ê¸°', 'ì¢‹ì•„ìš”', 'ëŒ“ê¸€', 'ê³µìœ ', 'ì‹ ê³ ', 'ì €ì‘ê¶Œ',
        'ê´€ë ¨ê¸°ì‚¬', 'ì´ì „ê¸°ì‚¬', 'ë‹¤ìŒê¸°ì‚¬', 'ì¶”ì²œê¸°ì‚¬', 'ì¸ê¸°ê¸°ì‚¬',
        'ê´‘ê³ ', 'AD', 'í”„ë¦¬ë¯¸ì—„', 'êµ¬ë…', 'ë¡œê·¸ì¸', 'íšŒì›ê°€ì…',
        'ì¹´ì¹´ì˜¤í†¡', 'í˜ì´ìŠ¤ë¶', 'íŠ¸ìœ„í„°', 'ë„¤ì´ë²„', 'êµ¬ê¸€',
        'â“’', 'ë¬´ë‹¨ì „ì¬', 'ì¬ë°°í¬ê¸ˆì§€', 'Copyright', 'ì €ì‘ê¶Œì',
        'ë¯¼ì¤‘ì˜ì†Œë¦¬ë¥¼ ì‘ì›í•´ì£¼ì„¸ìš”', 'í›„ì›íšŒì›ì´ ë˜ì–´ì£¼ì„¸ìš”', 'ê¸°ì ì‘ì›í•˜ê¸°',
        'ì •ê¸°í›„ì›', 'ê¸°ìí›„ì›', 'ë…ìë‹˜ì˜ ì‘ì›', 'ë…ìë‹˜ì˜ í›„ì›ê¸ˆ',
        'ê¸°ì‚¬ ì˜ ë³´ì…¨ë‚˜ìš”', 'ë…ìë‹˜ì˜ ì‘ì›ì´', 'í›„ì›íšŒì›',
        'í”„ë¦°íŠ¸', 'ì´ë©”ì¼', 'ìŠ¤í¬ë©', 'ê¸€ìí¬ê¸°', 'í°íŠ¸'
    ]
    
    return any(pattern in text for pattern in unwanted_patterns)

def print_vop_sample_data(csv_filename):
    """
    ìƒì„±ëœ CSV íŒŒì¼ì˜ ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
    """
    try:
        with open(csv_filename, 'r', encoding='utf-8-sig') as f:
            reader = csv.reader(f)
            rows = list(reader)
            
        print("\nğŸ“‹ ìƒì„±ëœ ë°ì´í„° ìƒ˜í”Œ:")
        print("=" * 60)
        
        # í—¤ë” ì¶œë ¥
        if rows:
            print(f"ì»¬ëŸ¼: {' | '.join(rows[0])}")
            print("-" * 60)
            
            # ì²˜ìŒ 3ê°œ ë°ì´í„° ìƒ˜í”Œ ì¶œë ¥
            for i in range(1, min(4, len(rows))):
                row = rows[i]
                print(f"ê¸°ì‚¬ {i}:")
                print(f"  ì œëª©: {row[0][:50]}...")
                print(f"  ë‚ ì§œ: {row[1]}")
                print(f"  ê¸°ì: {row[2]}")
                print(f"  ë³¸ë¬¸: {row[3][:100]}...")
                print()
                
    except Exception as e:
        print(f"ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥ ì˜¤ë¥˜: {e}")

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("ğŸ“° ë¯¼ì¤‘ì˜ì†Œë¦¬ RSS/ì›¹ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ê¸°")
    print("=" * 60)
    print("ğŸ“° ê¸°ëŠ¥: RSS í”¼ë“œ ë˜ëŠ” ì›¹ì‚¬ì´íŠ¸ â†’ ì œëª©/ë‚ ì§œ/ê¸°ìëª…/ë³¸ë¬¸ ì¶”ì¶œ â†’ CSV ì €ì¥")
    print("â±ï¸ ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 2-4ë¶„")
    print("=" * 60)
    
    start_time = time.time()
    csv_file = parse_vop_rss_to_csv()
    end_time = time.time()
    
    if csv_file:
        print("=" * 60)
        print("âœ… ë°ì´í„° ìˆ˜ì§‘ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼: {csv_file}")
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {int(end_time - start_time)}ì´ˆ")
        
        # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
        print_vop_sample_data(csv_file)
        
        print("\nğŸ”§ ë¯¼ì¤‘ì˜ì†Œë¦¬ íŠ¹í™” ê¸°ëŠ¥:")
        print("  â€¢ RSS í”¼ë“œ ìš°ì„ , ì‹¤íŒ¨ì‹œ ì›¹ì‚¬ì´íŠ¸ ì§ì ‘ í¬ë¡¤ë§")
        print("  â€¢ ë¯¼ì¤‘ì˜ì†Œë¦¬ ê¸°ì‚¬ êµ¬ì¡° ìµœì í™”")
        print("  â€¢ 'ê¸°ì ì‘ì›í•˜ê¸°' íŒ¨í„´ ì¸ì‹")
        print("  â€¢ HTTP/HTTPS ìë™ ì²˜ë¦¬")
        print("  â€¢ SSL ì¸ì¦ì„œ ë¬¸ì œ íšŒí”¼")
        
    else:
        print("âŒ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ì¸í„°ë„· ì—°ê²°ì´ë‚˜ ì›¹ì‚¬ì´íŠ¸ ì ‘ê·¼ì„±ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
