#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ì œì£¼ë‰´ìŠ¤(ì œì£¼ì¼ë³´) RSS ìˆ˜ì§‘ê¸°
Created: 2025ë…„ 8ì›”
Description: ì œì£¼ë‰´ìŠ¤(www.jejunews.com)ì˜ RSS í”¼ë“œë¥¼ ìˆ˜ì§‘í•˜ì—¬ CSV íŒŒì¼ë¡œ ì €ì¥
"""

import feedparser
import requests
from bs4 import BeautifulSoup
import csv
import time
import random
from datetime import datetime
import re
import urllib.parse


class JejuNewsRSSCollector:
    def __init__(self):
        self.base_url = "http://www.jejunews.com"  # HTTP í”„ë¡œí† ì½œ ì‚¬ìš©
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
        ]

        # ì œì£¼ë‰´ìŠ¤ RSS í”¼ë“œ ì¹´í…Œê³ ë¦¬ (ì´ë¯¸ì§€ì—ì„œ í™•ì¸í•œ ì •í™•í•œ êµ¬ì¡°)
        self.rss_categories = {"ì „ì²´ê¸°ì‚¬": "allArticle.xml", "ì¸ê¸°ê¸°ì‚¬": "clickTop.xml"}

        self.session = requests.Session()

    def get_random_user_agent(self):
        """ëœë¤ User-Agent ë°˜í™˜"""
        return random.choice(self.user_agents)

    def clean_text(self, text):
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text:
            return ""

        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r"<[^>]+>", "", text)
        # íŠ¹ìˆ˜ë¬¸ì ì •ì œ
        text = re.sub(r"[\r\n\t]+", " ", text)
        text = re.sub(r"\s+", " ", text)
        # ë”°ì˜´í‘œ ì²˜ë¦¬
        text = text.replace('"', '""')

        return text.strip()

    def extract_reporter_name(self, article_url):
        """ê¸°ì‚¬ URLì—ì„œ ê¸°ìëª… ì¶”ì¶œ"""
        try:
            headers = {"User-Agent": self.get_random_user_agent()}
            response = self.session.get(article_url, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            # ì œì£¼ë‰´ìŠ¤(ì œì£¼ì¼ë³´) ê¸°ìëª… íŒ¨í„´ ì°¾ê¸°
            reporter_patterns = [
                # ê¸°ë³¸ ê¸°ìëª… íŒ¨í„´: "ì§„ì£¼ë¦¬ ê¸°ì"
                r"([ê°€-í£]{2,4})\s*ê¸°ì",
                # ì´ë©”ì¼ê³¼ í•¨ê»˜: "ì§„ì£¼ë¦¬ê¸°ì jjl@jejunews.com"
                r"([ê°€-í£]{2,4})ê¸°ì\s+[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
                # íƒœê·¸ ë‚´ ê¸°ìëª…
                r'<[^>]*class="reporter"[^>]*>([ê°€-í£]{2,4})',
                r'<[^>]*class="writer"[^>]*>([ê°€-í£]{2,4})',
                # ê¸°ì‚¬ ì •ë³´ ì˜ì—­
                r"ê¸°ìëª…\s*[:ï¼š]\s*([ê°€-í£]{2,4})",
                r"ê¸€\s*[:ï¼š]\s*([ê°€-í£]{2,4})",
                r"ì·¨ì¬\s*[:ï¼š]\s*([ê°€-í£]{2,4})",
                # ì œì£¼ì¼ë³´ íŠ¹ì„±: ê¸°ì‚¬ í•˜ë‹¨ ê¸°ìëª…
                r"([ê°€-í£]{2,4})\s*ê¸°ì\s*$",
                r"ì €ì‘ê¶Œì.*ì œì£¼ì¼ë³´.*ë¬´ë‹¨ì „ì¬.*ì¬ë°°í¬.*ê¸ˆì§€",  # ì €ì‘ê¶Œ ë¬¸êµ¬ ì „ ê¸°ìëª… í™•ì¸
                # ê¸°ì‚¬ ë ë‹¨ë… ê¸°ìëª… (ì œì£¼ì¼ë³´ íŠ¹ì„±)
                r"^([ê°€-í£]{2,4})\s*ê¸°ì\s*$",
            ]

            article_text = soup.get_text()

            for pattern in reporter_patterns:
                matches = re.findall(pattern, article_text, re.MULTILINE)
                if matches:
                    reporter_name = matches[-1].strip()
                    if len(reporter_name) >= 2 and not re.search(r"[0-9]", reporter_name):
                        # ì œì£¼ì¼ë³´ íŠ¹ì„±: íŠ¹ì • ë‹¨ì–´ ì œì™¸
                        if reporter_name not in ["ì œì£¼ì¼ë³´", "ì œì£¼ë‰´ìŠ¤", "ì €ì‘ê¶Œì", "ë¬´ë‹¨ì „ì¬", "ì¬ë°°í¬", "ê¸°ìëª…"]:
                            return reporter_name

        except Exception as e:
            print(f"ê¸°ìëª… ì¶”ì¶œ ì˜¤ë¥˜ ({article_url}): {e}")

        return "ì •ë³´ì—†ìŒ"

    def collect_rss_feed(self, category_name, rss_file):
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ RSS í”¼ë“œ ìˆ˜ì§‘"""
        rss_url = f"{self.base_url}/rss/{rss_file}"

        try:
            print(f"{category_name} ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì¤‘: {rss_url}")

            headers = {"User-Agent": self.get_random_user_agent()}
            response = self.session.get(rss_url, headers=headers, timeout=15)
            response.raise_for_status()

            # RSS íŒŒì‹±
            feed = feedparser.parse(response.content)

            if not feed.entries:
                print(f"âŒ {category_name}: RSS í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
                return []

            articles = []

            for entry in feed.entries:
                try:
                    # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                    title = self.clean_text(entry.title)
                    link = entry.link

                    # ë°œí–‰ì¼ ì²˜ë¦¬
                    pub_date = ""
                    if hasattr(entry, "published"):
                        try:
                            # ì œì£¼ë‰´ìŠ¤ì˜ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
                            from dateutil import parser

                            parsed_date = parser.parse(entry.published)
                            pub_date = parsed_date.strftime("%Y-%m-%d %H:%M:%S")
                        except:
                            pub_date = entry.published
                    elif hasattr(entry, "updated"):
                        try:
                            from dateutil import parser

                            parsed_date = parser.parse(entry.updated)
                            pub_date = parsed_date.strftime("%Y-%m-%d %H:%M:%S")
                        except:
                            pub_date = entry.updated

                    # ìš”ì•½ ë‚´ìš©
                    summary = ""
                    if hasattr(entry, "summary"):
                        summary = self.clean_text(entry.summary)
                    elif hasattr(entry, "description"):
                        summary = self.clean_text(entry.description)

                    # ì‘ì„±ì ì •ë³´ (RSSì—ì„œ ë¨¼ì € í™•ì¸)
                    reporter = "ì •ë³´ìˆ˜ì§‘ì¤‘"
                    if hasattr(entry, "author") and entry.author:
                        reporter = self.clean_text(entry.author)
                        # 'ì œì£¼ì¼ë³´'ì¸ ê²½ìš° ê³µì‹ ê¸°ì‚¬ë¡œ ì²˜ë¦¬
                        if "ì œì£¼ì¼ë³´" in reporter or "ì œì£¼ë‰´ìŠ¤" in reporter:
                            reporter = "ì œì£¼ì¼ë³´"
                    else:
                        # RSSì— ì‘ì„±ì ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ì‚¬ì—ì„œ ì¶”ì¶œ (ì„ íƒì )
                        if len(articles) < 5:  # ì²˜ìŒ 5ê°œ ê¸°ì‚¬ë§Œ ê¸°ìëª… ì¶”ì¶œ
                            reporter = self.extract_reporter_name(link)
                            time.sleep(random.uniform(0.5, 1.0))  # ìš”ì²­ ê°„ê²©

                    article_data = {
                        "category": category_name,
                        "title": title,
                        "link": link,
                        "published": pub_date,
                        "summary": summary,
                        "reporter": reporter,
                        "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    }

                    articles.append(article_data)

                except Exception as e:
                    print(f"ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue

            print(f"âœ… {category_name}: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
            return articles

        except Exception as e:
            print(f"âŒ {category_name} RSS ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    def save_to_csv(self, all_articles, filename=None):
        """ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì„ CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"results/ì œì£¼ë‰´ìŠ¤_RSS_{timestamp}.csv"

        try:
            with open(filename, "w", newline="", encoding="utf-8-sig") as csvfile:
                fieldnames = ["category", "title", "link", "published", "summary", "reporter", "collected_at"]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

                writer.writeheader()
                for article in all_articles:
                    writer.writerow(article)

            print(f"ğŸ“„ CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            return filename

        except Exception as e:
            print(f"âŒ CSV ì €ì¥ ì‹¤íŒ¨: {e}")
            return None

    def test_connection(self):
        """ë„ë©”ì¸ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            headers = {"User-Agent": self.get_random_user_agent()}
            response = self.session.get(f"{self.base_url}/rss/allArticle.xml", headers=headers, timeout=10)
            print(f"âœ… ë„ë©”ì¸ ì—°ê²° ì„±ê³µ: {self.base_url}")
            return True
        except Exception as e:
            print(f"âŒ ë„ë©”ì¸ ì—°ê²° ì‹¤íŒ¨: {e}")
            print("âš ï¸  ë„ë©”ì¸ ì£¼ì†Œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return False

    def collect_all_categories(self, selected_categories=None):
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ ë˜ëŠ” ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì˜ RSS ìˆ˜ì§‘"""
        if selected_categories is None:
            selected_categories = list(self.rss_categories.keys())

        print("ğŸ“° ì œì£¼ë‰´ìŠ¤(ì œì£¼ì¼ë³´) RSS ìˆ˜ì§‘ê¸° ì‹œì‘")
        print("=" * 50)

        # ë„ë©”ì¸ ì—°ê²° í…ŒìŠ¤íŠ¸
        print("ğŸ” ë„ë©”ì¸ ì—°ê²° ìƒíƒœ í™•ì¸ ì¤‘...")
        if not self.test_connection():
            print("âŒ ë„ë©”ì¸ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì´ë‚˜ ë„ë©”ì¸ ì£¼ì†Œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return []

        all_articles = []

        for category in selected_categories:
            if category in self.rss_categories:
                rss_file = self.rss_categories[category]
                articles = self.collect_rss_feed(category, rss_file)
                all_articles.extend(articles)

                # ìš”ì²­ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(random.uniform(1.0, 2.0))
            else:
                print(f"âš ï¸  ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬: {category}")

        print("=" * 50)
        print(f"ğŸ“Š ì´ ìˆ˜ì§‘ ê¸°ì‚¬: {len(all_articles)}ê°œ")

        if all_articles:
            saved_file = self.save_to_csv(all_articles)
            if saved_file:
                print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ! íŒŒì¼: {saved_file}")

        return all_articles


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    collector = JejuNewsRSSCollector()

    print("ğŸ“° ì œì£¼ë‰´ìŠ¤(ì œì£¼ì¼ë³´) RSS ìˆ˜ì§‘ê¸°")
    print("=" * 50)
    print("ğŸ“ ì œì£¼ì¼ë³´ì˜ ì˜¨ë¼ì¸ ë‰´ìŠ¤ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.")
    print("ğŸï¸  ì œì£¼íŠ¹ë³„ìì¹˜ë„ ëŒ€í‘œ ì–¸ë¡ ì‚¬ì˜ ìµœì‹  ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    print()
    print("ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬:")

    for i, category in enumerate(collector.rss_categories.keys(), 1):
        print(f"{i}. {category}")

    print("=" * 50)

    # ì‚¬ìš©ì ì„ íƒ
    choice = input("\nìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš” (ë²ˆí˜¸ ì…ë ¥, ì „ì²´ëŠ” 'all'): ").strip()

    category_list = list(collector.rss_categories.keys())

    if choice.lower() == "all":
        selected_categories = category_list
        print("ğŸ”„ ëª¨ë“  ì¹´í…Œê³ ë¦¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    else:
        try:
            if "," in choice:
                # ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ ì„ íƒ
                indices = [int(x.strip()) - 1 for x in choice.split(",")]
                selected_categories = [category_list[i] for i in indices if 0 <= i < len(category_list)]
                print(f"ğŸ”„ ì„ íƒëœ ì¹´í…Œê³ ë¦¬: {', '.join(selected_categories)}")
            else:
                # ë‹¨ì¼ ì¹´í…Œê³ ë¦¬ ì„ íƒ
                index = int(choice) - 1
                if 0 <= index < len(category_list):
                    selected_categories = [category_list[index]]
                    print(f"ğŸ”„ ì„ íƒëœ ì¹´í…Œê³ ë¦¬: {selected_categories[0]}")
                else:
                    print("âŒ ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                    return
        except ValueError:
            print("âŒ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return

    # RSS ìˆ˜ì§‘ ì‹¤í–‰
    articles = collector.collect_all_categories(selected_categories)

    if articles:
        print(f"\nğŸ‰ ì œì£¼ë‰´ìŠ¤ RSS ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ˆ ì´ {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸï¸  ì œì£¼ íŠ¹ì„±: ì œì£¼íŠ¹ë³„ìì¹˜ë„ ì§€ì—­ ë‰´ìŠ¤ ë° ì‚¬íšŒ ì´ìŠˆ")
        print(f"ğŸ“° ì–¸ë¡ ì‚¬: ì œì£¼ì¼ë³´ (jejunews.com)")

        # ê°„ë‹¨í•œ í†µê³„ ì •ë³´
        if len(articles) > 0:
            print(f"ğŸ“Š ìˆ˜ì§‘ í†µê³„:")
            category_counts = {}
            for article in articles:
                category = article["category"]
                category_counts[category] = category_counts.get(category, 0) + 1

            for category, count in category_counts.items():
                print(f"   - {category}: {count}ê°œ")

    else:
        print("\nâŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë„ë©”ì¸ ì£¼ì†Œë‚˜ ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()
