import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import pandas as pd
import time
import random
import re
from datetime import datetime
import csv
from urllib.parse import urljoin, urlparse
import logging


class SegyeNewsRSSCrawler:
    def __init__(self):
        """ì„¸ê³„ì¼ë³´ RSS í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.base_url = "https://www.segye.com"

        # ì„¸ê³„ì¼ë³´ ë³¸ì§€ RSS í”¼ë“œ (11ê°œ)
        self.segye_feeds = {
            "ì „ì²´ë‰´ìŠ¤": "https://www.segye.com/Articles/RSSList/segye_recent.xml",
            "ì •ì¹˜": "https://www.segye.com/Articles/RSSList/segye_politic.xml",
            "ê²½ì œ": "https://www.segye.com/Articles/RSSList/segye_economy.xml",
            "ì‚¬íšŒ": "https://www.segye.com/Articles/RSSList/segye_society.xml",
            "êµ­ì œ": "https://www.segye.com/Articles/RSSList/segye_international.xml",
            "ì „êµ­": "https://www.segye.com/Articles/RSSList/segye_local.xml",
            "ë¬¸í™”": "https://www.segye.com/Articles/RSSList/segye_culture.xml",
            "ì˜¤í”¼ë‹ˆì–¸": "https://www.segye.com/Articles/RSSList/segye_opinion.xml",
        }

        # ì „ì²´ RSS í”¼ë“œ ëª©ë¡
        self.all_feeds = self.segye_feeds

        # ë§¤ì²´ë³„ ë¶„ë¥˜ (í˜„ì¬ ì„¸ê³„ì¼ë³´ë§Œ ì§€ì›)
        self.media_classification = {
            "ì„¸ê³„ì¼ë³´": list(self.segye_feeds.keys()),
        }

        # ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ ë¶„ë¥˜
        self.category_groups = {
            "ì¢…í•©ë‰´ìŠ¤": ["ì „ì²´ë‰´ìŠ¤", "SW_ì „ì²´ë‰´ìŠ¤", "SF_ì „ì²´ë‰´ìŠ¤"],
            "ì •ì¹˜Â·ì‚¬íšŒ": ["ì •ì¹˜", "ì‚¬íšŒ", "êµ­ì œ", "ì „êµ­"],
            "ê²½ì œÂ·ê¸ˆìœµ": ["ê²½ì œ", "SF_ê¸ˆìœµ", "SF_ì‚°ì—…", "SF_ë¶€ë™ì‚°", "SF_ì¦ê¶Œ"],
            "ë¬¸í™”Â·ì—°ì˜ˆ": ["ë¬¸í™”", "ì—°ì˜ˆ", "SW_ì—°ì˜ˆ"],
            "ìŠ¤í¬ì¸ Â·ë¼ì´í”„": ["ìŠ¤í¬ì¸ ", "SW_ìŠ¤í¬ì¸ ", "SW_ë¼ì´í”„"],
            "ì˜¤í”¼ë‹ˆì–¸": ["ì˜¤í”¼ë‹ˆì–¸", "SF_ì˜¤í”¼ë‹ˆì–¸"],
            "íŠ¹ë³„": ["í¬í† ", "SF_CSR"],
        }

        # User-Agent ë¦¬ìŠ¤íŠ¸
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
        ]

        self.articles = []
        self.session = requests.Session()

        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
        self.logger = logging.getLogger(__name__)

    def get_random_headers(self):
        """ëœë¤ í—¤ë” ìƒì„±"""
        return {
            "User-Agent": random.choice(self.user_agents),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Referer": "https://www.segye.com/",
        }

    def random_delay(self, min_delay=1, max_delay=3):
        """ëœë¤ ë”œë ˆì´"""
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)

    def fetch_rss_feed(self, rss_url, max_retries=3):
        """RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°"""
        for attempt in range(max_retries):
            try:
                headers = self.get_random_headers()
                response = self.session.get(rss_url, headers=headers, timeout=30)
                response.raise_for_status()

                # ì¸ì½”ë”© ì²˜ë¦¬
                if response.encoding.lower() in ["euc-kr", "cp949"]:
                    response.encoding = "euc-kr"
                else:
                    response.encoding = "utf-8"

                return response.text
            except Exception as e:
                self.logger.warning(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    self.random_delay(2, 5)
                else:
                    self.logger.error(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ìµœì¢… ì‹¤íŒ¨: {rss_url}")
                    return None

    def parse_rss_feed(self, rss_content):
        """RSS í”¼ë“œ íŒŒì‹±"""
        try:
            root = ET.fromstring(rss_content)
            items = []

            for item in root.findall(".//item"):
                article_info = {}

                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title_elem = item.find("title")
                article_info["title"] = title_elem.text.strip() if title_elem is not None else ""

                link_elem = item.find("link")
                article_info["link"] = link_elem.text.strip() if link_elem is not None else ""

                pubdate_elem = item.find("pubDate")
                article_info["pub_date"] = pubdate_elem.text.strip() if pubdate_elem is not None else ""

                # author/creator ì¶”ì¶œ
                author_elem = item.find("author")
                if author_elem is None:
                    author_elem = item.find(".//{http://purl.org/dc/elements/1.1/}creator")
                article_info["author"] = author_elem.text.strip() if author_elem is not None else ""

                # descriptionì—ì„œ ê°„ë‹¨í•œ ë‚´ìš© ì¶”ì¶œ
                desc_elem = item.find("description")
                if desc_elem is not None:
                    desc_text = desc_elem.text or ""
                    # CDATA ì²˜ë¦¬
                    if desc_text.startswith("<![CDATA[") and desc_text.endswith("]]>"):
                        desc_text = desc_text[9:-3]

                    # HTML íƒœê·¸ ì œê±°í•˜ì—¬ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                    soup = BeautifulSoup(desc_text, "html.parser")
                    article_info["description"] = (
                        soup.get_text().strip()[:300] + "..."
                        if len(soup.get_text().strip()) > 300
                        else soup.get_text().strip()
                    )
                else:
                    article_info["description"] = ""

                items.append(article_info)

            return items
        except Exception as e:
            self.logger.error(f"RSS íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []

    def extract_article_content(self, article_url, max_retries=3):
        """ê°œë³„ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ - ì„¸ê³„ì¼ë³´ ê³„ì—´ í˜ì´ì§€ ìµœì í™”"""
        for attempt in range(max_retries):
            try:
                headers = self.get_random_headers()
                response = self.session.get(article_url, headers=headers, timeout=30)
                response.raise_for_status()

                # ì¸ì½”ë”© ì²˜ë¦¬
                if "segye.com" in article_url:
                    response.encoding = "utf-8"
                elif "sportsworld.com" in article_url:
                    response.encoding = "utf-8"
                elif "segyefn.com" in article_url:
                    response.encoding = "utf-8"

                soup = BeautifulSoup(response.text, "html.parser")

                # ì„¸ê³„ì¼ë³´ ê³„ì—´ í˜ì´ì§€ êµ¬ì¡°ì— ìµœì í™”ëœ ë³¸ë¬¸ ì¶”ì¶œ ì…€ë ‰í„°
                content_selectors = [
                    "article.viewBox2[itemprop='articleBody']",  # Segye ì›ë¬¸ ë³¸ë¬¸
                    'article[itemprop="articleBody"]',  # ì¼ë°˜ itemprop ë³¸ë¬¸
                    ".article-content",  # ì„¸ê³„ì¼ë³´ ê¸°ì‚¬ ë‚´ìš©
                    ".news-content",  # ë‰´ìŠ¤ ë‚´ìš©
                    ".view-content",  # ë·° ë‚´ìš©
                    ".article-body",  # ê¸°ì‚¬ ë³¸ë¬¸
                    ".content-body",  # ë³¸ë¬¸ ë‚´ìš©
                    ".news-text",  # ë‰´ìŠ¤ í…ìŠ¤íŠ¸
                    ".article_content",  # ê¸°ì‚¬ ì½˜í…ì¸ 
                    ".view_content",  # ë·° ì½˜í…ì¸ 
                    "#article-view-content-div",  # íŠ¹ì • ID
                    ".article_view",  # ê¸°ì‚¬ ë·°
                ]

                content = ""
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        # ìš”ì•½(em.precis) ì œê±°
                        for em in content_elem.find_all("em", class_="precis"):
                            em.decompose()
                        # ìš”ì•½ ì„¹ì…˜ ì œê±°
                        for sec in content_elem.find_all("section"):
                            sec.decompose()
                        content = content_elem.get_text().strip()
                        break

                # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„
                if not content:
                    # í—¤ë”, í‘¸í„°, ì‚¬ì´ë“œë°” ë“± ì œê±°
                    for elem in soup.find_all(["header", "footer", "nav", "aside", "script", "style"]):
                        elem.decompose()

                    main_content = soup.find("main") or soup.find("div", class_="content") or soup.find("body")
                    if main_content:
                        content = main_content.get_text().strip()

                # ê¸°ìëª… ì¶”ì¶œ: ì§€ì •ëœ ê²½ë¡œì—ì„œ ìš°ì„  ì¶”ì¶œ
                reporter_elem = soup.select_one(
                    "body > div:nth-of-type(1) > div > div:nth-of-type(2) > div:nth-of-type(2) > div:nth-of-type(1) > section > div:nth-of-type(1) > div:nth-of-type(1) > article > div"
                )
                if reporter_elem and reporter_elem.get_text().strip():
                    reporter = reporter_elem.get_text().strip()
                else:
                    reporter = self.extract_reporter_name(soup, content)

                # í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = self.extract_keywords(content)

                # í…ìŠ¤íŠ¸ ì •ë¦¬
                content = re.sub(r"\s+", " ", content).strip()

                return {
                    "content": content[:3000] + "..." if len(content) > 3000 else content,
                    "reporter": reporter,
                    "keywords": keywords,
                }

            except Exception as e:
                self.logger.warning(f"ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    self.random_delay(1, 3)
                else:
                    return {"content": "ì¶”ì¶œ ì‹¤íŒ¨", "reporter": "", "keywords": ""}

    def extract_reporter_name(self, soup, content):
        """ê¸°ìëª… ì¶”ì¶œ"""
        # ì„¸ê³„ì¼ë³´ ê³„ì—´ ê¸°ìëª… ì¶”ì¶œ íŒ¨í„´
        reporter_patterns = [
            r"([ê°€-í£]{2,4})\s*ê¸°ì",
            r"ê¸°ì\s*([ê°€-í£]{2,4})",
            r"ê¸°ì\s*:\s*([ê°€-í£]{2,4})",
            r"ì·¨ì¬\s*:\s*([ê°€-í£]{2,4})",
            r"ê¸€\s*:\s*([ê°€-í£]{2,4})",
            r"ì‘ì„±ì\s*:\s*([ê°€-í£]{2,4})",
            r"([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›",
            r"([ê°€-í£]{2,4})\s*ë…¼ì„¤ìœ„ì›",
        ]

        # HTMLì—ì„œ ê¸°ìëª… ì°¾ê¸°
        reporter_selectors = [".reporter-name", ".author-name", ".writer-name", ".byline", ".reporter", ".author"]

        for selector in reporter_selectors:
            reporter_elem = soup.select_one(selector)
            if reporter_elem:
                reporter_text = reporter_elem.get_text().strip()
                # ê¸°ìë¡œ ëë‚˜ëŠ” ì „ì²´ ë¬¸êµ¬ ì¶”ì¶œ
                for pattern in reporter_patterns:
                    match = re.search(pattern, reporter_text)
                    if match:
                        return match.group(0).strip()  # e.g., 'í™ì¤€í‘œ ê¸°ì'
                return reporter_text

        # í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìëª… íŒ¨í„´ ë§¤ì¹­
        for pattern in reporter_patterns:
            # ë³¸ë¬¸ì—ì„œ ê¸°ìë¡œ ëë‚˜ëŠ” ë¬¸ì¥ ì°¾ê¸°
            match = re.search(pattern, content)
            if match:
                return match.group(0).strip()

        return ""

    def extract_keywords(self, content):
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ì£¼ìš” í‚¤ì›Œë“œ íŒ¨í„´
        keyword_patterns = [
            r"(ì •ë¶€|êµ­íšŒ|ëŒ€í†µë ¹|ì´ë¦¬|ì¥ê´€)",
            r"(ê²½ì œ|ê¸ˆìœµ|ì¦ì‹œ|ë¶€ë™ì‚°|ì‚°ì—…)",
            r"(ë¬¸í™”|ì˜ˆìˆ |ì˜í™”|ìŒì•…|ë°©ì†¡)",
            r"(ìŠ¤í¬ì¸ |ì˜¬ë¦¼í”½|ì›”ë“œì»µ|ë¦¬ê·¸)",
            r"(êµìœ¡|ëŒ€í•™|í•™êµ|í•™ìƒ)",
            r"(ì˜ë£Œ|ê±´ê°•|ë³‘ì›|ì˜ì‚¬)",
            r"(í™˜ê²½|ê¸°í›„|ì—ë„ˆì§€|íƒ„ì†Œ)",
            r"(IT|ì¸ê³µì§€ëŠ¥|ë””ì§€í„¸|ë©”íƒ€ë²„ìŠ¤)",
        ]

        keywords = set()
        for pattern in keyword_patterns:
            matches = re.findall(pattern, content)
            keywords.update(matches)

        return ", ".join(list(keywords)[:10])  # ìµœëŒ€ 10ê°œ í‚¤ì›Œë“œ

    def get_media_name(self, category_name):
        """ì¹´í…Œê³ ë¦¬ëª…ìœ¼ë¡œ ë§¤ì²´ëª… ë°˜í™˜"""
        for media, categories in self.media_classification.items():
            if category_name in categories:
                return media
        return "ê¸°íƒ€"

    def get_category_group(self, category_name):
        """ì¹´í…Œê³ ë¦¬ì˜ ê·¸ë£¹ ë°˜í™˜"""
        for group, categories in self.category_groups.items():
            if category_name in categories:
                return group
        return "ê¸°íƒ€"

    def crawl_category_feed(self, category, rss_url, max_items=30):
        """ê°œë³„ ì¹´í…Œê³ ë¦¬ RSS í”¼ë“œ í¬ë¡¤ë§"""
        media_name = self.get_media_name(category)
        self.logger.info(f"{media_name} - {category} í¬ë¡¤ë§ ì‹œì‘")

        # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
        rss_content = self.fetch_rss_feed(rss_url)
        if not rss_content:
            return

        # RSS íŒŒì‹±
        rss_items = self.parse_rss_feed(rss_content)
        if not rss_items:
            self.logger.warning(f"RSS ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤: {category}")
            return

        # ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ë§Œ ì²˜ë¦¬
        items_to_process = rss_items[:max_items]

        for i, item in enumerate(items_to_process, 1):
            try:
                self.logger.info(f"{category} ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘: {i}/{len(items_to_process)} - {item['title'][:50]}...")

                # ê¸°ì‚¬ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
                if item["link"]:
                    article_detail = self.extract_article_content(item["link"])

                    article_data = {
                        "media": media_name,
                        "category": category,
                        "category_group": self.get_category_group(category),
                        "title": item["title"],
                        "link": item["link"],
                        "pub_date": item["pub_date"],
                        "author": item["author"],
                        "description": item["description"],
                        "content": article_detail["content"],
                        # ê¸°ìëª…ì„ 'ì„¸ê³„ì¼ë³´'ë¡œ í†µì¼
                        "reporter": "ì„¸ê³„ì¼ë³´",
                        "keywords": article_detail["keywords"],
                        "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    }

                    self.articles.append(article_data)

                # ë”œë ˆì´
                self.random_delay(1, 3)

            except Exception as e:
                self.logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue

        self.logger.info(f"{category} í¬ë¡¤ë§ ì™„ë£Œ: {len(items_to_process)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬")

    def crawl_all_feeds(self, max_items_per_category=30):
        """ëª¨ë“  RSS í”¼ë“œ í¬ë¡¤ë§"""
        total_categories = len(self.all_feeds)
        self.logger.info(f"ì „ì²´ {total_categories}ê°œ ì„¸ê³„ì¼ë³´ ê³„ì—´ RSS í”¼ë“œ í¬ë¡¤ë§ ì‹œì‘")

        for i, (category, rss_url) in enumerate(self.all_feeds.items(), 1):
            try:
                media_name = self.get_media_name(category)
                self.logger.info(f"[{i}/{total_categories}] {media_name} - {category} í”¼ë“œ í¬ë¡¤ë§ ì¤‘...")
                self.crawl_category_feed(category, rss_url, max_items_per_category)

                # ì¹´í…Œê³ ë¦¬ ê°„ ë”œë ˆì´
                if i < total_categories:
                    self.random_delay(2, 4)

            except Exception as e:
                self.logger.error(f"{category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                continue

        self.logger.info(f"ì „ì²´ ì„¸ê³„ì¼ë³´ ê³„ì—´ í¬ë¡¤ë§ ì™„ë£Œ: {len(self.articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        self.print_statistics()

    def crawl_specific_categories(self, category_names, max_items_per_category=30):
        """íŠ¹ì • ì¹´í…Œê³ ë¦¬ë“¤ë§Œ í¬ë¡¤ë§"""
        for category_name in category_names:
            if category_name in self.all_feeds:
                self.crawl_category_feed(category_name, self.all_feeds[category_name], max_items_per_category)
            else:
                self.logger.warning(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬: {category_name}")
                available_categories = list(self.all_feeds.keys())
                self.logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬: {available_categories}")

    def crawl_by_media(self, media_names, max_items_per_category=25):
        """ë§¤ì²´ë³„ í¬ë¡¤ë§"""
        target_categories = []

        for media_name in media_names:
            if media_name in self.media_classification:
                target_categories.extend(self.media_classification[media_name])
            else:
                self.logger.warning(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë§¤ì²´: {media_name}")

        if target_categories:
            self.logger.info(f"ë§¤ì²´ '{', '.join(media_names)}'ì— í•´ë‹¹í•˜ëŠ” ì¹´í…Œê³ ë¦¬: {target_categories}")
            self.crawl_specific_categories(target_categories, max_items_per_category)
        else:
            self.logger.warning(f"í•´ë‹¹ ë§¤ì²´ì— ë§ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {media_names}")

    def crawl_by_group(self, groups, max_items_per_category=25):
        """ê·¸ë£¹ë³„ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§"""
        target_categories = []

        for group in groups:
            if group in self.category_groups:
                target_categories.extend(self.category_groups[group])
            else:
                self.logger.warning(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê·¸ë£¹: {group}")

        if target_categories:
            self.logger.info(f"ê·¸ë£¹ '{', '.join(groups)}'ì— í•´ë‹¹í•˜ëŠ” ì¹´í…Œê³ ë¦¬: {target_categories}")
            self.crawl_specific_categories(target_categories, max_items_per_category)
        else:
            self.logger.warning(f"í•´ë‹¹ ê·¸ë£¹ì— ë§ëŠ” ì¹´í…Œê³ ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {groups}")

    def save_to_csv(self, filename=None):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"results/ì„¸ê³„ì¼ë³´_ì „ì²´_{timestamp}.csv"

        try:
            df = pd.DataFrame(self.articles)
            # í•„ìš”í•œ ì—´ë§Œ ì„ íƒí•˜ê³  ìˆœì„œ ì§€ì •
            df_out = df[["media", "title", "pub_date", "category", "reporter", "content"]].rename(
                columns={
                    "media": "ì–¸ë¡ ì‚¬",
                    "title": "ì œëª©",
                    "pub_date": "ë‚ ì§œ",
                    "category": "ì¹´í…Œê³ ë¦¬",
                    "reporter": "ê¸°ìëª…",
                    "content": "ë³¸ë¬¸",
                }
            )
            df_out.to_csv(filename, index=False, encoding="utf-8-sig")
            self.logger.info(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            self.logger.info(f"ì´ {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥")
        except Exception as e:
            self.logger.error(f"CSV ì €ì¥ ì˜¤ë¥˜: {e}")

    def save_by_media(self):
        """ë§¤ì²´ë³„ë¡œ ê°œë³„ CSV íŒŒì¼ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        df = pd.DataFrame(self.articles)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        for media in df["media"].unique():
            media_df = df[df["media"] == media]
            filename = f"results/{media}_{timestamp}.csv"
            media_df.to_csv(filename, index=False, encoding="utf-8-sig")
            self.logger.info(f"{media} ì €ì¥ ì™„ë£Œ: {filename} ({len(media_df)}ê°œ ê¸°ì‚¬)")

    def save_by_category(self):
        """ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê°œë³„ CSV íŒŒì¼ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        df = pd.DataFrame(self.articles)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        for category in df["category"].unique():
            category_df = df[df["category"] == category]
            media_name = category_df["media"].iloc[0]
            filename = f"results/{media_name}_{category}_{timestamp}.csv"
            category_df.to_csv(filename, index=False, encoding="utf-8-sig")
            self.logger.info(f"{category} ì €ì¥ ì™„ë£Œ: {filename} ({len(category_df)}ê°œ ê¸°ì‚¬)")

    def print_statistics(self):
        """í¬ë¡¤ë§ í†µê³„ ì¶œë ¥"""
        if not self.articles:
            return

        df = pd.DataFrame(self.articles)

        print("\n" + "=" * 60)
        print("ì„¸ê³„ì¼ë³´ ê³„ì—´ RSS í¬ë¡¤ë§ í†µê³„")
        print("=" * 60)

        # ë§¤ì²´ë³„ í†µê³„
        media_stats = df["media"].value_counts()
        print(f"\nğŸ“° ë§¤ì²´ë³„ ê¸°ì‚¬ ìˆ˜:")
        for media, count in media_stats.items():
            print(f"  â€¢ {media}: {count}ê°œ")

        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = df["category"].value_counts()
        print(f"\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜:")
        for category, count in category_stats.items():
            media_name = df[df["category"] == category]["media"].iloc[0]
            print(f"  â€¢ {category} ({media_name}): {count}ê°œ")

        # ê·¸ë£¹ë³„ í†µê³„
        group_stats = df["category_group"].value_counts()
        print(f"\nğŸ—‚ï¸ ê·¸ë£¹ë³„ ê¸°ì‚¬ ìˆ˜:")
        for group, count in group_stats.items():
            print(f"  â€¢ {group}: {count}ê°œ")

        # ê¸°ìë³„ í†µê³„ (ìƒìœ„ 10ëª…)
        reporter_stats = df[df["reporter"] != ""]["reporter"].value_counts().head(10)
        if not reporter_stats.empty:
            print(f"\nâœï¸ ì£¼ìš” ê¸°ìë³„ ê¸°ì‚¬ ìˆ˜:")
            for reporter, count in reporter_stats.items():
                print(f"  â€¢ {reporter} ê¸°ì: {count}ê°œ")

        # í‚¤ì›Œë“œ í†µê³„
        keyword_stats = df[df["keywords"] != ""]["keywords"].str.split(", ").explode().value_counts().head(10)
        if not keyword_stats.empty:
            print(f"\nğŸ” ì£¼ìš” í‚¤ì›Œë“œë³„ ì–¸ê¸‰ ìˆ˜:")
            for keyword, count in keyword_stats.items():
                print(f"  â€¢ {keyword}: {count}íšŒ")

        print(f"\nğŸ“ˆ ì „ì²´ ìš”ì•½:")
        print(f"  â€¢ ì´ ê¸°ì‚¬ ìˆ˜: {len(self.articles)}ê°œ")
        print(f"  â€¢ í¬ë¡¤ë§ ë§¤ì²´ ìˆ˜: {len(media_stats)}ê°œ")
        print(f"  â€¢ í¬ë¡¤ë§ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(category_stats)}ê°œ")
        print(f"  â€¢ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {len(df[df['content'] != 'ì¶”ì¶œ ì‹¤íŒ¨'])}ê°œ")
        print(f"  â€¢ ê¸°ìëª… ì¶”ì¶œ: {len(df[df['reporter'] != ''])}ê°œ")
        print(f"  â€¢ í‚¤ì›Œë“œ ì¶”ì¶œ: {len(df[df['keywords'] != ''])}ê°œ")
        print("=" * 60)

    def get_available_categories(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ ëª©ë¡ ë°˜í™˜"""
        return list(self.all_feeds.keys())

    def get_media_list(self):
        """ë§¤ì²´ ëª©ë¡ ë°˜í™˜"""
        return list(self.media_classification.keys())

    def get_groups(self):
        """ê·¸ë£¹ ëª©ë¡ ë°˜í™˜"""
        return list(self.category_groups.keys())


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ì„¸ê³„ì¼ë³´ ê³„ì—´ RSS í¬ë¡¤ëŸ¬")
    print("=" * 50)

    crawler = SegyeNewsRSSCrawler()

    print("ì „ì²´ ì„¸ê³„ì¼ë³´ ê³„ì—´ RSS í”¼ë“œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    crawler.crawl_all_feeds(max_items_per_category=20)

    # CSV ì €ì¥
    crawler.save_to_csv()

    print("\nì„¸ê³„ì¼ë³´ ê³„ì—´ í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
