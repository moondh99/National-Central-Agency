import requests
from bs4 import BeautifulSoup
import csv
import re
from datetime import datetime
import time
from urllib.parse import urljoin, urlparse

def extract_dailian_article_content(url):
    """ë°ì¼ë¦¬ì•ˆ ê¸°ì‚¬ URLì—ì„œ ì „ì²´ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë°ì¼ë¦¬ì•ˆ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
        content_selectors = [
            'div.article_txt',           # ì£¼ìš” ë³¸ë¬¸ ì˜ì—­
            'div.news_article_body',     # ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸
            'div.view_con',              # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ
            'div.article-body',          # ê¸°ì‚¬ ë³¸ë¬¸
            'div.news_view',             # ë‰´ìŠ¤ ë·°
            '.article_content',          # ê¸°ì‚¬ ì»¨í…ì¸ 
            'div.txt_area',              # í…ìŠ¤íŠ¸ ì˜ì—­
            '#article_content'           # ID ê¸°ë°˜ ë³¸ë¬¸
        ]
        
        full_content = ""
        
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                for element in elements:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ ì œê±°
                    for unwanted in element.find_all(['script', 'style', 'iframe', 'ins', 'div.ad', '.advertisement', '.related-articles', '.tags', '.share']):
                        unwanted.decompose()
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    text = element.get_text(separator='\n', strip=True)
                    if text and len(text) > len(full_content):
                        full_content = text
                        break
                
                if full_content:
                    break
        
        # ë³¸ë¬¸ì´ ì—¬ì „íˆ ì§§ë‹¤ë©´ p íƒœê·¸ë“¤ë¡œ ì¬ì‹œë„
        if len(full_content) < 100:
            paragraphs = soup.find_all('p')
            paragraph_texts = []
            for p in paragraphs:
                text = p.get_text(strip=True)
                if text and len(text) > 20 and 'Â©' not in text and 'Copyright' not in text:
                    paragraph_texts.append(text)
            
            if paragraph_texts:
                candidate_content = '\n'.join(paragraph_texts)
                if len(candidate_content) > len(full_content):
                    full_content = candidate_content
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        if full_content:
            # ì €ì‘ê¶Œ í‘œì‹œ ì œê±°
            full_content = re.sub(r'Â©.*?ë°ì¼ë¦¬ì•ˆ.*?ê¸ˆì§€.*?$', '', full_content, flags=re.MULTILINE)
            full_content = re.sub(r'Copyright.*?dailian.*?$', '', full_content, flags=re.MULTILINE | re.IGNORECASE)
            # ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
            full_content = re.sub(r'\n+', '\n', full_content)
            full_content = re.sub(r'\s+', ' ', full_content)
            full_content = full_content.strip()
        
        return full_content
        
    except Exception as e:
        print(f"ë³¸ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""

def extract_dailian_reporter_name(soup, article_text):
    """ë°ì¼ë¦¬ì•ˆ ê¸°ìëª…ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        # ë°ì¼ë¦¬ì•ˆì˜ ê¸°ìëª… ì¶”ì¶œ íŒ¨í„´
        reporter_patterns = [
            # HTMLì—ì„œ ê¸°ìëª… ì¶”ì¶œ
            r'<span[^>]*class[^>]*reporter[^>]*>([^<]+)</span>',
            r'<div[^>]*class[^>]*reporter[^>]*>([^<]+)</div>',
            r'<p[^>]*class[^>]*reporter[^>]*>([^<]+)</p>',
            
            # í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìëª… ì¶”ì¶œ (ë°ì¼ë¦¬ì•ˆ íŠ¹ì„±ì— ë§ê²Œ)
            r'([ê°€-í£]{2,4})\s*ê¸°ì(?:\s*=|\s*âˆ™|\s*Â·|\s*ì…ë ¥|\s*ìˆ˜ì •|\s*ì‘ì„±)',
            r'ê¸°ì\s*([ê°€-í£]{2,4})(?:\s*=|\s*âˆ™|\s*Â·)',
            r'([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›',
            r'([ê°€-í£]{2,4})\s*ë…¼ì„¤ìœ„ì›',
            r'([ê°€-í£]{2,4})\s*í¸ì§‘ìœ„ì›',
            r'/\s*([ê°€-í£]{2,4})\s*ê¸°ì',
            r'=\s*([ê°€-í£]{2,4})\s*ê¸°ì',
            r'âˆ™\s*([ê°€-í£]{2,4})\s*ê¸°ì',
            r'Â·\s*([ê°€-í£]{2,4})\s*ê¸°ì',
            r'ê¸°ì\s*:\s*([ê°€-í£]{2,4})',
            r'\[([ê°€-í£]{2,4})\s*ê¸°ì\]'
        ]
        
        # BeautifulSoup ê°ì²´ì—ì„œ ê¸°ìëª… ì°¾ê¸°
        if soup:
            # ê¸°ìëª…ì´ í¬í•¨ë  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ìš”ì†Œë“¤ ì°¾ê¸°
            reporter_elements = soup.find_all(['span', 'div', 'p'], string=re.compile(r'ê¸°ì|íŠ¹íŒŒì›|ë…¼ì„¤ìœ„ì›'))
            for element in reporter_elements:
                text = element.get_text(strip=True)
                if 'ê¸°ì' in text:
                    match = re.search(r'([ê°€-í£]{2,4})', text)
                    if match:
                        return match.group(1) + ' ê¸°ì'
        
        # ê¸°ì‚¬ í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìëª… ì°¾ê¸°
        full_text = str(soup) + '\n' + article_text if soup else article_text
        
        for pattern in reporter_patterns:
            matches = re.findall(pattern, full_text)
            if matches:
                reporter = matches[0].strip()
                if reporter and len(reporter) >= 2:
                    return reporter + (' ê¸°ì' if 'ê¸°ì' not in reporter else '')
        
        return "ê¸°ìëª… ì—†ìŒ"
        
    except Exception as e:
        print(f"ê¸°ìëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return "ê¸°ìëª… ì—†ìŒ"

def get_dailian_news_list(base_url="https://www.dailian.co.kr", max_pages=3):
    """ë°ì¼ë¦¬ì•ˆ ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        news_items = []
        
        # ì—¬ëŸ¬ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
        for page in range(1, max_pages + 1):
            try:
                # ë°ì¼ë¦¬ì•ˆ ë‰´ìŠ¤ ëª©ë¡ í˜ì´ì§€ (ìµœì‹ ìˆœ)
                list_url = f"{base_url}/news/list/?page={page}"
                
                print(f"ğŸ“„ {page}í˜ì´ì§€ ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
                
                response = requests.get(list_url, headers=headers, timeout=15)
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ë‰´ìŠ¤ ë§í¬ë“¤ ì°¾ê¸° (ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„)
                link_selectors = [
                    'a[href*="/news/view/"]',      # ë‰´ìŠ¤ ë·° ë§í¬
                    'h3 a, h2 a, .title a',       # ì œëª© ë§í¬
                    '.news_list a',                # ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ë§í¬
                    '.article_list a'              # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ë§í¬
                ]
                
                page_links = []
                for selector in link_selectors:
                    links = soup.select(selector)
                    if links:
                        page_links.extend(links)
                        break
                
                # ì¤‘ë³µ ì œê±° ë° ìœ íš¨í•œ ë§í¬ë§Œ ì„ ë³„
                seen_urls = set()
                for link in page_links:
                    href = link.get('href')
                    if href and '/news/view/' in href:
                        full_url = urljoin(base_url, href)
                        if full_url not in seen_urls:
                            seen_urls.add(full_url)
                            
                            # ì œëª© ì¶”ì¶œ
                            title = link.get_text(strip=True)
                            if not title:
                                title_elem = link.find_parent().find(['h1', 'h2', 'h3', 'h4'])
                                if title_elem:
                                    title = title_elem.get_text(strip=True)
                            
                            if title and len(title) > 5:  # ë„ˆë¬´ ì§§ì€ ì œëª© ì œì™¸
                                news_items.append({
                                    'url': full_url,
                                    'title': title[:100]  # ì œëª© ê¸¸ì´ ì œí•œ
                                })
                
                new_items = news_items[-len(page_links):] if page_links else []
                print(f"  â¤ {page}í˜ì´ì§€ì—ì„œ {len(new_items)} ê°œ ìƒˆë¡œìš´ ë‰´ìŠ¤ ë°œê²¬")
                
                time.sleep(1)  # í˜ì´ì§€ ìš”ì²­ ê°„ ë”œë ˆì´
                
            except Exception as e:
                print(f"  â¤ {page}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ì¤‘ë³µ ì œê±°
        unique_news = []
        seen_urls = set()
        for item in news_items:
            if item['url'] not in seen_urls:
                seen_urls.add(item['url'])
                unique_news.append(item)
        
        print(f"ğŸ“Š ì´ {len(unique_news)}ê°œì˜ ê³ ìœ í•œ ë‰´ìŠ¤ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        return unique_news
        
    except Exception as e:
        print(f"ë‰´ìŠ¤ ëª©ë¡ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        return []

def scrape_dailian_news(max_articles=50, max_pages=3):
    """ë°ì¼ë¦¬ì•ˆ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ì—¬ CSVë¡œ ì €ì¥í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    
    print("ğŸ—ï¸  ë°ì¼ë¦¬ì•ˆ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 60)
    
    try:
        # ë‰´ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        news_list = get_dailian_news_list(max_pages=max_pages)
        
        if not news_list:
            print("âŒ ë‰´ìŠ¤ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ ì œí•œ
        if len(news_list) > max_articles:
            news_list = news_list[:max_articles]
            print(f"âš ï¸  ìµœëŒ€ {max_articles}ê°œ ê¸°ì‚¬ë¡œ ì œí•œí•©ë‹ˆë‹¤.")
        
        news_data = []
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        print(f"\nğŸ“° {len(news_list)}ê°œ ê¸°ì‚¬ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
        
        for i, news_item in enumerate(news_list):
            try:
                url = news_item['url']
                base_title = news_item['title']
                
                print(f"[{i+1}/{len(news_list)}] ì²˜ë¦¬ ì¤‘: {base_title[:50]}...")
                
                # ê°œë³„ ê¸°ì‚¬ í˜ì´ì§€ í¬ë¡¤ë§
                response = requests.get(url, headers=headers, timeout=20)
                response.encoding = 'utf-8'
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ì œëª© ì¶”ì¶œ (ë” ì •í™•í•œ ì œëª©)
                title = base_title
                title_selectors = ['h1.title', 'h1', '.news_title', '.article_title', 'title']
                for selector in title_selectors:
                    title_elem = soup.select_one(selector)
                    if title_elem:
                        extracted_title = title_elem.get_text(strip=True)
                        if extracted_title and len(extracted_title) > len(title):
                            title = extracted_title
                        break
                
                # ë‚ ì§œ ì¶”ì¶œ
                date_text = "ë‚ ì§œ ì—†ìŒ"
                date_selectors = [
                    '.date', '.news_date', '.article_date', '.view_date',
                    '[class*="date"]', '[class*="time"]'
                ]
                
                for selector in date_selectors:
                    date_elem = soup.select_one(selector)
                    if date_elem:
                        date_text = date_elem.get_text(strip=True)
                        break
                
                # ë‚ ì§œ í˜•ì‹ ì •ë¦¬
                if date_text != "ë‚ ì§œ ì—†ìŒ":
                    # í•œêµ­ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
                    date_match = re.search(r'(\d{4})[-./ë…„]\s*(\d{1,2})[-./ì›”]\s*(\d{1,2})', date_text)
                    if date_match:
                        year, month, day = date_match.groups()
                        date_text = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                    
                    # ì‹œê°„ ì •ë³´ê°€ ìˆë‹¤ë©´ ì¶”ê°€
                    time_match = re.search(r'(\d{1,2}):(\d{2})', date_text)
                    if time_match:
                        hour, minute = time_match.groups()
                        date_text += f" {hour.zfill(2)}:{minute}"
                
                # ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ
                full_content = extract_dailian_article_content(url)
                
                # ê¸°ìëª… ì¶”ì¶œ
                reporter_name = extract_dailian_reporter_name(soup, full_content)
                
                # ë°ì´í„° ì €ì¥
                if full_content.strip():  # ë³¸ë¬¸ì´ ìˆëŠ” ê²½ìš°ë§Œ ì €ì¥
                    news_data.append({
                        'ì œëª©': title.strip(),
                        'ë‚ ì§œ': date_text,
                        'ê¸°ìëª…': reporter_name,
                        'ë³¸ë¬¸': full_content
                    })
                else:
                    print(f"  â¤ ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€
                time.sleep(1.5)
                
            except Exception as e:
                print(f"  â¤ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # CSV íŒŒì¼ë¡œ ì €ì¥
        if news_data:
            filename = f"results/ë°ì¼ë¦¬ì•ˆ_ë‰´ìŠ¤_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            
            with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['ì œëª©', 'ë‚ ì§œ', 'ê¸°ìëª…', 'ë³¸ë¬¸']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                
                writer.writeheader()
                writer.writerows(news_data)
            
            print(f"\nâœ… ì„±ê³µì ìœ¼ë¡œ {len(news_data)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“ íŒŒì¼ëª…: {filename}")
            
            # í†µê³„ ì •ë³´ ì¶œë ¥
            total_chars = sum(len(item['ë³¸ë¬¸']) for item in news_data)
            avg_chars = total_chars // len(news_data) if news_data else 0
            print(f"ğŸ“Š í‰ê·  ë³¸ë¬¸ ê¸¸ì´: {avg_chars:,}ì")
            print(f"ğŸ“Š ì´ ë³¸ë¬¸ ê¸¸ì´: {total_chars:,}ì")
            
            return filename
        else:
            print("âŒ ì¶”ì¶œëœ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def scrape_dailian_by_category(category="all", max_articles=30):
    """ì¹´í…Œê³ ë¦¬ë³„ ë°ì¼ë¦¬ì•ˆ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    
    category_urls = {
        "all": "https://www.dailian.co.kr/news/list/",
        "politics": "https://www.dailian.co.kr/news/list/?sc=politics",
        "economy": "https://www.dailian.co.kr/news/list/?sc=economy", 
        "society": "https://www.dailian.co.kr/news/list/?sc=society",
        "international": "https://www.dailian.co.kr/news/list/?sc=international",
        "culture": "https://www.dailian.co.kr/news/list/?sc=culture",
        "sports": "https://www.dailian.co.kr/news/list/?sc=sports",
        "it": "https://www.dailian.co.kr/news/list/?sc=it"
    }
    
    if category not in category_urls:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤.")
        print(f"âœ… ì§€ì› ì¹´í…Œê³ ë¦¬: {', '.join(category_urls.keys())}")
        return None
    
    print(f"ğŸ“° {category} ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë§ì¶¤ í¬ë¡¤ë§ ë¡œì§ì€ ìœ„ì˜ ë©”ì¸ í•¨ìˆ˜ì™€ ë™ì¼í•˜ì§€ë§Œ
    # URLë§Œ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë³€ê²½í•˜ì—¬ ì‚¬ìš©
    return scrape_dailian_news(max_articles=max_articles, max_pages=2)

if __name__ == "__main__":
    print("ğŸ—ï¸  ë°ì¼ë¦¬ì•ˆ ë‰´ìŠ¤ í¬ë¡¤ë§")
    print("=" * 60)
    
    try:
        print("\nğŸ“‹ ì˜µì…˜ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ì „ì²´ ìµœì‹  ë‰´ìŠ¤ (50ê°œ)")
        print("2. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10ê°œ)")
        print("3. ëŒ€ëŸ‰ ìˆ˜ì§‘ (100ê°œ)")
        
        choice = input("\nì„ íƒ (1-3): ").strip()
        
        if choice == "1":
            print("\nğŸš€ ì „ì²´ ìµœì‹  ë‰´ìŠ¤ 50ê°œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
            result = scrape_dailian_news(max_articles=50, max_pages=3)
            
        elif choice == "2":
            print("\nğŸš€ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (10ê°œ ê¸°ì‚¬)ë¡œ ì‹œì‘í•©ë‹ˆë‹¤...")
            result = scrape_dailian_news(max_articles=10, max_pages=1)
            
        elif choice == "3":
            print("\nğŸš€ ëŒ€ëŸ‰ ìˆ˜ì§‘ ëª¨ë“œ (100ê°œ ê¸°ì‚¬)ë¡œ ì‹œì‘í•©ë‹ˆë‹¤...")
            result = scrape_dailian_news(max_articles=100, max_pages=5)
            
        else:
            print("âš ï¸  ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            result = scrape_dailian_news(max_articles=30, max_pages=2)
        
        if result:
            print(f"\nğŸ‰ ì™„ë£Œ! íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {result}")
        else:
            print("\nâŒ ì‘ì—…ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
