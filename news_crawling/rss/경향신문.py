import requests
import pandas as pd
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import os
from datetime import datetime
import time
import re

# RSS URL ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ (9ê°œ ì¹´í…Œê³ ë¦¬)
rss_urls = {
    "ì „ì²´ë‰´ìŠ¤": "https://www.khan.co.kr/rss/rssdata/total_news.xml",
    "ì •ì¹˜": "https://www.khan.co.kr/rss/rssdata/politic_news.xml",
    "ê²½ì œ": "https://www.khan.co.kr/rss/rssdata/economy_news.xml",
    "ì‚¬íšŒ": "https://www.khan.co.kr/rss/rssdata/society_news.xml",
    "ë¬¸í™”": "https://www.khan.co.kr/rss/rssdata/culture_news.xml",
    "ì§€ì—­": "https://www.khan.co.kr/rss/rssdata/local_news.xml",
    "ì˜¤í”¼ë‹ˆì–¸": "https://www.khan.co.kr/rss/rssdata/opinion_news.xml",
    "êµ­ì œ": "https://www.khan.co.kr/rss/rssdata/kh_world.xml",
    "ì‚¬ëŒ": "https://www.khan.co.kr/rss/rssdata/people_news.xml",
}


def parse_rss_feed(rss_url, max_items=20):
    """RSS í”¼ë“œë¥¼ íŒŒì‹±í•˜ì—¬ ë‰´ìŠ¤ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        response = requests.get(rss_url, timeout=10)
        response.raise_for_status()

        # XML íŒŒì‹±
        root = ET.fromstring(response.content)

        items = []
        # item íƒœê·¸ë“¤ì„ ì°¾ì•„ì„œ ì²˜ë¦¬
        for item in root.findall(".//item")[:max_items]:
            # ì œëª© ì¶”ì¶œ
            title_elem = item.find("title")
            title = title_elem.text if title_elem is not None else ""
            if title.startswith("<![CDATA[") and title.endswith("]]>"):
                title = title[9:-3].strip()

            # ë§í¬ ì¶”ì¶œ
            link_elem = item.find("link")
            link = link_elem.text if link_elem is not None else ""

            # ë‚ ì§œ ì¶”ì¶œ (dc:date ë˜ëŠ” pubDate)
            date_elem = item.find("{http://purl.org/dc/elements/1.1/}date")
            if date_elem is None:
                date_elem = item.find("pubDate")
            date = date_elem.text if date_elem is not None else ""

            # ê¸°ìëª… ì¶”ì¶œ (author íƒœê·¸ì—ì„œ)
            author_elem = item.find("author")
            author = author_elem.text if author_elem is not None else ""
            if author.startswith("<![CDATA[") and author.endswith("]]>"):
                author = author[9:-3].strip()

            items.append({"title": title, "link": link, "date": date, "author": author})

        return items

    except Exception as e:
        print(f"RSS í”¼ë“œ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return []


def extract_article_content(url):
    """ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, "html.parser")

        # í•œêµ­ì¼ë³´ì— ìµœì í™”ëœ ì„ íƒìë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
        selectors = [
            '[class*="content"]',  # ë°œê²¬ëœ ìµœì  ì„ íƒì
            ".art_txt",
            "main section:first-child article section div",
            ".article_txt",
            ".news_txt",
            ".content_area",
            "article p",
        ]

        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                # ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ ìš”ì†Œ ì„ íƒ
                best_content = ""
                for elem in elements:
                    text = elem.get_text(strip=True, separator=" ")
                    if len(text) > len(best_content) and len(text) > 100:  # ì¶©ë¶„í•œ ê¸¸ì´
                        best_content = text

                if best_content:
                    # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
                    content = re.sub(r"\s+", " ", best_content)
                    return content[:2000]  # ìµœëŒ€ 2000ìë¡œ ì œí•œ

        return "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"

    except Exception as e:
        print(f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜ ({url}): {e}")
        return f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}"


def collect_news_by_category(category, rss_url, max_items=20):
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜"""
    print(f"\n[{category}] ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")

    # RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    rss_items = parse_rss_feed(rss_url, max_items)

    if not rss_items:
        print(f"[{category}] RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return []

    collected_news = []

    for i, item in enumerate(rss_items, 1):
        print(f"[{category}] {i}/{len(rss_items)} - {item['title'][:50]}...")

        # ë³¸ë¬¸ ì¶”ì¶œ
        content = extract_article_content(item["link"])

        # ë°ì´í„° êµ¬ì„±
        news_data = {
            "ì–¸ë¡ ì‚¬": "í•œêµ­ì¼ë³´",
            "ì œëª©": item["title"],
            "ë‚ ì§œ": item["date"],
            "ì¹´í…Œê³ ë¦¬": category,
            "ê¸°ìëª…": item["author"],
            "ë³¸ë¬¸": content,
        }

        collected_news.append(news_data)

        # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
        time.sleep(0.5)

    print(f"[{category}] ì´ {len(collected_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
    return collected_news


def collect_all_news():
    """ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    print("=== í•œêµ­ì¼ë³´ ë‰´ìŠ¤ ìë™ ìˆ˜ì§‘ ì‹œì‘ ===")
    print(f"ìˆ˜ì§‘ ëŒ€ìƒ: {len(rss_urls)}ê°œ ì¹´í…Œê³ ë¦¬, ê° ì¹´í…Œê³ ë¦¬ë‹¹ 20ê°œì”©")
    print(f"ì˜ˆìƒ ì´ ë‰´ìŠ¤ ê°œìˆ˜: {len(rss_urls) * 20}ê°œ")

    all_news = []

    for category, url in rss_urls.items():
        try:
            category_news = collect_news_by_category(category, url, 20)
            all_news.extend(category_news)
        except Exception as e:
            print(f"[{category}] ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

    print(f"\n=== ìˆ˜ì§‘ ì™„ë£Œ ===")
    print(f"ì´ ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {len(all_news)}ê°œ")

    return all_news


def save_to_csv(news_data):
    """ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    if not news_data:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # DataFrame ìƒì„±
    df = pd.DataFrame(news_data)

    # ì—´ ìˆœì„œ ì§€ì •: ì–¸ë¡ ì‚¬, ì œëª©, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, ê¸°ìëª…, ë³¸ë¬¸
    column_order = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
    df = df[column_order]

    # results ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("results", exist_ok=True)

    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"results/í•œêµ­ì¼ë³´_ì „ì²´_{timestamp}.csv"

    # CSV ì €ì¥ (UTF-8 ì¸ì½”ë”©)
    df.to_csv(filename, index=False, encoding="utf-8-sig")

    print(f"\nCSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
    print(f"ì €ì¥ëœ ë°ì´í„°: {len(df)}í–‰ x {len(df.columns)}ì—´")
    print(f"ì»¬ëŸ¼: {', '.join(df.columns)}")

    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    category_stats = df["ì¹´í…Œê³ ë¦¬"].value_counts()
    print(f"\nì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ê°œìˆ˜:")
    for category, count in category_stats.items():
        print(f"  {category}: {count}ê°œ")

    return filename


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    start_time = time.time()

    print("ğŸš€ í•œêµ­ì¼ë³´ ë‰´ìŠ¤ ìë™ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("=" * 60)

    # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘
    all_collected_news = collect_all_news()

    # CSV íŒŒì¼ë¡œ ì €ì¥
    if all_collected_news:
        saved_filename = save_to_csv(all_collected_news)

        end_time = time.time()
        elapsed_time = end_time - start_time

        print(f"\nâœ… ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
        print(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ ({elapsed_time/60:.1f}ë¶„)")
        print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼: {saved_filename}")
        print(f"ğŸ“Š ìˆ˜ì§‘ëœ ì´ ë‰´ìŠ¤: {len(all_collected_news)}ê°œ")
    else:
        print("âŒ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
