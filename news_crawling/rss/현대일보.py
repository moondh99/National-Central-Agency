import feedparser
import requests
from bs4 import BeautifulSoup
import csv
from datetime import datetime
import time
import random
import re


class HyundaiIlboRSSCollector:
    def __init__(self):
        self.base_url = "http://www.hyundaiilbo.com"
        self.rss_urls = {
            "ì „ì²´ê¸°ì‚¬": "http://www.hyundaiilbo.com/rss/allArticle.xml",
            "ì¸ê¸°ê¸°ì‚¬": "http://www.hyundaiilbo.com/rss/clickTop.xml",
            "ë‰´ìŠ¤": "http://www.hyundaiilbo.com/rss/S1N1.xml",
            "ì¸ì²œÂ·ê²½ê¸°": "http://www.hyundaiilbo.com/rss/S1N2.xml",
            "ì˜¤í”¼ë‹ˆì–¸": "http://www.hyundaiilbo.com/rss/S1N3.xml",
            "ì‚¬ëŒë“¤": "http://www.hyundaiilbo.com/rss/S1N4.xml",
            "í¬í† ë‰´ìŠ¤": "http://www.hyundaiilbo.com/rss/S1N5.xml",
        }

        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
        ]

        self.session = requests.Session()

    def get_random_headers(self):
        return {
            "User-Agent": random.choice(self.user_agents),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }

    def extract_reporter_name(self, content):
        """ê¸°ìëª… ì¶”ì¶œ"""
        if not content:
            return ""

        # í˜„ëŒ€ì¼ë³´ ê¸°ìëª… íŒ¨í„´ë“¤
        patterns = [
            r"([ê°€-í£]{2,4})\s*ê¸°ì",
            r"ê¸°ì\s*([ê°€-í£]{2,4})",
            r"([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›",
            r"([ê°€-í£]{2,4})\s*ë…¼ì„¤ìœ„ì›",
            r"([ê°€-í£]{2,4})\s*í¸ì§‘ìœ„ì›",
            r"â– \s*([ê°€-í£]{2,4})",
            r"â–²\s*([ê°€-í£]{2,4})",
            r"â–³\s*([ê°€-í£]{2,4})",
            r"([ê°€-í£]{2,4})\s*@",
            r"([ê°€-í£]{2,4})\s*\w+@\w+\.\w+",
        ]

        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                name = match.group(1).strip()
                if len(name) >= 2 and name not in ["ê¸°ì", "íŠ¹íŒŒì›", "ìœ„ì›", "ë…¼ì„¤", "í¸ì§‘"]:
                    return name

        return ""

    def clean_content(self, content):
        """ë³¸ë¬¸ ë‚´ìš© ì •ì œ"""
        if not content:
            return ""

        # HTML íƒœê·¸ ì œê±°
        soup = BeautifulSoup(content, "html.parser")
        text = soup.get_text()

        # ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°
        remove_patterns = [
            r"ì €ì‘ê¶Œì\s*Â©\s*í˜„ëŒ€ì¼ë³´.*?ê¸ˆì§€",
            r"ë¬´ë‹¨ì „ì¬.*?ì¬ë°°í¬.*?ê¸ˆì§€",
            r"Copyright.*?All rights reserved",
            r"ê³„ì •ì„ ì„ íƒí•˜ì‹œë©´.*?ëŒ“ê¸€ì„ ë‚¨ê¸°ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤\.",
            r"ë¡œê·¸ì¸Â·ê³„ì •ì¸ì¦ì„ í†µí•´.*?ëŒ“ê¸€ì„",
            r"\[í˜„ëŒ€ì¼ë³´\]",
            r"í˜„ëŒ€ì¼ë³´\s*=",
            r"â–².*?=",
            r"â– .*?=",
            r"â–³.*?=",
            r"\s+",
            r"^\s*\n",
            r"\n\s*$",
        ]

        for pattern in remove_patterns:
            text = re.sub(pattern, " ", text, flags=re.MULTILINE | re.DOTALL)

        # ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r"\s+", " ", text)
        text = text.strip()

        return text

    def get_article_content(self, article_url):
        """ê°œë³„ ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            headers = self.get_random_headers()
            response = self.session.get(article_url, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (í˜„ëŒ€ì¼ë³´ êµ¬ì¡°ì— ë§ì¶˜ ì„ íƒì)
            content_selectors = [
                "div.news-content",
                "div.article-content",
                "div.view-content",
                "div#article-view-content-div",
                "div.user-content",
                "div.article_txt",
                "div.content",
            ]

            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem.get_text()
                    break

            # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ì¶”ì¶œ ì‹œë„
            if not content:
                # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ë„¤ë¹„ê²Œì´ì…˜ ë“± ì œê±°
                for element in soup(["script", "style", "nav", "header", "footer", "aside"]):
                    element.decompose()

                content = soup.get_text()

            return self.clean_content(content)

        except Exception as e:
            print(f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜ ({article_url}): {e}")
            return ""

    def collect_rss_data(self, category=None, max_articles=50):
        """RSS ë°ì´í„° ìˆ˜ì§‘"""
        collected_data = []

        # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ ìˆ˜ì§‘í•˜ê±°ë‚˜ ì „ì²´ ìˆ˜ì§‘
        urls_to_process = {}
        if category and category in self.rss_urls:
            urls_to_process[category] = self.rss_urls[category]
        else:
            urls_to_process = self.rss_urls

        for category_name, rss_url in urls_to_process.items():
            print(f"\n=== {category_name} RSS ìˆ˜ì§‘ ì‹œì‘ ===")

            try:
                # RSS í”¼ë“œ íŒŒì‹±
                feed = feedparser.parse(rss_url)

                if not feed.entries:
                    print(f"âŒ {category_name}: RSS í”¼ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    continue

                print(f"ğŸ“° {category_name}: {len(feed.entries)}ê°œ ê¸°ì‚¬ ë°œê²¬")

                # ê° ê¸°ì‚¬ ì²˜ë¦¬
                for i, entry in enumerate(feed.entries[:max_articles]):
                    try:
                        print(f"ì²˜ë¦¬ ì¤‘... {i+1}/{min(len(feed.entries), max_articles)}: {entry.title[:50]}...")

                        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                        title = entry.title if hasattr(entry, "title") else ""
                        link = entry.link if hasattr(entry, "link") else ""

                        # ë°œí–‰ì¼ ì²˜ë¦¬
                        pub_date = ""
                        if hasattr(entry, "published"):
                            try:
                                pub_date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S")
                            except:
                                pub_date = entry.published

                        # ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
                        content = ""
                        reporter = ""
                        if link:
                            content = self.get_article_content(link)
                            reporter = self.extract_reporter_name(content)

                        # ìš”ì•½ ì •ë³´ (RSSì—ì„œ ì œê³µë˜ëŠ” ê²½ìš°)
                        summary = ""
                        if hasattr(entry, "summary"):
                            summary = BeautifulSoup(entry.summary, "html.parser").get_text()

                        # ë°ì´í„° ì €ì¥
                        article_data = {
                            "category": category_name,
                            "title": title,
                            "link": link,
                            "pub_date": pub_date,
                            "reporter": reporter,
                            "summary": summary[:200] + "..." if len(summary) > 200 else summary,
                            "content": content[:1000] + "..." if len(content) > 1000 else content,
                            "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        }

                        collected_data.append(article_data)

                        # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                        time.sleep(random.uniform(0.5, 1.5))

                    except Exception as e:
                        print(f"âŒ ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue

                print(
                    f"âœ… {category_name}: {len([d for d in collected_data if d['category'] == category_name])}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ"
                )

            except Exception as e:
                print(f"âŒ {category_name} RSS ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue

        return collected_data

    def save_to_csv(self, data, filename=None):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not data:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"results/hyundaiilbo_news_{timestamp}.csv"

        try:
            with open(filename, "w", newline="", encoding="utf-8-sig") as csvfile:
                fieldnames = ["category", "title", "link", "pub_date", "reporter", "summary", "content", "collected_at"]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

                writer.writeheader()
                for row in data:
                    writer.writerow(row)

            print(f"\nâœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")
            print(f"ğŸ“Š ì´ {len(data)}ê°œ ê¸°ì‚¬ ì €ì¥")

            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            category_stats = {}
            for item in data:
                cat = item["category"]
                category_stats[cat] = category_stats.get(cat, 0) + 1

            print("\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ í˜„í™©:")
            for cat, count in category_stats.items():
                print(f"  - {cat}: {count}ê°œ")

        except Exception as e:
            print(f"âŒ CSV ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    print("ğŸ›ï¸ í˜„ëŒ€ì¼ë³´ RSS ìˆ˜ì§‘ê¸° ì‹œì‘")
    print("=" * 50)

    collector = HyundaiIlboRSSCollector()

    # ì‚¬ìš© ì˜ˆì‹œ
    print("ğŸ“‹ ìˆ˜ì§‘ ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬:")
    for i, category in enumerate(collector.rss_urls.keys(), 1):
        print(f"  {i}. {category}")

    print("\nğŸš€ ì „ì²´ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # ì „ì²´ ì¹´í…Œê³ ë¦¬ì—ì„œ ê°ê° 30ê°œì”© ìˆ˜ì§‘
    collected_data = collector.collect_rss_data(max_articles=30)

    # CSV ì €ì¥
    if collected_data:
        collector.save_to_csv(collected_data)
        print(f"\nğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(collected_data)}ê°œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
