import feedparser
import requests
from bs4 import BeautifulSoup
import csv
from datetime import datetime
import time
import random
import re
import os

NEWS_OUTLET = "í˜„ëŒ€ì¼ë³´"


class HyundaiIlboRSSCollector:
    def __init__(self):
        self.base_url = "http://www.hyundaiilbo.com"
        self.rss_urls = {
            "ì „ì²´ê¸°ì‚¬": "http://www.hyundaiilbo.com/rss/allArticle.xml",
            "ë‰´ìŠ¤": "http://www.hyundaiilbo.com/rss/S1N1.xml",
            "ì¸ì²œÂ·ê²½ê¸°": "http://www.hyundaiilbo.com/rss/S1N2.xml",
            "ì˜¤í”¼ë‹ˆì–¸": "http://www.hyundaiilbo.com/rss/S1N3.xml",
        }
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
        ]

        self.session = requests.Session()

    def get_random_headers(self):
        return {
            "User-Agent": random.choice(self.user_agents),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }

    def extract_reporter_name(self, content):
        """ê¸°ìëª… ì¶”ì¶œ"""
        if not content:
            return ""

        # í˜„ëŒ€ì¼ë³´ ê¸°ìëª… íŒ¨í„´ë“¤
        patterns = [
            r"([ê°€-í£]{2,4})\s*ê¸°ì",
            r"ê¸°ì\s*([ê°€-í£]{2,4})",
            r"([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›",
            r"([ê°€-í£]{2,4})\s*ë…¼ì„¤ìœ„ì›",
            r"([ê°€-í£]{2,4})\s*í¸ì§‘ìœ„ì›",
            r"â– \s*([ê°€-í£]{2,4})",
            r"â–²\s*([ê°€-í£]{2,4})",
            r"â–³\s*([ê°€-í£]{2,4})",
            r"([ê°€-í£]{2,4})\s*@",
            r"([ê°€-í£]{2,4})\s*\w+@\w+\.\w+",
        ]

        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                name = match.group(1).strip()
                if len(name) >= 2 and name not in ["ê¸°ì", "íŠ¹íŒŒì›", "ìœ„ì›", "ë…¼ì„¤", "í¸ì§‘"]:
                    return name

        return ""

    def clean_content(self, content):
        """ë³¸ë¬¸ ë‚´ìš© ì •ì œ"""
        if not content:
            return ""

        # HTML íƒœê·¸ ì œê±°
        soup = BeautifulSoup(content, "html.parser")
        text = soup.get_text()

        # ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°
        remove_patterns = [
            r"ì €ì‘ê¶Œì\s*Â©\s*í˜„ëŒ€ì¼ë³´.*?ê¸ˆì§€",
            r"ë¬´ë‹¨ì „ì¬.*?ì¬ë°°í¬.*?ê¸ˆì§€",
            r"Copyright.*?All rights reserved",
            r"ê³„ì •ì„ ì„ íƒí•˜ì‹œë©´.*?ëŒ“ê¸€ì„ ë‚¨ê¸°ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤\.",
            r"ë¡œê·¸ì¸Â·ê³„ì •ì¸ì¦ì„ í†µí•´.*?ëŒ“ê¸€ì„",
            r"\[í˜„ëŒ€ì¼ë³´\]",
            r"í˜„ëŒ€ì¼ë³´\s*=",
            r"â–².*?=",
            r"â– .*?=",
            r"â–³.*?=",
            r"\s+",
            r"^\s*\n",
            r"\n\s*$",
        ]

        for pattern in remove_patterns:
            text = re.sub(pattern, " ", text, flags=re.MULTILINE | re.DOTALL)

        # ì—°ì†ëœ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆ ì •ë¦¬
        text = re.sub(r"\s+", " ", text)
        text = text.strip()

        return text

    def get_article_content(self, article_url):
        """ê°œë³„ ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°"""
        try:
            headers = self.get_random_headers()
            response = self.session.get(article_url, headers=headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            # 1ìˆœìœ„: ì›ë¬¸ ë³¸ë¬¸ ì»¨í…Œì´ë„ˆì—ì„œ ì¶”ì¶œ
            content = ""
            container = soup.select_one('div#article-view-content-div[itemprop="articleBody"]')
            if not container:
                container = soup.select_one("div#article-view-content-div")

            if container:
                # ë¶ˆí•„ìš” ìš”ì†Œ ì œê±°
                for el in container.find_all(
                    [
                        "script",
                        "style",
                        "noscript",
                        "iframe",
                        "aside",
                        "nav",
                        "header",
                        "footer",
                        "figure",
                        "table",
                        "img",
                    ]
                ):
                    el.decompose()
                # í¸ì§‘/ì €ì‘ê¶Œ/ìº¡ì…˜ ì˜ì—­ ì œê±°
                for cls in ["view-copyright", "view-editors", "article-head-sub", "caption"]:
                    for el in container.select(f".{cls}"):
                        el.decompose()

                # ë¬¸ë‹¨ ê¸°ë°˜ ìˆ˜ì§‘
                parts = []
                for p in container.find_all("p"):
                    text = p.get_text(" ", strip=True)
                    if not text:
                        continue
                    # ë¶ˆí•„ìš” ë¬¸êµ¬ í•„í„°
                    if any(key in text for key in ["ì €ì‘ê¶Œì", "ë¬´ë‹¨ì „ì¬", "ì¬ë°°í¬", "ë‹¤ë¥¸ê¸°ì‚¬ ë³´ê¸°"]):
                        continue
                    parts.append(text)
                if parts:
                    content = " ".join(parts)
                else:
                    content = container.get_text(" ", strip=True)
            else:
                # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (í˜„ëŒ€ì¼ë³´ êµ¬ì¡°ì— ë§ì¶˜ í´ë°± ì„ íƒì)
                content_selectors = [
                    "div.news-content",
                    "div.article-content",
                    "div.view-content",
                    "div.user-content",
                    "div.article_txt",
                    "div.content",
                ]

                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text(" ", strip=True)
                        if content:
                            break

            # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ì¶”ì¶œ ì‹œë„
            if not content:
                # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼, ë„¤ë¹„ê²Œì´ì…˜ ë“± ì œê±°
                for element in soup(["script", "style", "nav", "header", "footer", "aside"]):
                    element.decompose()

                content = soup.get_text(" ", strip=True)

            return self.clean_content(content)

        except Exception as e:
            print(f"ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜ ({article_url}): {e}")
            return ""

    def collect_rss_data(self, category=None, max_articles=50):
        """RSS ë°ì´í„° ìˆ˜ì§‘"""
        collected_data = []

        # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ ìˆ˜ì§‘í•˜ê±°ë‚˜ ì „ì²´ ìˆ˜ì§‘
        urls_to_process = {}
        if category and category in self.rss_urls:
            urls_to_process[category] = self.rss_urls[category]
        else:
            urls_to_process = self.rss_urls

        for category_name, rss_url in urls_to_process.items():
            print(f"\n=== {category_name} RSS ìˆ˜ì§‘ ì‹œì‘ ===")

            try:
                # RSS í”¼ë“œ íŒŒì‹±
                feed = feedparser.parse(rss_url)

                if not feed.entries:
                    print(f"âŒ {category_name}: RSS í”¼ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    continue

                print(f"ğŸ“° {category_name}: {len(feed.entries)}ê°œ ê¸°ì‚¬ ë°œê²¬")

                # ê° ê¸°ì‚¬ ì²˜ë¦¬
                for i, entry in enumerate(feed.entries[:max_articles]):
                    try:
                        print(f"ì²˜ë¦¬ ì¤‘... {i+1}/{min(len(feed.entries), max_articles)}: {entry.title[:50]}...")

                        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                        title = entry.title if hasattr(entry, "title") else ""
                        link = entry.link if hasattr(entry, "link") else ""

                        # ë°œí–‰ì¼ ì²˜ë¦¬
                        pub_date = ""
                        if hasattr(entry, "published"):
                            try:
                                pub_date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S")
                            except:
                                pub_date = entry.published

                        # ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
                        content = ""
                        reporter = ""
                        if link:
                            content = self.get_article_content(link)
                            reporter = self.extract_reporter_name(content)

                        # ìš”ì•½ ì •ë³´ (RSSì—ì„œ ì œê³µë˜ëŠ” ê²½ìš°)
                        summary = ""
                        if hasattr(entry, "summary"):
                            summary = BeautifulSoup(entry.summary, "html.parser").get_text()

                        # ë°ì´í„° ì €ì¥
                        article_data = {
                            "category": category_name,
                            "title": title,
                            "link": link,
                            "pub_date": pub_date,
                            "reporter": reporter,
                            "summary": summary[:200] + "..." if len(summary) > 200 else summary,
                            "content": content[:1000] + "..." if len(content) > 1000 else content,
                            "collected_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        }

                        collected_data.append(article_data)

                        # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                        time.sleep(random.uniform(0.5, 1.5))

                    except Exception as e:
                        print(f"âŒ ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue

                print(
                    f"âœ… {category_name}: {len([d for d in collected_data if d['category'] == category_name])}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ"
                )

            except Exception as e:
                print(f"âŒ {category_name} RSS ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue

        return collected_data

    def append_rss_category(self, rss_url: str, category_name: str, writer: csv.DictWriter, max_articles: int = 20):
        """RSSë¥¼ íŒŒì‹±í•´ ì§€ì • writerì— (ì–¸ë¡ ì‚¬, ì œëª©, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, ê¸°ìëª…, ë³¸ë¬¸) í–‰ ì¶”ê°€"""
        print(f"\n=== {category_name} RSS ìˆ˜ì§‘ ì‹œì‘ ===")

        # RSS í”¼ë“œ íŒŒì‹±
        feed = feedparser.parse(rss_url)
        if not feed.entries:
            print(f"âŒ {category_name}: RSS í”¼ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return 0, 0

        total = min(len(feed.entries), max_articles)
        success = 0
        print(f"ğŸ“° {category_name}: {len(feed.entries)}ê°œ ê¸°ì‚¬ ë°œê²¬ (ìµœëŒ€ {total}ê±´ ì²˜ë¦¬)")

        for i, entry in enumerate(feed.entries[:max_articles]):
            try:
                print(f"ì²˜ë¦¬ ì¤‘... {i+1}/{total}: {getattr(entry, 'title', '')[:50]}...")

                # ì œëª©
                title = getattr(entry, "title", "")
                title = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", title)

                # ë§í¬
                link = getattr(entry, "link", "")

                # ë‚ ì§œ
                if hasattr(entry, "published_parsed") and entry.published_parsed:
                    date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S")
                else:
                    date = getattr(entry, "published", "") or datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                # ì¹´í…Œê³ ë¦¬
                category = ""
                if hasattr(entry, "category") and entry.category:
                    category = entry.category.strip()
                elif hasattr(entry, "tags") and entry.tags:
                    try:
                        category = entry.tags[0].get("term") or entry.tags[0].term
                    except Exception:
                        category = ""
                if not category:
                    category = category_name

                # ê¸°ìëª… (RSS author)
                reporter = getattr(entry, "author", "")
                if reporter:
                    reporter = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", str(reporter)).strip()
                    reporter = re.sub(r"\s*ê¸°ì\s*$", "", reporter).strip()

                # ë³¸ë¬¸ (ì›ë¬¸ í˜ì´ì§€ì—ì„œ ì¶”ì¶œ)
                content = self.get_article_content(link) if link else ""
                if len(content.strip()) < 20:
                    print(f"    âš  ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì•„ ê±´ë„ˆëœ€ (ê¸¸ì´: {len(content)})")
                    continue

                # ê¸°ë¡ (ì—´ ìˆœì„œ: ì–¸ë¡ ì‚¬, ì œëª©, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, ê¸°ìëª…, ë³¸ë¬¸)
                writer.writerow(
                    {
                        "ì–¸ë¡ ì‚¬": NEWS_OUTLET,
                        "ì œëª©": title,
                        "ë‚ ì§œ": date,
                        "ì¹´í…Œê³ ë¦¬": category if category else "ë¯¸ë¶„ë¥˜",
                        "ê¸°ìëª…": reporter if reporter else "ë¯¸ìƒ",
                        "ë³¸ë¬¸": content,
                    }
                )

                success += 1

                # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                time.sleep(random.uniform(0.6, 1.6))
            except KeyboardInterrupt:
                print("\nâš  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"    âŒ ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue

        print(f"âœ… {category_name}: {success}/{total}ê±´ ì €ì¥")
        return success, total

    def save_to_csv(self, data, filename=None):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not data:
            print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"results/hyundaiilbo_news_{timestamp}.csv"

        try:
            # ê²°ê³¼ ë””ë ‰í„°ë¦¬ ë³´ì¥
            out_dir = os.path.dirname(filename)
            if out_dir:
                os.makedirs(out_dir, exist_ok=True)
            with open(filename, "w", newline="", encoding="utf-8-sig") as csvfile:
                fieldnames = ["category", "title", "link", "pub_date", "reporter", "summary", "content", "collected_at"]
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

                writer.writeheader()
                for row in data:
                    writer.writerow(row)

            print(f"\nâœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")
            print(f"ğŸ“Š ì´ {len(data)}ê°œ ê¸°ì‚¬ ì €ì¥")

            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            category_stats = {}
            for item in data:
                cat = item["category"]
                category_stats[cat] = category_stats.get(cat, 0) + 1

            print("\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ í˜„í™©:")
            for cat, count in category_stats.items():
                print(f"  - {cat}: {count}ê°œ")

        except Exception as e:
            print(f"âŒ CSV ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    print("ğŸ›ï¸ í˜„ëŒ€ì¼ë³´ RSS ìˆ˜ì§‘ê¸° ì‹œì‘")
    print("=" * 50)

    collector = HyundaiIlboRSSCollector()

    print("\nğŸš€ ì „ì²´ ì¹´í…Œê³ ë¦¬ì—ì„œ ê°ê° 20ê°œ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ë‹¨ì¼ CSV ì €ì¥)...")

    max_articles = 20
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    output_file = f"results/{NEWS_OUTLET}_ì „ì²´_{timestamp}.csv"
    out_dir = os.path.dirname(output_file)
    if out_dir:
        os.makedirs(out_dir, exist_ok=True)

    total_success = 0
    total_expected = 0

    with open(output_file, "w", newline="", encoding="utf-8-sig") as csvfile:
        fieldnames = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        for category_name, rss_url in collector.rss_urls.items():
            success, expected = collector.append_rss_category(rss_url, category_name, writer, max_articles=max_articles)
            total_success += success
            total_expected += expected
            # ì¹´í…Œê³ ë¦¬ ê°„ ê°„ê²©
            time.sleep(random.uniform(1.2, 2.2))

    print(f"\nğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ! CSV ì €ì¥: {output_file}")
    if total_expected:
        print(f"ğŸ“Š ì´í•©: {total_success}/{total_expected}ê±´ ì €ì¥")


if __name__ == "__main__":
    main()
