import requests
import xml.etree.ElementTree as ET
import csv
import re
from datetime import datetime
from bs4 import BeautifulSoup
import time


def extract_full_article_content(url):
    """ë™ì•„ì¼ë³´ ê¸°ì‚¬ URLì—ì„œ ì „ì²´ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        response.encoding = "utf-8"

        soup = BeautifulSoup(response.text, "html.parser")

        # ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ: ì›ë¬¸ í˜ì´ì§€ì—ì„œ ì§€ì •ëœ XPathì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„(CSS ì„ íƒìë¡œ ë³€í™˜)
        content_element = soup.select_one(
            "html > body > div:nth-of-type(1) > div:nth-of-type(1) > main > div:nth-of-type(2) > div > div:nth-of-type(1) > section:nth-of-type(1)"
        )
        if content_element:
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for unwanted in content_element.find_all(
                ["script", "style", "iframe", "ins", "div.ad", "advertisement", "related-articles"]
            ):
                unwanted.decompose()

            full_content = content_element.get_text(separator="\n", strip=True)
            # ì •ë¦¬: ì—°ì†ëœ ì¤„ë°”ê¿ˆê³¼ ê³µë°± ì •ë¦¬
            full_content = re.sub(r"\n+", "\n", full_content)
            full_content = re.sub(r"\s+", " ", full_content)
            return full_content.strip()
        else:
            return ""

    except Exception as e:
        print(f"ë³¸ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return ""


def extract_reporter_name(soup, article_text):
    """ê¸°ìëª…ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        # ì¶”ê°€: ì§€ì •ëœ ìœ„ì¹˜ì—ì„œ ê¸°ìëª… ì¶”ì¶œ (CSS ì„ íƒìë¡œ ë³€í™˜)
        reporter_element = soup.select_one(
            "html > body > div:nth-of-type(1) > div:nth-of-type(1) > main > div:nth-of-type(2) > div > div:nth-of-type(1) > div:nth-of-type(3)"
        )
        if reporter_element:
            reporter_text = reporter_element.get_text(strip=True)
            if reporter_text:
                if "ê¸°ì" not in reporter_text:
                    reporter_text += " ê¸°ì"
                return reporter_text

        # ë‹¤ì–‘í•œ ê¸°ìëª… ì¶”ì¶œ íŒ¨í„´ (ê¸°ì¡´ ë¡œì§)
        reporter_patterns = [
            # HTMLì—ì„œ ê¸°ìëª… ì¶”ì¶œ
            r"<span[^>]*class[^>]*reporter[^>]*>([^<]+)</span>",
            r"<div[^>]*class[^>]*reporter[^>]*>([^<]+)</div>",
            r"<p[^>]*class[^>]*reporter[^>]*>([^<]+)</p>",
            # í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìëª… ì¶”ì¶œ
            r"([ê°€-í£]{2,4})\s*ê¸°ì(?:\s*=|\s*âˆ™|\s*Â·|\s*ì…ë ¥|\s*ìˆ˜ì •|\s*ì‘ì„±)",
            r"ê¸°ì\s*([ê°€-í£]{2,4})(?:\s*=|\s*âˆ™|\s*Â·)",
            r"([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›",
            r"([ê°€-í£]{2,4})\s*ë…¼ì„¤ìœ„ì›",
            r"([ê°€-í£]{2,4})\s*ì„ ì„ê¸°ì",
            r"([ê°€-í£]{2,4})\s*í¸ì§‘ìœ„ì›",
            r"/\s*([ê°€-í£]{2,4})\s*ê¸°ì",
            r"=\s*([ê°€-í£]{2,4})\s*ê¸°ì",
            r"âˆ™\s*([ê°€-í£]{2,4})\s*ê¸°ì",
            r"Â·\s*([ê°€-í£]{2,4})\s*ê¸°ì",
        ]

        # BeautifulSoup ê°ì²´ì—ì„œ ê¸°ìëª… ì°¾ê¸° (ê¸°ì¡´ ë¡œì§)
        if soup:
            reporter_elements = soup.find_all(["span", "div", "p"], class_=re.compile(r"reporter|writer|author"))
            for element in reporter_elements:
                text = element.get_text(strip=True)
                if "ê¸°ì" in text:
                    match = re.search(r"([ê°€-í£]{2,4})", text)
                    if match:
                        return match.group(1) + " ê¸°ì"

        # ê¸°ì‚¬ í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìëª… ì°¾ê¸° (ê¸°ì¡´ ë¡œì§)
        full_text = str(soup)

        for pattern in reporter_patterns:
            matches = re.findall(pattern, full_text)
            if matches:
                reporter = matches[0].strip()
                if reporter and len(reporter) >= 2:
                    return reporter + (" ê¸°ì" if "ê¸°ì" not in reporter else "")

        return "ê¸°ìëª… ì—†ìŒ"

    except Exception as e:
        print(f"ê¸°ìëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return "ê¸°ìëª… ì—†ìŒ"


def parse_donga_rss_full_content(max_articles=None):
    """ë™ì•„ì¼ë³´ RSSë¥¼ íŒŒì‹±í•˜ì—¬ ì „ì²´ ë³¸ë¬¸ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""

    rss_url = "https://rss.donga.com/total.xml"

    try:
        print("ë™ì•„ì¼ë³´ RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        response = requests.get(rss_url, headers=headers, timeout=30)
        response.raise_for_status()
        response.encoding = "utf-8"

        # XML íŒŒì‹±
        root = ET.fromstring(response.content)
        items = root.findall(".//item")
        news_data = []

        # ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ ì œí•œ
        if max_articles and len(items) > max_articles:
            items = items[:max_articles]
            print(f"âš ï¸  ìµœëŒ€ {max_articles}ê°œ ê¸°ì‚¬ë¡œ ì œí•œí•©ë‹ˆë‹¤.")

        print(f"ì´ {len(items)}ê°œì˜ ë‰´ìŠ¤ í•­ëª©ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        print("ê° ê¸°ì‚¬ì˜ ì „ì²´ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” ì¤‘... (ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

        for i, item in enumerate(items):
            try:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = item.find("title").text if item.find("title") is not None else "ì œëª© ì—†ìŒ"
                title = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", title)
                title = re.sub(r"<[^>]+>", "", title).strip()

                pub_date = item.find("pubDate").text if item.find("pubDate") is not None else ""

                # ë‚ ì§œ í¬ë§· ë³€í™˜
                formatted_date = ""
                if pub_date:
                    try:
                        date_obj = datetime.strptime(pub_date.split(" +")[0], "%a, %d %b %Y %H:%M:%S")
                        formatted_date = date_obj.strftime("%Y-%m-%d %H:%M:%S")
                    except:
                        formatted_date = pub_date

                link = item.find("link").text if item.find("link") is not None else ""

                print(f"[{i+1}/{len(items)}] ì²˜ë¦¬ ì¤‘: {title[:80]}...")

                if link:
                    try:
                        article_response = requests.get(link, headers=headers, timeout=20)
                        article_response.encoding = "utf-8"
                        soup = BeautifulSoup(article_response.text, "html.parser")

                        # ì „ì²´ ë³¸ë¬¸ ì¶”ì¶œ
                        full_content = extract_full_article_content(link)

                        # ê¸°ìëª… ì¶”ì¶œ
                        reporter_name = extract_reporter_name(soup, full_content)

                        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš° RSS descriptionë„ í¬í•¨
                        if len(full_content) < 200:
                            rss_description = (
                                item.find("description").text if item.find("description") is not None else ""
                            )
                            rss_description = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", rss_description)
                            rss_description = re.sub(r"<[^>]+>", "", rss_description).strip()

                            if rss_description:
                                full_content = (
                                    rss_description + "\n\n" + full_content if full_content else rss_description
                                )

                        # ë°ì´í„° ì €ì¥ (ì–¸ë¡ ì‚¬: ë™ì•„ì¼ë³´, ì¹´í…Œê³ ë¦¬: ì „ì²´)
                        news_data.append(
                            {
                                "ì–¸ë¡ ì‚¬": "ë™ì•„ì¼ë³´",
                                "ì œëª©": title,
                                "ë‚ ì§œ": formatted_date,
                                "ì¹´í…Œê³ ë¦¬": "ì „ì²´",
                                "ê¸°ìëª…": reporter_name,
                                "ë³¸ë¬¸": full_content,
                            }
                        )

                        # ì„œë²„ ë¶€í•˜ ë°©ì§€
                        time.sleep(1)

                    except Exception as e:
                        print(f"  â¤ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ RSS ê¸°ë³¸ ì •ë³´ëŠ” ì €ì¥
                        description = item.find("description").text if item.find("description") is not None else ""
                        description = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", description)
                        description = re.sub(r"<[^>]+>", "", description).strip()

                        news_data.append(
                            {
                                "ì–¸ë¡ ì‚¬": "ë™ì•„ì¼ë³´",
                                "ì œëª©": title,
                                "ë‚ ì§œ": formatted_date,
                                "ì¹´í…Œê³ ë¦¬": "ì „ì²´",
                                "ê¸°ìëª…": "ê¸°ìëª… ì—†ìŒ",
                                "ë³¸ë¬¸": description,
                            }
                        )
                        continue

            except Exception as e:
                print(f"RSS í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        # CSV ì €ì¥ ëŒ€ì‹  ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°˜í™˜
        if news_data:
            print(f"\nâœ… ì„±ê³µì ìœ¼ë¡œ {len(news_data)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
            total_chars = sum(len(item["ë³¸ë¬¸"]) for item in news_data)
            avg_chars = total_chars // len(news_data) if news_data else 0
            print(f"ğŸ“Š í‰ê·  ë³¸ë¬¸ ê¸¸ì´: {avg_chars:,}ì")
            print(f"ğŸ“Š ì´ ë³¸ë¬¸ ê¸¸ì´: {total_chars:,}ì")
            return news_data
        else:
            print("âŒ ì¶”ì¶œëœ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

    except Exception as e:
        print(f"âŒ RSS íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


def parse_donga_category_rss_full(category="total", max_articles=None):
    """íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ë™ì•„ì¼ë³´ RSSì—ì„œ ì „ì²´ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜"""

    category_urls = {
        "total": "https://rss.donga.com/total.xml",
        "politics": "https://rss.donga.com/politics.xml",
        "national": "https://rss.donga.com/national.xml",
        "economy": "https://rss.donga.com/economy.xml",
        "international": "https://rss.donga.com/international.xml",
    }

    if category not in category_urls:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬ì…ë‹ˆë‹¤.")
        print(f"âœ… ì§€ì› ì¹´í…Œê³ ë¦¬: {', '.join(category_urls.keys())}")
        return None

    print(f"ğŸ“° {category} ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")

    # ì „ì—­ ë³€ìˆ˜ ìˆ˜ì •í•˜ì—¬ íŠ¹ì • ì¹´í…Œê³ ë¦¬ URL ì‚¬ìš©
    global rss_url
    original_url = "https://rss.donga.com/total.xml"

    # í•¨ìˆ˜ ë‚´ì—ì„œ URL ë³€ê²½
    import types

    def modified_parse():
        # parse_donga_rss_full_content í•¨ìˆ˜ì˜ rss_urlì„ ì„ì‹œ ë³€ê²½
        func_code = parse_donga_rss_full_content.__code__
        func_globals = parse_donga_rss_full_content.__globals__.copy()

        # ìƒˆë¡œìš´ í•¨ìˆ˜ ìƒì„± (ì¹´í…Œê³ ë¦¬ URL ì‚¬ìš©)
        def category_parse():
            rss_url = category_urls[category]

            try:
                print(f"ğŸ“¡ {category} RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                }

                response = requests.get(rss_url, headers=headers, timeout=30)
                response.raise_for_status()
                response.encoding = "utf-8"

                root = ET.fromstring(response.content)
                items = root.findall(".//item")

                # ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ ì œí•œ
                if max_articles and len(items) > max_articles:
                    items = items[:max_articles]
                    print(f"âš ï¸  ìµœëŒ€ {max_articles}ê°œ ê¸°ì‚¬ë¡œ ì œí•œí•©ë‹ˆë‹¤.")

                news_data = []
                print(f"ì´ {len(items)}ê°œì˜ ë‰´ìŠ¤ í•­ëª©ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
                print("ê° ê¸°ì‚¬ì˜ ì „ì²´ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” ì¤‘...")

                for i, item in enumerate(items):
                    try:
                        title = item.find("title").text if item.find("title") is not None else "ì œëª© ì—†ìŒ"
                        title = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", title)
                        title = re.sub(r"<[^>]+>", "", title).strip()

                        pub_date = item.find("pubDate").text if item.find("pubDate") is not None else ""
                        formatted_date = ""
                        if pub_date:
                            try:
                                date_obj = datetime.strptime(pub_date.split(" +")[0], "%a, %d %b %Y %H:%M:%S")
                                formatted_date = date_obj.strftime("%Y-%m-%d %H:%M:%S")
                            except:
                                formatted_date = pub_date

                        link = item.find("link").text if item.find("link") is not None else ""

                        print(f"[{i+1}/{len(items)}] ì²˜ë¦¬ ì¤‘: {title[:60]}...")

                        if link:
                            try:
                                full_content = extract_full_article_content(link)

                                article_response = requests.get(link, headers=headers, timeout=20)
                                soup = BeautifulSoup(article_response.text, "html.parser")
                                reporter_name = extract_reporter_name(soup, full_content)

                                if len(full_content) < 200:
                                    rss_description = (
                                        item.find("description").text if item.find("description") is not None else ""
                                    )
                                    rss_description = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", rss_description)
                                    rss_description = re.sub(r"<[^>]+>", "", rss_description).strip()

                                    if rss_description:
                                        full_content = (
                                            rss_description + "\n\n" + full_content if full_content else rss_description
                                        )

                                news_data.append(
                                    {
                                        "ì–¸ë¡ ì‚¬": "ë™ì•„ì¼ë³´",
                                        "ì œëª©": title,
                                        "ë‚ ì§œ": formatted_date,
                                        "ì¹´í…Œê³ ë¦¬": category,
                                        "ê¸°ìëª…": reporter_name,
                                        "ë³¸ë¬¸": full_content,
                                    }
                                )

                                time.sleep(1)

                            except Exception as e:
                                print(f"  â¤ ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                                continue

                    except Exception as e:
                        print(f"RSS í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue

                # CSV ì €ì¥ ëŒ€ì‹  ë°ì´í„° ë°˜í™˜
                if news_data:
                    print(f"\nâœ… ì„±ê³µì ìœ¼ë¡œ {len(news_data)}ê°œì˜ {category} ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")

                    total_chars = sum(len(item["ë³¸ë¬¸"]) for item in news_data)
                    avg_chars = total_chars // len(news_data) if news_data else 0
                    print(f"ğŸ“Š í‰ê·  ë³¸ë¬¸ ê¸¸ì´: {avg_chars:,}ì")

                    return news_data
                else:
                    print("âŒ ì¶”ì¶œëœ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return []

            except Exception as e:
                print(f"âŒ RSS íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return []

        return category_parse()

    return modified_parse()


# ìƒˆ í•¨ìˆ˜: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_news_csv(news_data):
    """ì–¸ë¡ ì‚¬, ì œëª©, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, ê¸°ìëª…, ë³¸ë¬¸ ìˆœìœ¼ë¡œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    import os
    from datetime import datetime

    # results í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists("results"):
        os.makedirs("results")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"results/ë™ì•„ì¼ë³´_ì „ì²´_{timestamp}.csv"
    fieldnames = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]

    with open(filename, "w", newline="", encoding="utf-8-sig") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for row in news_data:
            # ê° ì—´ì˜ ìˆœì„œë¥¼ ë³´ì¥í•˜ë©° row ë°ì´í„° ì €ì¥
            writer.writerow(
                {
                    "ì–¸ë¡ ì‚¬": row.get("ì–¸ë¡ ì‚¬", ""),
                    "ì œëª©": row.get("ì œëª©", ""),
                    "ë‚ ì§œ": row.get("ë‚ ì§œ", ""),
                    "ì¹´í…Œê³ ë¦¬": row.get("ì¹´í…Œê³ ë¦¬", ""),
                    "ê¸°ìëª…": row.get("ê¸°ìëª…", ""),
                    "ë³¸ë¬¸": row.get("ë³¸ë¬¸", ""),
                }
            )

    print(f"ğŸ“ íŒŒì¼ëª…: {filename}")
    return filename


if __name__ == "__main__":
    print("ğŸ—ï¸  ë™ì•„ì¼ë³´ RSS ì „ì²´ ë³¸ë¬¸ í¬ë¡¤ë§ (ëª¨ë“  ì¹´í…Œê³ ë¦¬)")
    print("=" * 60)

    try:
        # ëª¨ë“  ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘
        categories = ["total", "politics", "national", "economy", "international"]
        all_news_data = []

        for category in categories:
            print(f"\nğŸš€ {category} ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ìµœëŒ€ 20ê°œ)")

            if category == "total":
                news_data = parse_donga_rss_full_content(max_articles=20)
            else:
                news_data = parse_donga_category_rss_full(category, max_articles=20)

            if news_data:
                # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì—…ë°ì´íŠ¸
                for item in news_data:
                    item["ì¹´í…Œê³ ë¦¬"] = category
                all_news_data.extend(news_data)
                print(f"âœ… {category} ì¹´í…Œê³ ë¦¬: {len(news_data)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                print(f"âŒ {category} ì¹´í…Œê³ ë¦¬: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

        if all_news_data:
            saved_file = save_news_csv(all_news_data)
            print(f"\nğŸ‰ ì™„ë£Œ! ì´ {len(all_news_data)}ê°œ ê¸°ì‚¬ CSV íŒŒì¼ ì €ì¥: {saved_file}")
        else:
            print("âŒ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
