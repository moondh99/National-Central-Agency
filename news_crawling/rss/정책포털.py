import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import pandas as pd
import time
import random
import re
from datetime import datetime
import csv
from urllib.parse import urljoin, urlparse
import logging

class KoreaPolicyRSSCrawler:
    def __init__(self):
        """ì •ì±…ë¸Œë¦¬í•‘ RSS í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.base_url = "https://www.korea.kr"
        
        # 16ê°œ RSS í”¼ë“œ ì •ì˜
        self.rss_feeds = {
            'ì •ì±…ë‰´ìŠ¤': 'https://www.korea.kr/rss/policy.xml',
            'êµ­ë¯¼ì´_ë§í•˜ëŠ”_ì •ì±…': 'https://www.korea.kr/rss/reporter.xml',
            'ê¸°ê³ _ì¹¼ëŸ¼': 'https://www.korea.kr/rss/gigo_column.xml',
            'ë³´ë„ìë£Œ': 'https://www.korea.kr/rss/pressrelease.xml',
            'ì‚¬ì‹¤ì€_ì´ë ‡ìŠµë‹ˆë‹¤': 'https://www.korea.kr/rss/fact.xml',
            'ë¶€ì²˜_ë¸Œë¦¬í•‘': 'https://www.korea.kr/rss/ebriefing.xml',
            'ëŒ€í†µë ¹ì‹¤_ë¸Œë¦¬í•‘': 'https://www.korea.kr/rss/president.xml',
            'êµ­ë¬´íšŒì˜_ë¸Œë¦¬í•‘': 'https://www.korea.kr/rss/cabinet.xml',
            'ì—°ì„¤ë¬¸': 'https://www.korea.kr/rss/speech.xml',
            'ì •ì±…ìë£Œ_ì „ë¬¸ìë£Œ': 'https://www.korea.kr/rss/expdoc.xml',
            'Kê³µê°_ì „ì²´': 'https://www.korea.kr/rss/archive.xml'
        }
        
        # User-Agent ë¦¬ìŠ¤íŠ¸
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15'
        ]
        
        self.articles = []
        self.session = requests.Session()
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)

    def get_random_headers(self):
        """ëœë¤ í—¤ë” ìƒì„±"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }

    def random_delay(self, min_delay=1, max_delay=3):
        """ëœë¤ ë”œë ˆì´"""
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)

    def fetch_rss_feed(self, rss_url, max_retries=3):
        """RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°"""
        for attempt in range(max_retries):
            try:
                headers = self.get_random_headers()
                response = self.session.get(rss_url, headers=headers, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except Exception as e:
                self.logger.warning(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    self.random_delay(2, 5)
                else:
                    self.logger.error(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ìµœì¢… ì‹¤íŒ¨: {rss_url}")
                    return None

    def parse_rss_feed(self, rss_content):
        """RSS í”¼ë“œ íŒŒì‹±"""
        try:
            root = ET.fromstring(rss_content)
            items = []
            
            for item in root.findall('.//item'):
                article_info = {}
                
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title_elem = item.find('title')
                article_info['title'] = title_elem.text.strip() if title_elem is not None else ''
                
                link_elem = item.find('link')
                article_info['link'] = link_elem.text.strip() if link_elem is not None else ''
                
                pubdate_elem = item.find('pubDate')
                article_info['pub_date'] = pubdate_elem.text.strip() if pubdate_elem is not None else ''
                
                guid_elem = item.find('guid')
                article_info['guid'] = guid_elem.text.strip() if guid_elem is not None else ''
                
                # dc:creator ì¶”ì¶œ (ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ê³ ë ¤)
                creator_elem = item.find('.//{http://purl.org/dc/elements/1.1/}creator')
                article_info['creator'] = creator_elem.text.strip() if creator_elem is not None else ''
                
                # descriptionì—ì„œ ê°„ë‹¨í•œ ë‚´ìš© ì¶”ì¶œ
                desc_elem = item.find('description')
                if desc_elem is not None:
                    desc_text = desc_elem.text or ''
                    # CDATA ì²˜ë¦¬
                    if desc_text.startswith('<![CDATA[') and desc_text.endswith(']]>'):
                        desc_text = desc_text[9:-3]
                    
                    # HTML íƒœê·¸ ì œê±°í•˜ì—¬ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                    soup = BeautifulSoup(desc_text, 'html.parser')
                    article_info['description'] = soup.get_text().strip()[:200] + '...' if len(soup.get_text().strip()) > 200 else soup.get_text().strip()
                else:
                    article_info['description'] = ''
                
                items.append(article_info)
            
            return items
        except Exception as e:
            self.logger.error(f"RSS íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []

    def extract_article_content(self, article_url, max_retries=3):
        """ê°œë³„ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        for attempt in range(max_retries):
            try:
                headers = self.get_random_headers()
                response = self.session.get(article_url, headers=headers, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ë³¸ë¬¸ ì¶”ì¶œ (ì •ì±…ë¸Œë¦¬í•‘ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìµœì í™”)
                content_selectors = [
                    '.article_body',  # ì¼ë°˜ ê¸°ì‚¬
                    '.rbody',         # ë¸Œë¦¬í•‘ í˜ì´ì§€
                    '.view_cont',     # ë·° í˜ì´ì§€
                    '.cont_body',     # ì½˜í…ì¸  ë³¸ë¬¸
                    '.policy_body',   # ì •ì±… ë³¸ë¬¸
                    '.briefing_cont', # ë¸Œë¦¬í•‘ ë‚´ìš©
                    '.news_cont'      # ë‰´ìŠ¤ ë‚´ìš©
                ]
                
                content = ""
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        content = content_elem.get_text().strip()
                        break
                
                # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„
                if not content:
                    # í—¤ë”, í‘¸í„°, ì‚¬ì´ë“œë°” ë“± ì œê±°
                    for elem in soup.find_all(['header', 'footer', 'nav', 'aside', 'script', 'style']):
                        elem.decompose()
                    
                    main_content = soup.find('main') or soup.find('div', class_='content') or soup.find('body')
                    if main_content:
                        content = main_content.get_text().strip()
                
                # ë¶€ì²˜ëª…/ë‹´ë‹¹ì ì¶”ì¶œ
                department = self.extract_department_info(content)
                
                # í…ìŠ¤íŠ¸ ì •ë¦¬
                content = re.sub(r'\s+', ' ', content).strip()
                
                return {
                    'content': content[:2000] + '...' if len(content) > 2000 else content,
                    'department': department
                }
                
            except Exception as e:
                self.logger.warning(f"ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    self.random_delay(1, 3)
                else:
                    return {'content': 'ì¶”ì¶œ ì‹¤íŒ¨', 'department': ''}

    def extract_department_info(self, content):
        """ë¶€ì²˜ëª…/ë‹´ë‹¹ì ì •ë³´ ì¶”ì¶œ"""
        # ì •ì±…ë¸Œë¦¬í•‘ íŠ¹ì„±ì— ë§ëŠ” ë¶€ì²˜ëª… ì¶”ì¶œ íŒ¨í„´
        patterns = [
            r'ë¬¸ì˜\s*:\s*([^(]+?)(?:\(|$)',
            r'ë‹´ë‹¹\s*:\s*([^(]+?)(?:\(|$)',
            r'ìë£Œì œê³µ\s*:\s*([^(]+?)(?:\(|$)',
            r'ë¬¸ì˜ì²˜\s*:\s*([^(]+?)(?:\(|$)',
            r'ë°œí‘œë¶€ì²˜\s*:\s*([^(]+?)(?:\(|$)',
            r'([ê°€-í£]+ë¶€|[ê°€-í£]+ì²­|[ê°€-í£]+ì›|[ê°€-í£]+ì‹¤|[ê°€-í£]+ìœ„ì›íšŒ)\s+[ê°€-í£]+ê³¼',
            r'([ê°€-í£]+ë¶€|[ê°€-í£]+ì²­|[ê°€-í£]+ì›|[ê°€-í£]+ì‹¤|[ê°€-í£]+ìœ„ì›íšŒ)'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                dept = match.group(1).strip()
                # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
                dept = re.sub(r'[^\wê°€-í£\s]', '', dept).strip()
                if len(dept) > 1 and len(dept) < 50:
                    return dept
        
        return ''

    def crawl_feed(self, category, rss_url, max_items=20):
        """ê°œë³„ RSS í”¼ë“œ í¬ë¡¤ë§"""
        self.logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {category}")
        
        # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
        rss_content = self.fetch_rss_feed(rss_url)
        if not rss_content:
            return
        
        # RSS íŒŒì‹±
        rss_items = self.parse_rss_feed(rss_content)
        if not rss_items:
            self.logger.warning(f"RSS ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤: {category}")
            return
        
        # ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ë§Œ ì²˜ë¦¬
        items_to_process = rss_items[:max_items]
        
        for i, item in enumerate(items_to_process, 1):
            try:
                self.logger.info(f"{category} ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘: {i}/{len(items_to_process)} - {item['title'][:50]}...")
                
                # ê¸°ì‚¬ ìƒì„¸ ë‚´ìš© ì¶”ì¶œ
                if item['link']:
                    article_detail = self.extract_article_content(item['link'])
                    
                    article_data = {
                        'category': category,
                        'title': item['title'],
                        'link': item['link'],
                        'pub_date': item['pub_date'],
                        'creator': item['creator'],
                        'description': item['description'],
                        'content': article_detail['content'],
                        'department': article_detail['department'],
                        'collected_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    }
                    
                    self.articles.append(article_data)
                
                # ë”œë ˆì´
                self.random_delay(1, 3)
                
            except Exception as e:
                self.logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue
        
        self.logger.info(f"{category} í¬ë¡¤ë§ ì™„ë£Œ: {len(items_to_process)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬")

    def crawl_all_feeds(self, max_items_per_feed=20):
        """ëª¨ë“  RSS í”¼ë“œ í¬ë¡¤ë§"""
        total_feeds = len(self.rss_feeds)
        self.logger.info(f"ì „ì²´ {total_feeds}ê°œ RSS í”¼ë“œ í¬ë¡¤ë§ ì‹œì‘")
        
        for i, (category, rss_url) in enumerate(self.rss_feeds.items(), 1):
            try:
                self.logger.info(f"[{i}/{total_feeds}] {category} í”¼ë“œ í¬ë¡¤ë§ ì¤‘...")
                self.crawl_feed(category, rss_url, max_items_per_feed)
                
                # í”¼ë“œ ê°„ ë”œë ˆì´
                if i < total_feeds:
                    self.random_delay(2, 5)
                    
            except Exception as e:
                self.logger.error(f"{category} í”¼ë“œ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                continue
        
        self.logger.info(f"ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: {len(self.articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        self.print_statistics()

    def crawl_specific_feeds(self, feed_names, max_items_per_feed=20):
        """íŠ¹ì • RSS í”¼ë“œë§Œ í¬ë¡¤ë§"""
        for feed_name in feed_names:
            if feed_name in self.rss_feeds:
                self.crawl_feed(feed_name, self.rss_feeds[feed_name], max_items_per_feed)
            else:
                self.logger.warning(f"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í”¼ë“œ: {feed_name}")
                available_feeds = list(self.rss_feeds.keys())
                self.logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ë“œ: {available_feeds}")

    def save_to_csv(self, filename=None):
        """CSV íŒŒì¼ë¡œ ì €ì¥"""
        if not self.articles:
            self.logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'results/ì •ì±…ë¸Œë¦¬í•‘_RSS_{timestamp}.csv'
        
        try:
            df = pd.DataFrame(self.articles)
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            self.logger.info(f"CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            self.logger.info(f"ì´ {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥")
        except Exception as e:
            self.logger.error(f"CSV ì €ì¥ ì˜¤ë¥˜: {e}")

    def print_statistics(self):
        """í¬ë¡¤ë§ í†µê³„ ì¶œë ¥"""
        if not self.articles:
            return
        
        df = pd.DataFrame(self.articles)
        
        print("\n" + "="*60)
        print("ì •ì±…ë¸Œë¦¬í•‘ RSS í¬ë¡¤ë§ í†µê³„")
        print("="*60)
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = df['category'].value_counts()
        print(f"\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜:")
        for category, count in category_stats.items():
            print(f"  â€¢ {category}: {count}ê°œ")
        
        # ë¶€ì²˜ë³„ í†µê³„ (ìƒìœ„ 10ê°œ)
        dept_stats = df[df['department'] != '']['department'].value_counts().head(10)
        if not dept_stats.empty:
            print(f"\nğŸ›ï¸ ì£¼ìš” ë¶€ì²˜ë³„ ê¸°ì‚¬ ìˆ˜ (ìƒìœ„ 10ê°œ):")
            for dept, count in dept_stats.items():
                print(f"  â€¢ {dept}: {count}ê°œ")
        
        print(f"\nğŸ“ˆ ì „ì²´ ìš”ì•½:")
        print(f"  â€¢ ì´ ê¸°ì‚¬ ìˆ˜: {len(self.articles)}ê°œ")
        print(f"  â€¢ í¬ë¡¤ë§ ì¹´í…Œê³ ë¦¬: {len(category_stats)}ê°œ")
        print(f"  â€¢ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {len(df[df['content'] != 'ì¶”ì¶œ ì‹¤íŒ¨'])}ê°œ")
        print(f"  â€¢ ë¶€ì²˜ ì •ë³´ ì¶”ì¶œ: {len(df[df['department'] != ''])}ê°œ")
        print("="*60)

    def get_available_feeds(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ RSS í”¼ë“œ ëª©ë¡ ë°˜í™˜"""
        return list(self.rss_feeds.keys())


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ì •ì±…ë¸Œë¦¬í•‘(korea.kr) RSS í¬ë¡¤ëŸ¬")
    print("="*50)
    
    crawler = KoreaPolicyRSSCrawler()
    
    # ì‚¬ìš© ì˜ˆì‹œ 1: ì „ì²´ í”¼ë“œ í¬ë¡¤ë§ (ê° í”¼ë“œë‹¹ 10ê°œì”©)
    print("ì „ì²´ RSS í”¼ë“œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    crawler.crawl_all_feeds(max_items_per_feed=5)
    
    # CSV ì €ì¥
    crawler.save_to_csv()
    
    # ì‚¬ìš© ì˜ˆì‹œ 2: íŠ¹ì • í”¼ë“œë§Œ í¬ë¡¤ë§
    # crawler.crawl_specific_feeds(['ì •ì±…ë‰´ìŠ¤', 'ë³´ë„ìë£Œ'], max_items_per_feed=20)
    
    print("\ní¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
