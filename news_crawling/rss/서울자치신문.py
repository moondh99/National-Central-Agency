import feedparser
import csv
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import time
import random


def get_random_user_agent():
    """ëœë¤ User-Agent ë°˜í™˜"""
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
    ]
    return random.choice(user_agents)


def extract_onseoul_article_content(url, rss_summary=""):
    """ì„œìš¸ìì¹˜ì‹ ë¬¸ ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ê³¼ ê¸°ìëª…ì„ ì¶”ì¶œ"""
    try:
        session = requests.Session()

        headers = {
            "User-Agent": get_random_user_agent(),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Referer": "http://www.onseoul.net/",
            "Cache-Control": "no-cache",
        }

        print(f"    ì ‘ì† ì‹œë„: {url[:80]}...")

        # ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸ í›„ ì‹¤ì œ ê¸°ì‚¬ ì ‘ê·¼
        try:
            session.get("http://www.onseoul.net/", headers=headers, timeout=5)
            time.sleep(0.5)

            response = session.get(url, headers=headers, timeout=10)
            response.raise_for_status()

            # ì„œìš¸ìì¹˜ì‹ ë¬¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ì ‘ê·¼ ì œí•œì´ ì—†ìŒ
            if len(response.content) < 3000:  # 3KB ë¯¸ë§Œì´ë©´ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŒ
                print(f"    âš  ì‘ë‹µ í¬ê¸°ê°€ ì‘ìŒ (í¬ê¸°: {len(response.content)} bytes)")

        except Exception as e:
            print(f"    âš  ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
            return "", rss_summary if rss_summary else "ì›¹í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨"

        soup = BeautifulSoup(response.content, "html.parser")
        full_text = soup.get_text()

        # ê¸°ìëª… ì¶”ì¶œ - ì„œìš¸ìì¹˜ì‹ ë¬¸ íŒ¨í„´ì— ë§ê²Œ ìˆ˜ì •
        reporter = ""

        # ì„œìš¸ìì¹˜ì‹ ë¬¸ì€ ê¸°ì‚¬ í•˜ë‹¨ì— ê¸°ìëª…ì´ í‘œì‹œë¨
        reporter_patterns = [
            r"([ê°€-í£]{2,4})\s*ê¸°ì\s*[0-9]{4}-[0-9]{2}-[0-9]{2}",  # ê¸°ìëª… ê¸°ì ë‚ ì§œ
            r"([ê°€-í£]{2,4})\s*ê¸°ì",  # ê¸°ìëª… ê¸°ì
            r"ê¸°ì\s*([ê°€-í£]{2,4})",  # ê¸°ì ê¸°ìëª…
            r"([ê°€-í£]{2,4})\s*íŠ¹íŒŒì›",  # ê¸°ìëª… íŠ¹íŒŒì›
        ]

        # ê¸°ì‚¬ ë ë¶€ë¶„ì—ì„œ ê¸°ìëª… ì°¾ê¸°
        article_end = full_text[-500:]

        for pattern in reporter_patterns:
            match = re.search(pattern, article_end)
            if match:
                reporter = match.group(1)
                reporter = re.sub(r"ê¸°ì|íŠ¹íŒŒì›", "", reporter).strip()
                if len(reporter) >= 2 and len(reporter) <= 4:
                    break

        # ë³¸ë¬¸ ì¶”ì¶œ - ì„œìš¸ìì¹˜ì‹ ë¬¸ HTML êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
        content = ""

        # ë°©ë²• 1: ê¸°ì‚¬ ë³¸ë¬¸ êµ¬ì¡° ì°¾ê¸°
        content_selectors = [
            "div.user-content",  # ì‚¬ìš©ì ì½˜í…ì¸  ì˜ì—­
            'div[class*="article"]',  # article ê´€ë ¨ í´ë˜ìŠ¤
            'div[class*="content"]',  # content ê´€ë ¨ í´ë˜ìŠ¤
            'div[class*="news"]',  # news ê´€ë ¨ í´ë˜ìŠ¤
            "article",  # article íƒœê·¸
            "main",  # main íƒœê·¸
        ]

        for selector in content_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text().strip()
                if len(text) > len(content):
                    content = text

        # ë°©ë²• 2: P íƒœê·¸ ê¸°ë°˜ ì¶”ì¶œ
        if len(content) < 200:
            paragraphs = soup.find_all("p")
            content_parts = []

            for p in paragraphs:
                text = p.get_text().strip()
                if (
                    len(text) > 20
                    and not re.search(
                        r"ì…ë ¥\s*\d{4}\.\d{2}\.\d{2}|Copyright|ì €ì‘ê¶Œ|ì„œìš¸ìì¹˜ì‹ ë¬¸|www\.onseoul\.net", text
                    )
                    and not text.startswith(("â–¶", "â˜", "â€»", "â– ", "â–²", "["))
                    and not re.search(r"[ê°€-í£]{2,4}\s*ê¸°ì\s*\d{4}-\d{2}-\d{2}", text)
                ):
                    content_parts.append(text)

            if content_parts:
                content = " ".join(content_parts)

        # ë°©ë²• 3: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ (ìµœí›„ ìˆ˜ë‹¨)
        if len(content) < 100:
            # ì œëª© ì´í›„ ë³¸ë¬¸ ì‹œì‘ ì§€ì  ì°¾ê¸°
            lines = full_text.split("\n")
            content_lines = []
            article_started = False

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                # ê¸°ì‚¬ ì‹œì‘ ê°ì§€ - ì´ë¯¸ì§€ ë‹¤ìŒë¶€í„° ë³¸ë¬¸ ì‹œì‘
                if not article_started and (
                    len(line) > 30
                    and not line.startswith(("ì„œëª…ì˜¥", "ê¹€ì •ë¯¼", "ë°•í˜„ìˆ˜"))
                    and not re.search(r"\.(png|jpg|jpeg)$", line)
                ):
                    article_started = True

                # ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘
                if article_started and len(line) > 10:
                    # ë¶ˆí•„ìš”í•œ ë¼ì¸ ì œì™¸
                    if not re.search(
                        r"Copyright|ì €ì‘ê¶Œ|ì„œìš¸ìì¹˜ì‹ ë¬¸|www\.onseoul\.net|[ê°€-í£]{2,4}\s*ê¸°ì\s*\d{4}-\d{2}-\d{2}", line
                    ):
                        content_lines.append(line)

                # ê¸°ì‚¬ ë ê°ì§€
                if re.search(r"[ê°€-í£]{2,4}\s*ê¸°ì\s*\d{4}-\d{2}-\d{2}", line):
                    break

            if content_lines:
                content = " ".join(content_lines)

        # ë³¸ë¬¸ ì •ì œ
        content = clean_onseoul_content(content)

        # RSS ìš”ì•½ì´ ë” ì¢‹ìœ¼ë©´ RSS ìš”ì•½ ì‚¬ìš©
        if rss_summary and (len(content) < 100 or len(rss_summary) > len(content)):
            content = rss_summary
            print(f"    RSS ìš”ì•½ ì±„íƒ (ê¸¸ì´: {len(rss_summary)})")

        print(f"    ìµœì¢… ë³¸ë¬¸ ê¸¸ì´: {len(content)}")
        return reporter, content

    except Exception as e:
        print(f"    âŒ ì—ëŸ¬: {e}")
        return "", rss_summary if rss_summary else f"ì˜¤ë¥˜: {str(e)}"


def clean_onseoul_content(content):
    """ì„œìš¸ìì¹˜ì‹ ë¬¸ ê¸°ì‚¬ ë³¸ë¬¸ ì •ì œ"""
    if not content:
        return ""

    # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ë“¤ ì œê±° - ì„œìš¸ìì¹˜ì‹ ë¬¸ íŠ¹ì„±ì— ë§ê²Œ ìˆ˜ì •
    remove_patterns = [
        r"ì…ë ¥\s*\d{4}\.\d{2}\.\d{2}.*?\d{2}:\d{2}",
        r"[ê°€-í£]{2,4}\s*ê¸°ì\s*\d{4}-\d{2}-\d{2}.*?\d{2}:\d{2}",
        r"Copyright.*www\.onseoul\.net.*reserved",
        r"ì„œìš¸ìì¹˜ì‹ ë¬¸.*ë¬´ë‹¨.*ì „ì¬.*ê¸ˆì§€",
        r"ê´€ë ¨ê¸°ì‚¬.*ë”ë³´ê¸°",
        r"í˜ì´ìŠ¤ë¶.*íŠ¸ìœ„í„°.*ì¹´ì¹´ì˜¤",
        r"êµ¬ë….*ì‹ ì²­",
        r"ê´‘ê³ ",
        r"ì‚¬ì§„.*ì œê³µ",  # ì‚¬ì§„ ì¶œì²˜ ì œê±°
        r"í¬ìŠ¤í„°.*jpg",  # í¬ìŠ¤í„° íŒŒì¼ëª… ì œê±°
        r"ì„œìš¸ìì¹˜ì‹ ë¬¸.*ì¹¼ëŸ¼í•„ì§„",  # ì¹¼ëŸ¼ ì†Œê°œ ì œê±°
    ]

    for pattern in remove_patterns:
        content = re.sub(pattern, "", content, flags=re.IGNORECASE)

    # ê³µë°± ì •ë¦¬
    content = re.sub(r"\s+", " ", content).strip()

    # ê¸¸ì´ ì œí•œ
    if len(content) > 1500:
        content = content[:1500] + "..."

    return content


def fetch_onseoul_rss_to_csv(rss_url, output_file, max_articles=30, category="ì „ì²´ê¸°ì‚¬"):  # category ì¶”ê°€
    """ì„œìš¸ìì¹˜ì‹ ë¬¸ RSSë¥¼ íŒŒì‹±í•˜ì—¬ CSVë¡œ ì €ì¥"""

    print(f"ì„œìš¸ìì¹˜ì‹ ë¬¸ RSS í”¼ë“œ íŒŒì‹± ì¤‘: {rss_url}")

    # RSS íŒŒì‹±
    try:
        headers = {"User-Agent": get_random_user_agent()}
        response = requests.get(rss_url, headers=headers, timeout=10)
        feed = feedparser.parse(response.content)
    except:
        feed = feedparser.parse(rss_url)

    if not feed.entries:
        print("âŒ RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"âœ… RSSì—ì„œ {len(feed.entries)}ê°œ ê¸°ì‚¬ ë°œê²¬")

    success_count = 0
    total_count = min(len(feed.entries), max_articles)

    # CSV íŒŒì¼ ìƒì„±
    with open(output_file, "w", newline="", encoding="utf-8-sig") as csvfile:
        fieldnames = ["ì–¸ë¡ ì‚¬", "ì œëª©", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ê¸°ìëª…", "ë³¸ë¬¸"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

        print(f"ì´ {total_count}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘...\n")

        for i, entry in enumerate(feed.entries[:max_articles]):
            try:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                title = entry.title.strip()
                title = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", title)

                link = entry.link

                # RSS ìš”ì•½ ì •ë³´ ì¶”ì¶œ
                summary = ""
                if hasattr(entry, "description"):
                    summary = entry.description.strip()
                    # HTML íƒœê·¸ì™€ CDATA ì œê±°
                    summary = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", summary, flags=re.DOTALL)
                    summary = re.sub(r"<[^>]+>", "", summary)  # HTML íƒœê·¸ ì œê±°
                    summary = clean_onseoul_content(summary)
                elif hasattr(entry, "summary"):
                    summary = entry.summary.strip()
                    summary = re.sub(r"<[^>]+>", "", summary)
                    summary = clean_onseoul_content(summary)

                # ë‚ ì§œ í˜•ì‹ ë³€í™˜
                if hasattr(entry, "published_parsed") and entry.published_parsed:
                    date = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S")
                elif hasattr(entry, "pubdate_parsed") and entry.pubdate_parsed:
                    date = datetime(*entry.pubdate_parsed[:6]).strftime("%Y-%m-%d %H:%M:%S")
                else:
                    date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                print(f"[{i+1}/{total_count}] {title[:60]}...")

                # RSSì—ì„œ ê¸°ìëª… ì¶”ì¶œ
                if hasattr(entry, "author") and entry.author:
                    author_text = entry.author.strip()
                    match = re.search(r"([ê°€-í£]{2,4})", author_text)
                    reporter = match.group(1) + " ê¸°ì" if match else author_text
                else:
                    reporter = "ë¯¸ìƒ"
                # ë³¸ë¬¸ì€ RSS description ì‚¬ìš©
                content = summary

                if not content.strip():
                    print(f"    âš  ë³¸ë¬¸ì´ ë¹„ì–´ìˆì–´ ê±´ë„ˆëœ€")
                    continue

                # CSVì— ì“°ê¸°
                writer.writerow(
                    {
                        "ì–¸ë¡ ì‚¬": "ì„œìš¸ìì¹˜ì‹ ë¬¸",
                        "ì œëª©": title,
                        "ë‚ ì§œ": date,
                        "ì¹´í…Œê³ ë¦¬": category,
                        "ê¸°ìëª…": reporter,
                        "ë³¸ë¬¸": content,
                    }
                )

                success_count += 1
                print(f"    âœ… ì„±ê³µ! (ê¸°ì: {reporter}, ë³¸ë¬¸: {len(content)}ì)")

                # ì§„í–‰ë¥  í‘œì‹œ
                if (i + 1) % 5 == 0:
                    print(f"\nğŸ“Š ì§„í–‰ë¥ : {i+1}/{total_count} ({(i+1)/total_count*100:.1f}%)")
                    print(f"ğŸ“ˆ ì„±ê³µë¥ : {success_count}/{i+1} ({success_count/(i+1)*100:.1f}%)\n")

                # ëœë¤ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                delay = random.uniform(1.0, 2.0)
                time.sleep(delay)

            except KeyboardInterrupt:
                print("\nâš  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"    âŒ ì˜¤ë¥˜: {e}")
                continue

        print(f"\n{'='*70}")
        print(f"ğŸ‰ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥: {output_file}")
        print(f"ğŸ“Š ìµœì¢… ê²°ê³¼: {success_count}/{total_count}ê°œ ì„±ê³µ ({success_count/total_count*100:.1f}%)")
        print(f"{'='*70}")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ìë™ ì‹¤í–‰: ì „ì²´ ì¹´í…Œê³ ë¦¬ì—ì„œ 20ê°œ ê¸°ì‚¬ ìˆ˜ì§‘
    rss_url = "https://www.onseoul.net/rss/allArticle.xml"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    output_file = f"results/ì„œìš¸ìì¹˜ì‹ ë¬¸_ì „ì²´_{timestamp}.csv"
    print(f"ğŸš€ ì „ì²´ ì¹´í…Œê³ ë¦¬ì—ì„œ 20ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘!")
    print(f"ğŸ“ ì €ì¥ íŒŒì¼: {output_file}\n")
    fetch_onseoul_rss_to_csv(rss_url, output_file, max_articles=20, category="ì „ì²´")
